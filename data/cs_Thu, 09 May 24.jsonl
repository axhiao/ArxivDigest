{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Image Classification for CSSVD Detection in Cacao Plants", "authors": "Atuhurra Jesse, N'guessan Yves-Roland Douha, Pabitra Lenka", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "The detection of diseases within plants has attracted a lot of attention from computer vision enthusiasts. Despite the progress made to detect diseases in many plants, there remains a research gap to train image classifiers to detect the cacao swollen shoot virus disease or CSSVD for short, pertinent to cacao plants. This gap has mainly been due to the unavailability of high quality labeled training data. Moreover, institutions have been hesitant to share their data related to CSSVD. To fill these gaps, we propose the development of image classifiers to detect CSSVD-infected cacao plants. Our proposed solution is based on VGG16, ResNet50 and Vision Transformer (ViT). We evaluate the classifiers on a recently released and publicly accessible KaraAgroAI Cocoa dataset. Our best image classifier, based on ResNet50, achieves 95.39\\% precision, 93.75\\% recall, 94.34\\% F1-score and 94\\% accuracy on only 20 epochs. There is a +9.75\\% improvement in recall when compared to previous works. Our results indicate that the image classifiers learn to identify cacao plants infected with CSSVD."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          When Training-Free NAS Meets Vision Transformer: A Neural Tangent Kernel Perspective", "authors": "Qiqi Zhou, Yichen Zhu", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "This paper investigates the Neural Tangent Kernel (NTK) to search vision transformers without training. In contrast with the previous observation that NTK-based metrics can effectively predict CNNs performance at initialization, we empirically show their inefficacy in the ViT search space. We hypothesize that the fundamental feature learning preference within ViT contributes to the ineffectiveness of applying NTK to NAS for ViT. We both theoretically and empirically validate that NTK essentially estimates the ability of neural networks that learn low-frequency signals, completely ignoring the impact of high-frequency signals in feature learning. To address this limitation, we propose a new method called ViNTK that generalizes the standard NTK to the high-frequency domain by integrating the Fourier features from inputs. Experiments with multiple ViT search spaces on image classification and semantic segmentation tasks show that our method can significantly speed up search costs over prior state-of-the-art NAS for ViT while maintaining similar performance on searched architectures."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          An intuitive multi-frequency feature representation for SO(3)-equivariant networks", "authors": "Dongwon Son, Jaehyung Kim, Sanghyeon Son, Beomjoon Kim", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR)", "abstract": "The usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN) architectures. However, its performance is limited because it is designed to use only three-dimensional features, which is insufficient to capture the details present in 3D data. In this paper, we introduce an equivariant feature representation for mapping a 3D point to a high-dimensional feature space. Our feature can discern multiple frequencies present in 3D data, which is the key to designing an expressive feature for 3D vision tasks. Our representation can be used as an input to VNs, and the results demonstrate that with our feature representation, VN captures more details, overcoming the limitation raised in its original paper."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DiffFinger: Advancing Synthetic Fingerprint Generation through Denoising Diffusion Probabilistic Models", "authors": "Freddie Grabovski, Lior Yasur, Yaniv Hacmon, Lior Nisimov, Stav Nimrod", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "This study explores the generation of synthesized fingerprint images using Denoising Diffusion Probabilistic Models (DDPMs). The significant obstacles in collecting real biometric data, such as privacy concerns and the demand for diverse datasets, underscore the imperative for synthetic biometric alternatives that are both realistic and varied. Despite the strides made with Generative Adversarial Networks (GANs) in producing realistic fingerprint images, their limitations prompt us to propose DDPMs as a promising alternative. DDPMs are capable of generating images with increasing clarity and realism while maintaining diversity. Our results reveal that DiffFinger not only competes with authentic training set data in quality but also provides a richer set of biometric data, reflecting true-to-life variability. These findings mark a promising stride in biometric synthesis, showcasing the potential of DDPMs to advance the landscape of fingerprint identification and authentication systems."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Applying the Iterative Development Process: The Creation of Fractal Emergence", "authors": "Christopher R. H. Hanusa, Eric Vergo", "subjects": "Subjects:\nOther Computer Science (cs.OH)", "abstract": "The iterative development process is a framework used to design products and applications across a wide range of domains. It centers around building prototypes, testing them, and updating based on the test results. We discuss how we applied this technique to create Fractal Emergence, an interactive piece of mathematical art."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Learning label-label correlations in Extreme Multi-label Classification via Label Features", "authors": "Siddhant Kharbanda, Devaansh Gupta, Erik Schultheis, Atmadeep Banerjee, Cho-Jui Hsieh, Rohit Babbar", "subjects": "Subjects:\nMachine Learning (cs.LG); Information Retrieval (cs.IR)", "abstract": "Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches. In this paper, we propose Gandalf, a novel approach which makes use of a label co-occurrence graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. Surprisingly, models trained on these new training instances, although being less than half of the original dataset, can outperform models trained on the original dataset, particularly on the PSP@k metric for tail labels. With this insight, we aim to train existing XMC algorithms on both, the original and new training instances, leading to an average 5% relative improvements for 6 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3M labels. Gandalf can be applied in a plug-and-play manner to various methods and thus forwards the state-of-the-art in the domain, without incurring any additional computational overheads."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ClothPPO: A Proximal Policy Optimization Enhancing Framework for Robotic Cloth Manipulation with Observation-Aligned Action Spaces", "authors": "Libing Yang, Yang Li, Long Chen", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)", "abstract": "Vision-based robotic cloth unfolding has made great progress recently. However, prior works predominantly rely on value learning and have not fully explored policy-based techniques. Recently, the success of reinforcement learning on the large language model has shown that the policy gradient algorithm can enhance policy with huge action space. In this paper, we introduce ClothPPO, a framework that employs a policy gradient algorithm based on actor-critic architecture to enhance a pre-trained model with huge 10^6 action spaces aligned with observation in the task of unfolding clothes. To this end, we redefine the cloth manipulation problem as a partially observable Markov decision process. A supervised pre-training stage is employed to train a baseline model of our policy. In the second stage, the Proximal Policy Optimization (PPO) is utilized to guide the supervised model within the observation-aligned action space. By optimizing and updating the strategy, our proposed method increases the garment's surface area for cloth unfolding under the soft-body manipulation task. Experimental results show that our proposed framework can further improve the unfolding performance of other state-of-the-art methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Differentially Private Federated Learning without Noise Addition: When is it Possible?", "authors": "Jiang Zhang, Yahya H Ezzeldin, Ahmed Roushdy Elkordy, Konstantinos Psounis, Salman Avestimehr", "subjects": "Subjects:\nCryptography and Security (cs.CR); Machine Learning (cs.LG)", "abstract": "Federated Learning (FL) with Secure Aggregation (SA) has gained significant attention as a privacy preserving framework for training machine learning models while preventing the server from learning information about users' data from their individual encrypted model updates. Recent research has extended privacy guarantees of FL with SA by bounding the information leakage through the aggregate model over multiple training rounds thanks to leveraging the \"noise\" from other users' updates. However, the privacy metric used in that work (mutual information) measures the on-average privacy leakage, without providing any privacy guarantees for worse-case scenarios. To address this, in this work we study the conditions under which FL with SA can provide worst-case differential privacy guarantees. Specifically, we formally identify the necessary condition that SA can provide DP without addition noise. We then prove that when the randomness inside the aggregated model update is Gaussian with non-singular covariance matrix, SA can provide differential privacy guarantees with the level of privacy $\\epsilon$ bounded by the reciprocal of the minimum eigenvalue of the covariance matrix. However, we further demonstrate that in practice, these conditions are almost unlikely to hold and hence additional noise added in model updates is still required in order for SA in FL to achieve DP. Lastly, we discuss the potential solution of leveraging inherent randomness inside aggregated model update to reduce the amount of addition noise required for DP guarantee."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Differentially Private Synthetic Data with Private Density Estimation", "authors": "Nikolija Bojkovic, Po-Ling Loh", "subjects": "Subjects:\nCryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)", "abstract": "The need to analyze sensitive data, such as medical records or financial data, has created a critical research challenge in recent years. In this paper, we adopt the framework of differential privacy, and explore mechanisms for generating an entire dataset which accurately captures characteristics of the original data. We build upon the work of Boedihardjo et al, which laid the foundations for a new optimization-based algorithm for generating private synthetic data. Importantly, we adapt their algorithm by replacing a uniform sampling step with a private distribution estimator; this allows us to obtain better computational guarantees for discrete distributions, and develop a novel algorithm suitable for continuous distributions. We also explore applications of our work to several statistical tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Reference Model for Information Quality in an IT Governance Context", "authors": "Dirk Steuperaert, Geert Poels, Jan Devos", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "IT Governance systems are increasingly required to keep todays organizations functioning. IT Governance requires a holistic system of interacting components, including processes, organizational structures, information, and others. Performance management of IT Governance systems is of utmost importance to maintain their effectiveness. Capability models are used to assess and manage IT Governance process performance, whereas similar mechanisms are lacking for other types of IT Governance system components, e.g. information. In this paper, we focus on how to define the quality of IT Governance information, as a proxy for the performance of the information component of the IT Governance system. Using a Design Science approach, we iteratively develop, based on theory, and empirically evaluate, based on expert validation, a reference model for IT Governance information quality, i.e., the Information Quality Reference Model that can be used for assessing the quality of IT Governance information items. The model is comprehensive yet manageable and provides a basis for building a capability model for IT Governance information."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Understanding High-Order Network Structure using Permissible Walks on Attributed Hypergraphs", "authors": "Enzo Battistella, Sean English, Robert Green, Cliff Joslyn, Evgeniya Lagoda, Van Magnan, Audun Myers, Evan D. Nash, Michael Robinson", "subjects": "Subjects:\nSocial and Information Networks (cs.SI); Combinatorics (math.CO)", "abstract": "Hypergraphs have been a recent focus of study in mathematical data science as a tool to understand complex networks with high-order connections. One question of particular relevance is how to leverage information carried in hypergraph attributions when doing walk-based techniques. In this work, we focus on a new generalization of a walk in a network that recovers previous approaches and allows for a description of permissible walks in hypergraphs. Permissible walk graphs are constructed by intersecting the attributed $s$-line graph of a hypergraph with a relation respecting graph. The attribution of the hypergraph's line graph commonly carries over information from categorical and temporal attributions of the original hypergraph. To demonstrate this approach on a temporally attributed example, we apply our framework to a Reddit data set composed of hyperedges as threads and authors as nodes where post times are tracked."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Inferring Discussion Topics about Exploitation of Vulnerabilities from Underground Hacking Forums", "authors": "Felipe Moreno-Vera", "subjects": "Subjects:\nCryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)", "abstract": "The increasing sophistication of cyber threats necessitates proactive measures to identify vulnerabilities and potential exploits. Underground hacking forums serve as breeding grounds for the exchange of hacking techniques and discussions related to exploitation. In this research, we propose an innovative approach using topic modeling to analyze and uncover key themes in vulnerabilities discussed within these forums. The objective of our study is to develop a machine learning-based model that can automatically detect and classify vulnerability-related discussions in underground hacking forums. By monitoring and analyzing the content of these forums, we aim to identify emerging vulnerabilities, exploit techniques, and potential threat actors. To achieve this, we collect a large-scale dataset consisting of posts and threads from multiple underground forums. We preprocess and clean the data to ensure accuracy and reliability. Leveraging topic modeling techniques, specifically Latent Dirichlet Allocation (LDA), we uncover latent topics and their associated keywords within the dataset. This enables us to identify recurring themes and prevalent discussions related to vulnerabilities, exploits, and potential targets."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Fast Decentralized Gradient Tracking for Federated Minimax Optimization with Local Updates", "authors": "Chris Junchi Li", "subjects": "Subjects:\nMachine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)", "abstract": "Federated learning (FL) for minimax optimization has emerged as a powerful paradigm for training models across distributed nodes/clients while preserving data privacy and model robustness on data heterogeneity. In this work, we delve into the decentralized implementation of federated minimax optimization by proposing \\texttt{K-GT-Minimax}, a novel decentralized minimax optimization algorithm that combines local updates and gradient tracking techniques. Our analysis showcases the algorithm's communication efficiency and convergence rate for nonconvex-strongly-concave (NC-SC) minimax optimization, demonstrating a superior convergence rate compared to existing methods. \\texttt{K-GT-Minimax}'s ability to handle data heterogeneity and ensure robustness underscores its significance in advancing federated learning research and applications."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A critical appraisal of water table depth estimation: Challenges and opportunities within machine learning", "authors": "Joseph Janssen, Ardalan Tootchi, Ali A. Ameli", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "Fine-resolution spatial patterns of water table depth (WTD) can inform the dynamics of groundwater-dependent systems, including ecological, hydrological, and anthropogenic systems. Generally, a large-scale (e.g., continental or global) spatial map of static WTD can be simulated using either physically-based (PB) or machine learning-based (ML) models. We construct three fine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and more than 20 million real and proxy observations of WTD, across the United States and Canada. The three ML models were constrained using known physical relations between WTD's drivers and WTD and were trained by sequentially adding real and proxy observations of WTD. We interpret the black box of our physically constrained ML models and compare it against available literature in groundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we demonstrate that our models can more accurately predict unseen real and proxy observations of WTD across most of North America's ecoregions compared to three available PB simulations of WTD. However, we still argue that large-scale WTD estimation is far from being a solved problem. We reason that due to biased and untrustworthy observational data, the misspecification of physically-based equations, and the over-flexibility of machine learning models, our community's confidence in ML or PB simulations of WTD is far too high and verifiably accurate simulations of WTD do not yet exist in the literature, particularly in arid high-elevation landscapes. Ultimately, we thoroughly discuss future directions that may help hydrogeologists decide how to proceed with WTD estimations, with a particular focus on the application of machine learning."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          PoPE: Legendre Orthogonal Polynomials Based Position Encoding for Large Language Models", "authors": "Arpit Aggarwal", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "There are several improvements proposed over the baseline Absolute Positional Encoding (APE) method used in original transformer. In this study, we aim to investigate the implications of inadequately representing positional encoding in higher dimensions on crucial aspects of the attention mechanism, the model's capacity to learn relative positional information, and the convergence of models, all stemming from the choice of sinusoidal basis functions. Through a combination of theoretical insights and empirical analyses, we elucidate how these challenges extend beyond APEs and may adversely affect the performance of Relative Positional Encoding (RPE) methods, such as Rotatory Positional Encoding (RoPE). Subsequently, we introduce an innovative solution termed Orthogonal Polynomial Based Positional Encoding (PoPE) to address some of the limitations associated with existing methods. The PoPE method encodes positional information by leveraging Orthogonal Legendre polynomials. Legendre polynomials as basis functions offers several desirable properties for positional encoding, including improved correlation structure, non-periodicity, orthogonality, and distinct functional forms among polynomials of varying orders. Our experimental findings demonstrate that transformer models incorporating PoPE outperform baseline transformer models on the $Multi30k$ English-to-German translation task, thus establishing a new performance benchmark. Furthermore, PoPE-based transformers exhibit significantly accelerated convergence rates. Additionally, we will present novel theoretical perspectives on position encoding based on the superior performance of PoPE."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Novel Wide-Area Multiobject Detection System with High-Probability Region Searching", "authors": "Xianlei Long, Hui Zhao, Chao Chen, Fuqiang Gu, Qingyi Gu", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "abstract": "In recent years, wide-area visual surveillance systems have been widely applied in various industrial and transportation scenarios. These systems, however, face significant challenges when implementing multi-object detection due to conflicts arising from the need for high-resolution imaging, efficient object searching, and accurate localization. To address these challenges, this paper presents a hybrid system that incorporates a wide-angle camera, a high-speed search camera, and a galvano-mirror. In this system, the wide-angle camera offers panoramic images as prior information, which helps the search camera capture detailed images of the targeted objects. This integrated approach enhances the overall efficiency and effectiveness of wide-area visual detection systems. Specifically, in this study, we introduce a wide-angle camera-based method to generate a panoramic probability map (PPM) for estimating high-probability regions of target object presence. Then, we propose a probability searching module that uses the PPM-generated prior information to dynamically adjust the sampling range and refine target coordinates based on uncertainty variance computed by the object detector. Finally, the integration of PPM and the probability searching module yields an efficient hybrid vision system capable of achieving 120 fps multi-object search and detection. Extensive experiments are conducted to verify the system's effectiveness and robustness."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Language Modeling Using Tensor Trains", "authors": "Zhan Su, Yuqin Zhou, Fengran Mo, Jakob Grue Simonsen", "subjects": "Subjects:\nComputation and Language (cs.CL); Information Retrieval (cs.IR)", "abstract": "We propose a novel tensor network language model based on the simplest tensor network (i.e., tensor trains), called `Tensor Train Language Model' (TTLM). TTLM represents sentences in an exponential space constructed by the tensor product of words, but computing the probabilities of sentences in a low-dimensional fashion. We demonstrate that the architectures of Second-order RNNs, Recurrent Arithmetic Circuits (RACs), and Multiplicative Integration RNNs are, essentially, special cases of TTLM. Experimental evaluations on real language modeling tasks show that the proposed variants of TTLM (i.e., TTLM-Large and TTLM-Tiny) outperform the vanilla Recurrent Neural Networks (RNNs) with low-scale of hidden units. (The code is available at this https URL.)"}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Visually Guided Swarm Motion Coordination via Insect-inspired Small Target Motion Reactions", "authors": "Md Arif Billah, Imraan A. Faruque", "subjects": "Subjects:\nSystems and Control (eess.SY)", "abstract": "Despite progress developing experimentally-consistent models of insect in-flight sensing and feedback for individual agents, a lack of systematic understanding of the multi-agent and group performance of the resulting bio-inspired sensing and feedback approaches remains a barrier to robotic swarm implementations. This study introduces the small-target motion reactive (STMR) swarming approach by designing a concise engineering model of the small target motion detector (STMD) neurons found in insect lobula complexes. The STMD neuron model identifies the bearing angle at which peak optic flow magnitude occurs, and this angle is used to design an output feedback switched control system. A theoretical stability analysis provides bi-agent stability and state boundedness in group contexts. The approach is simulated and implemented on ground vehicles for validation and behavioral studies. The results indicate despite having the lowest connectivity of contemporary approaches (each agent instantaneously regards only a single neighbor), collective group motion can be achieved. STMR group level metric analysis also highlights continuously varying polarization and decreasing heading variance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Integrating knowledge-guided symbolic regression and model-based design of experiments to automate process flow diagram development", "authors": "Alexander W. Rogers, Amanda Lane, Cesar Mendoza, Simon Watson, Adam Kowalski, Philip Martin, Dongda Zhang", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "New products must be formulated rapidly to succeed in the global formulated product market; however, key product indicators (KPIs) can be complex, poorly understood functions of the chemical composition and processing history. Consequently, scale-up must currently undergo expensive trial-and-error campaigns. To accelerate process flow diagram (PFD) optimisation and knowledge discovery, this work proposed a novel digital framework to automatically quantify process mechanisms by integrating symbolic regression (SR) within model-based design of experiments (MBDoE). Each iteration, SR proposed a Pareto front of interpretable mechanistic expressions, and then MBDoE designed a new experiment to discriminate between them while balancing PFD optimisation. To investigate the framework's performance, a new process model capable of simulating general formulated product synthesis was constructed to generate in-silico data for different case studies. The framework could effectively discover ground-truth process mechanisms within a few iterations, indicating its great potential for use within the general chemical industry for digital manufacturing and product innovation."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Contextual API Completion for Unseen Repositories Using LLMs", "authors": "Noor Nashid, Taha Shabani, Parsa Alian, Ali Mesbah", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "Large language models have made substantial progress in addressing diverse code-related tasks. However, their adoption is hindered by inconsistencies in generating output due to the lack of real-world, domain-specific information, such as for intra-repository API calls for unseen software projects. We introduce a novel technique to mitigate hallucinations by leveraging global and local contextual information within a code repository for API completion tasks. Our approach is tailored to refine code completion tasks, with a focus on optimizing local API completions. We examine relevant import statements during API completion to derive insights into local APIs, drawing from their method signatures. For API token completion, we analyze the inline variables and correlate them with the appropriate imported modules, thereby allowing our approach to rank the most contextually relevant suggestions from the available local APIs. Further, for conversational API completion, we gather APIs that are most relevant to the developer query with a retrieval-based search across the project. We employ our tool, LANCE, within the framework of our proposed benchmark, APIEval, encompassing two different programming languages. Our evaluation yields an average accuracy of 82.6% for API token completion and 76.9% for conversational API completion tasks. On average, LANCE surpasses Copilot by 143% and 142% for API token completion and conversational API completion, respectively. The implications of our findings are substantial for developers, suggesting that our lightweight context analysis can be applied to multilingual environments without language-specific training or fine-tuning, allowing for efficient implementation with minimal examples and effort."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          An Empirical Study of Kotlin-Java Interactions", "authors": "Qiong Feng, Huan Ji, Xiaotian Ma, Peng Liang", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "Background: Since Google introduced Kotlin as an official programming language for developing Android apps in 2017, Kotlin has gained widespread adoption in Android development. The interoperability of Java and Kotlin's design nature allows them to coexist and interact with each other smoothly within a project. Aims: However, there is limited research on how Java and Kotlin interact with each other in real-world projects and what challenges are faced during these interactions. The answers to these questions are key to understanding these kinds of cross-language software systems. Methods: In this paper, we implemented a tool named DependExtractor, which can extract 11 kinds of Kotlin-Java dependencies, and conducted an empirical study of 23 Kotlin-Java real-world projects with 3,227 Java and 8,630 Kotlin source files. Results: Our findings revealed that Java and Kotlin frequently interact with each other in these cross-language projects, with access and call dependency types being the most dominant. Compared to files interacting with other files in the same language, Java/Kotlin source files, which participate in the cross-language interactions, undergo more commits. Additionally, among all Kotlin-Java problematic interactions, we identified seven common mistakes, along with their fixing strategies. Conclusions: The findings of this study can help developers understand and address the challenges in Kotlin-Java projects."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Neural network based approach for solving problems in plane wave duct acoustics", "authors": "D. Veerababu, Prasanta K. Ghosh", "subjects": "Subjects:\nComputational Engineering, Finance, and Science (cs.CE)", "abstract": "Neural networks have emerged as a tool for solving differential equations in many branches of engineering and science. But their progress in frequency domain acoustics is limited by the vanishing gradient problem that occurs at higher frequencies. This paper discusses a formulation that can address this issue. The problem of solving the governing differential equation along with the boundary conditions is posed as an unconstrained optimization problem. The acoustic field is approximated to the output of a neural network which is constructed in such a way that it always satisfies the boundary conditions. The applicability of the formulation is demonstrated on popular problems in plane wave acoustic theory. The predicted solution from the neural network formulation is compared with those obtained from the analytical solution. A good agreement is observed between the two solutions. The method of transfer learning to calculate the particle velocity from the existing acoustic pressure field is demonstrated with and without mean flow effects. The sensitivity of the training process to the choice of the activation function and the number of collocation points is studied."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          AI in Lung Health: Benchmarking Detection and Diagnostic Models Across Multiple CT Scan Datasets", "authors": "Fakrul Islam Tushar, Avivah Wang, Lavsen Dahal, Michael R. Harowicz, Kyle J. Lafata, Tina D. Tailor, Joseph Y. Lo", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "BACKGROUND: Lung cancer's high mortality rate can be mitigated by early detection, which is increasingly reliant on artificial intelligence (AI) for diagnostic imaging. However, the performance of AI models is contingent upon the datasets used for their training and validation. METHODS: This study developed and validated the DLCSD-mD and LUNA16-mD models utilizing the Duke Lung Cancer Screening Dataset (DLCSD), encompassing over 2,000 CT scans with more than 3,000 annotations. These models were rigorously evaluated against the internal DLCSD and external LUNA16 and NLST datasets, aiming to establish a benchmark for imaging-based performance. The assessment focused on creating a standardized evaluation framework to facilitate consistent comparison with widely utilized datasets, ensuring a comprehensive validation of the model's efficacy. Diagnostic accuracy was assessed using free-response receiver operating characteristic (FROC) and area under the curve (AUC) analyses. RESULTS: On the internal DLCSD set, the DLCSD-mD model achieved an AUC of 0.93 (95% CI:0.91-0.94), demonstrating high accuracy. Its performance was sustained on the external datasets, with AUCs of 0.97 (95% CI: 0.96-0.98) on LUNA16 and 0.75 (95% CI: 0.73-0.76) on NLST. Similarly, the LUNA16-mD model recorded an AUC of 0.96 (95% CI: 0.95-0.97) on its native dataset and showed transferable diagnostic performance with AUCs of 0.91 (95% CI: 0.89-0.93) on DLCSD and 0.71 (95% CI: 0.70-0.72) on NLST. CONCLUSION: The DLCSD-mD model exhibits reliable performance across different datasets, establishing the DLCSD as a robust benchmark for lung cancer detection and diagnosis. Through the provision of our models and code to the public domain, we aim to accelerate the development of AI-based diagnostic tools and encourage reproducibility and collaborative advancements within the medical machine-learning (ML) field."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Probabilistic Byzantine Fault Tolerance (Extended Version)", "authors": "Diogo Avel\u00e3s, Hasan Heydari, Eduardo Alchieri, Tobias Distler, Alysson Bessani", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC)", "abstract": "Consensus is a fundamental building block for constructing reliable and fault-tolerant distributed services. Many Byzantine fault-tolerant consensus protocols designed for partially synchronous systems adopt a pessimistic approach when dealing with adversaries, ensuring safety in a deterministic way even under the worst-case scenarios that adversaries can create. Following this approach typically results in either an increase in the message complexity (e.g., PBFT) or an increase in the number of communication steps (e.g., HotStuff). In practice, however, adversaries are not as powerful as the ones assumed by these protocols. Furthermore, it might suffice to ensure safety and liveness properties with high probability. In order to accommodate more realistic and optimistic adversaries and improve the scalability of the BFT consensus, we propose ProBFT (Probabilistic Byzantine Fault Tolerance). ProBFT is a leader-based probabilistic consensus protocol with a message complexity of $O(n\\sqrt{n})$ and an optimal number of communication steps that tolerates Byzantine faults in permissioned partially synchronous systems. It is built on top of well-known primitives, such as probabilistic Byzantine quorums and verifiable random functions. ProBFT guarantees safety and liveness with high probabilities even with faulty leaders, as long as a supermajority of replicas is correct, and using only a fraction of messages employed in PBFT (e.g., $20\\%$). We provide a detailed description of ProBFT's protocol and its analysis."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Learning Distributional Demonstration Spaces for Task-Specific Cross-Pose Estimation", "authors": "Jenny Wang, Octavian Donca, David Held", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object. Previous work has shown success in learning relative placement tasks from just a small number of demonstrations when using relational reasoning networks with geometric inductive biases. However, such methods cannot flexibly represent multimodal tasks, like a mug hanging on any of n racks. We propose a method that incorporates additional properties that enable learning multimodal relative placement solutions, while retaining the provably translation-invariant and relational properties of prior work. We show that our method is able to learn precise relative placement tasks with only 10-20 multimodal demonstrations with no human annotations across a diverse set of objects within a category."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Numerical Fuzz: A Type System for Rounding Error Analysis", "authors": "Ariel E. Kellison, Justin Hsu", "subjects": "Subjects:\nProgramming Languages (cs.PL); Numerical Analysis (math.NA)", "abstract": "Algorithms operating on real numbers are implemented as floating-point computations in practice, but floating-point operations introduce roundoff errors that can degrade the accuracy of the result. We propose $\\Lambda_{num}$, a functional programming language with a type system that can express quantitative bounds on roundoff error. Our type system combines a sensitivity analysis, enforced through a linear typing discipline, with a novel graded monad to track the accumulation of roundoff errors. We prove that our type system is sound by relating the denotational semantics of our language to the exact and floating-point operational semantics. To demonstrate our system, we instantiate $\\Lambda_{num}$ with error metrics proposed in the numerical analysis literature and we show how to incorporate rounding operations that faithfully model aspects of the IEEE 754 floating-point standard. To show that $\\Lambda_{num}$ can be a useful tool for automated error analysis, we develop a prototype implementation for $\\Lambda_{num}$ that infers error bounds that are competitive with existing tools, while often running significantly faster. Finally, we consider semantic extensions of our graded monad to bound error under more complex rounding behaviors, such as non-deterministic and randomized rounding."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Multi-Margin Loss: Proposal and Application in Recommender Systems", "authors": "Makbule Gulcin Ozsoy", "subjects": "Subjects:\nMachine Learning (cs.LG); Information Retrieval (cs.IR)", "abstract": "Recommender systems guide users through vast amounts of information by suggesting items based on their predicted preferences. Collaborative filtering-based deep learning techniques have regained popularity due to their straightforward nature, relying only on user-item interactions. Typically, these systems consist of three main components: an interaction module, a loss function, and a negative sampling strategy. Initially, researchers focused on enhancing performance by developing complex interaction modules. However, there has been a recent shift toward refining loss functions and negative sampling strategies. This shift has led to an increased interest in contrastive learning, which pulls similar pairs closer while pushing dissimilar ones apart. Contrastive learning involves key practices such as heavy data augmentation, large batch sizes, and hard-negative sampling, but these also bring challenges like high memory demands and under-utilization of some negative samples. The proposed Multi-Margin Loss (MML) addresses these challenges by introducing multiple margins and varying weights for negative samples. This allows MML to efficiently utilize not only the hardest negatives but also other non-trivial negatives, offering a simpler yet effective loss function that outperforms more complex methods, especially when resources are limited. Experiments on two well-known datasets demonstrated that MML achieved up to a 20% performance improvement compared to a baseline contrastive loss function when fewer number of negative samples are used."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Unique continuation for the wave equation based on a discontinuous Galerkin time discretization", "authors": "Erik Burman, Janosch Preuss", "subjects": "Subjects:\nNumerical Analysis (math.NA)", "abstract": "We consider a stable unique continuation problem for the wave equation where the initial data is lacking and the solution is reconstructed using measurements in some subset of the bulk domain. Typically fairly sophisticated space-time methods have been used in previous work to obtain stable and accurate solutions to this reconstruction problem. Here we propose to solve the problem using a standard discontinuous Galerkin method for the temporal discretization and continuous finite elements for the space discretization. Error estimates are established under a geometric control condition. We also investigate two preconditioning strategies which can be used to solve the arising globally coupled space-time system by means of simple time-stepping procedures. Our numerical experiments test the performance of these strategies and highlight the importance of the geometric control condition for reconstructing the solution beyond the data domain."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          NACSOS-nexus: NLP Assisted Classification, Synthesis and Online Screening with New and EXtended Usage Scenarios", "authors": "Tim Repke, Max Callaghan", "subjects": "Subjects:\nDigital Libraries (cs.DL)", "abstract": "NACSOS is a web-based platform for curating data used in systematic maps. It contains several (experimental) features that aid the evidence synthesis process from finding and ingesting primary data (mainly scientific publications), basic search and exploration thereof, but mainly the handling of managing the manual and automated annotations. The platform supports prioritised screening algorithms and is the first to fully implement statistical stopping criteria. Annotations by multiple coders can be resolved and customisable quality metrics are computed on-the-fly. In its current state, the annotations are performed on document level. The ecosystem around NACSOS offers packages for accessing the underlying database and practical utility functions that have proven useful in a multitude of projects. Further, it provides the backbone of living maps, review ecosystems, and our public literature hub for sharing high-quality curated corpora."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Bounds on the Statistical Leakage-Resilience of Shamir's Secret Sharing", "authors": "Utkarsh Gupta, Hessam Mahdavifar", "subjects": "Subjects:\nInformation Theory (cs.IT)", "abstract": "Secret sharing is an instrumental tool for sharing secret keys in distributed systems. In a classical threshold setting, this involves a dealer who has a secret/key, a set of parties/users to which shares of the secret are sent, and a threshold on the number of users whose presence is needed in order to recover the secret. In secret sharing, secure links with no leakage are often assumed between the involved parties. However, when the users are nodes in a communication network and all the links are physical links, e.g., wireless, such assumptions are not valid anymore. In order to study this critical problem, we propose a statistical leakage model of secret sharing, where some noisy versions of all the secret shares might be independently leaked to an adversary. We then study the resilience of the seminal Shamir's secret sharing scheme with statistical leakage, and bound certain measures of security (i.e., semantic security, mutual information security), given other parameters of the system including the amount of leakage from each secret share. We show that for an extreme scenario of Shamir's scheme, in particular when the underlying field characteristic is $2$, the security of each bit of the secret against leakage improves exponentially with the number of users. To the best of our knowledge, this is the first attempt towards understanding secret sharing under general statistical noisy leakage."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models", "authors": "Abeba Birhane, Sepehr Dehdashtian, Vinay Uday Prabhu, Vishnu Boddeti", "subjects": "Subjects:\nComputers and Society (cs.CY)", "abstract": "Scale the model, scale the data, scale the GPU farms is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. Content warning: This article contains racially dehumanising and offensive descriptions."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic Segmentation of Diverse Landscapes", "authors": "Charles Gaydon, Michel Daab, Floryne Roche", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a new tool to monitor territory and support public policies. Processing ALS data at scale requires efficient point classification methods that perform well over highly diverse territories. To evaluate them, researchers need large annotated Lidar datasets, however, current Lidar benchmark datasets have restricted scope and often cover a single urban area. To bridge this data gap, we present the FRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an ultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with high-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is built upon France's nationwide open Lidar data. It achieves spatial and semantic diversity via a sampling scheme that explicitly concentrates rare classes and challenging landscapes from five French regions. It should support the development of 3D deep learning approaches for large-scale land monitoring. We describe the nature of the source data, the sampling workflow, the content of the resulting dataset, and provide an initial evaluation of segmentation performance using a performant 3D neural architecture."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Data-driven Error Estimation: Upper Bounding Multiple Errors with No Technical Debt", "authors": "Sanath Kumar Krishnamurthy, Susan Athey, Emma Brunskill", "subjects": "Subjects:\nMachine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "We formulate the problem of constructing multiple simultaneously valid confidence intervals (CIs) as estimating a high probability upper bound on the maximum error for a class/set of estimate-estimand-error tuples, and refer to this as the error estimation problem. For a single such tuple, data-driven confidence intervals can often be used to bound the error in our estimate. However, for a class of estimate-estimand-error tuples, nontrivial high probability upper bounds on the maximum error often require class complexity as input -- limiting the practicality of such methods and often resulting in loose bounds. Rather than deriving theoretical class complexity-based bounds, we propose a completely data-driven approach to estimate an upper bound on the maximum error. The simple and general nature of our solution to this fundamental challenge lends itself to several applications including: multiple CI construction, multiple hypothesis testing, estimating excess risk bounds (a fundamental measure of uncertainty in machine learning) for any training/fine-tuning algorithm, and enabling the development of a contextual bandit pipeline that can leverage any reward model estimation procedure as input (without additional mathematical analysis)."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Enhancing LLM-Based Feedback: Insights from Intelligent Tutoring Systems and the Learning Sciences", "authors": "John Stamper, Ruiwei Xiao, Xinynig Hou", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Computers and Society (cs.CY)", "abstract": "The field of Artificial Intelligence in Education (AIED) focuses on the intersection of technology, education, and psychology, placing a strong emphasis on supporting learners' needs with compassion and understanding. The growing prominence of Large Language Models (LLMs) has led to the development of scalable solutions within educational settings, including generating different types of feedback in Intelligent Tutoring Systems. However, the approach to utilizing these models often involves directly formulating prompts to solicit specific information, lacking a solid theoretical foundation for prompt construction and empirical assessments of their impact on learning. This work advocates careful and caring AIED research by going through previous research on feedback generation in ITS, with emphasis on the theoretical frameworks they utilized and the efficacy of the corresponding design in empirical evaluations, and then suggesting opportunities to apply these evidence-based principles to the design, experiment, and evaluation phases of LLM-based feedback generation. The main contributions of this paper include: an avocation of applying more cautious, theoretically grounded methods in feedback generation in the era of generative AI; and practical suggestions on theory and evidence-based feedback design for LLM-powered ITS."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Simpler and More General Distributed Coloring Based on Simple List Defective Coloring Algorithms", "authors": "Marc Fuchs, Fabian Kuhn", "subjects": "Subjects:\nData Structures and Algorithms (cs.DS); Distributed, Parallel, and Cluster Computing (cs.DC); Discrete Mathematics (cs.DM)", "abstract": "In this paper, we give list coloring variants of simple iterative defective coloring algorithms. Formally, in a list defective coloring instance, each node $v$ of a graph is given a list $L_v$ of colors and a list of allowed defects $d_v(x)$ for the colors. Each node $v$ needs to be colored with a color $x\\in L_v$ such that at most $d_v(x)$ neighbors of $v$ also pick the same color $x$. For a defect parameter $d$, it is known that by making two sweeps in opposite order over the nodes of an edge-oriented graph with maximum outdegree $\\beta$, one can compute a coloring with $O(\\beta^2/d^2)$ colors such that every node has at most $d$ outneighbors of the same color. We generalize this and show that if all nodes have lists of size $p^2$ and $\\forall v:\\sum_{x\\in L_v}(d_v(x)+1)>p\\cdot\\beta$, we can make two sweeps of the nodes such that at the end, each node $v$ has chosen a color $x\\in L_v$ for which at most $d_v(x)$ outneighbors of $v$ are colored with color $x$. Our algorithm is simpler and computationally significantly more efficient than existing algorithms for similar list defective coloring problems. We show that the above result can in particular be used to obtain an alternative $\\tilde{O}(\\sqrt{\\Delta})+O(\\log^* n)$-round algorithm for the $(\\Delta+1)$-coloring problem in the CONGEST model. The neighborhood independence $\\theta$ of a graph is the maximum number of pairwise non-adjacent neighbors of some node of the graph. It is known that by doing a single sweep over the nodes of a graph of neighborhood independence $\\theta$, one can compute a $d$-defective coloring with $O(\\theta\\cdot \\Delta/d)$ colors. We extend this approach to the list defective coloring setting and use it to obtain an efficient recursive coloring algorithm for graphs of neighborhood independence $\\theta$. In particular, if $\\theta=O(1)$, we get an $(\\log\\Delta)^{O(\\log\\log\\Delta)}+O(\\log^* n)$-round algorithm."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Self-Supervised Method for Body Part Segmentation and Keypoint Detection of Rat Images", "authors": "L\u00e1szl\u00f3 Kop\u00e1csi, \u00c1ron F\u00f3thi, Andr\u00e1s L\u0151rincz", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "abstract": "Recognition of individual components and keypoint detection supported by instance segmentation is crucial to analyze the behavior of agents on the scene. Such systems could be used for surveillance, self-driving cars, and also for medical research, where behavior analysis of laboratory animals is used to confirm the aftereffects of a given medicine. A method capable of solving the aforementioned tasks usually requires a large amount of high-quality hand-annotated data, which takes time and money to produce. In this paper, we propose a method that alleviates the need for manual labeling of laboratory rats. To do so, first, we generate initial annotations with a computer vision-based approach, then through extensive augmentation, we train a deep neural network on the generated data. The final system is capable of instance segmentation, keypoint detection, and body part segmentation even when the objects are heavily occluded."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large Language Models", "authors": "Yinru Long, Zilin Ma, Yiyang Mei, Zhaoyuan Su", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC)", "abstract": "LGBTQ+ community face disproportionate mental health challenges, including higher rates of depression, anxiety, and suicidal ideation. Research has shown that LGBTQ+ people have been using large language model-based chatbots, such as ChatGPT, for their mental health needs. Despite the potential for immediate support and anonymity these chatbots offer, concerns regarding their capacity to provide empathetic, accurate, and affirming responses remain. In response to these challenges, we propose a framework for evaluating the affirmativeness of LLMs based on principles of affirmative therapy, emphasizing the need for attitudes, knowledge, and actions that support and validate LGBTQ+ experiences. We propose a combination of qualitative and quantitative analyses, hoping to establish benchmarks for \"Affirmative AI,\" ensuring that LLM-based chatbots can provide safe, supportive, and effective mental health support to LGBTQ+ individuals. We benchmark LLM affirmativeness not as a mental health solution for LGBTQ+ individuals or to claim it resolves their mental health issues, as we highlight the need to consider complex discrimination in the LGBTQ+ community when designing technological aids. Our goal is to evaluate LLMs for LGBTQ+ mental health support since many in the community already use them, aiming to identify potential harms of using general-purpose LLMs in this context."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense", "authors": "Siqi Shen, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Soujanya Poria, Rada Mihalcea", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper, we conduct a comprehensive examination of the capabilities and limitations of several state-of-the-art LLMs in the context of cultural commonsense tasks. Using several general and cultural commonsense benchmarks, we find that (1) LLMs have a significant discrepancy in performance when tested on culture-specific commonsense knowledge for different cultures; (2) LLMs' general commonsense capability is affected by cultural context; and (3) The language used to query the LLMs can impact their performance on cultural-related tasks. Our study points to the inherent bias in the cultural understanding of LLMs and provides insights that can help develop culturally aware language models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Corporate Communication Companion (CCC): An LLM-empowered Writing Assistant for Workplace Social Media", "authors": "Zhuoran Lu, Sheshera Mysore, Tara Safavi, Jennifer Neville, Longqi Yang, Mengting Wan", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC)", "abstract": "Workplace social media platforms enable employees to cultivate their professional image and connect with colleagues in a semi-formal environment. While semi-formal corporate communication poses a unique set of challenges, large language models (LLMs) have shown great promise in helping users draft and edit their social media posts. However, LLMs may fail to capture individualized tones and voices in such workplace use cases, as they often generate text using a \"one-size-fits-all\" approach that can be perceived as generic and bland. In this paper, we present Corporate Communication Companion (CCC), an LLM-empowered interactive system that helps people compose customized and individualized workplace social media posts. Using need-finding interviews to motivate our system design, CCC decomposes the writing process into two core functions, outline and edit: First, it suggests post outlines based on users' job status and previous posts, and next provides edits with attributions that users can contextually customize. We conducted a within-subjects user study asking participants both to write posts and evaluate posts written by others. The results show that CCC enhances users' writing experience, and audience members rate CCC-enhanced posts as higher quality than posts written using a non-customized writing assistant. We conclude by discussing the implications of LLM-empowered corporate communication."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ACEGEN: Reinforcement learning of generative chemical agents for drug discovery", "authors": "Albert Bou, Morgan Thomas, Sebastian Dittert, Carles Navarro Ram\u00edrez, Maciej Majewski, Ye Wang, Shivam Patel, Gary Tresadern, Mazen Ahmad, Vincent Moens, Woody Sherman, Simone Sciabola, Gianni De Fabritiis", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)", "abstract": "In recent years, reinforcement learning (RL) has emerged as a valuable tool in drug design, offering the potential to propose and optimize molecules with desired properties. However, striking a balance between capability, flexibility, and reliability remains challenging due to the complexity of advanced RL algorithms and the significant reliance on specialized code. In this work, we introduce ACEGEN, a comprehensive and streamlined toolkit tailored for generative drug design, built using TorchRL, a modern decision-making library that offers efficient and thoroughly tested reusable components. ACEGEN provides a robust, flexible, and efficient platform for molecular design. We validate its effectiveness by benchmarking it across various algorithms and conducting multiple drug discovery case studies. ACEGEN is accessible at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar", "authors": "David Borts, Erich Liang, Tim Br\u00f6dermann, Andrea Ramazzina, Stefanie Walz, Edoardo Palladin, Jipeng Sun, David Bruggemann, Christos Sakaridis, Luc Van Gool, Mario Bijelic, Felix Heide", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle. While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored. Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques. Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors. We introduce Radar Fields - a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy. The proposed method does not rely on volume rendering. Instead, we learn fields in Fourier frequency space, supervised with raw radar data. We validate the effectiveness of the method across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and in harsh weather scenarios, where mm-wavelength sensing is especially favorable."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Proximal Policy Optimization with Adaptive Exploration", "authors": "Andrei Lixandru", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Proximal Policy Optimization with Adaptive Exploration (axPPO) is introduced as a novel learning algorithm. This paper investigates the exploration-exploitation tradeoff within the context of reinforcement learning and aims to contribute new insights into reinforcement learning algorithm design. The proposed adaptive exploration framework dynamically adjusts the exploration magnitude during training based on the recent performance of the agent. Our proposed method outperforms standard PPO algorithms in learning efficiency, particularly when significant exploratory behavior is needed at the beginning of the learning process."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Energy Bounds for Discontinuous Galerkin Spectral Element Approximations of Well-Posed Overset Grid Problems for Hyperbolic Systems", "authors": "David A. Kopriva, Andrew R. Winters, Jan Nordstr\u00f6m", "subjects": "Subjects:\nNumerical Analysis (math.NA)", "abstract": "We show that even though the Discontinuous Galerkin Spectral Element Method is stable for hyperbolic boundary-value problems, and the overset domain problem is well-posed in an appropriate norm, the energy of the approximation is bounded by data only for fixed polynomial order and time. In the absence of dissipation, coupling of the overlapping domains is destabilizing by allowing positive eigenvalues in the system to be integrated in time. This coupling can be stabilized in one space dimension by using the upwind numerical flux. To help provide additional dissipation, we introduce a novel penalty method that applies dissipation at arbitrary points within the overlap region and depends only on the difference between the solutions. We present numerical experiments in one space dimension to illustrate the implementation of the well-posed penalty formulation, and show spectral convergence of the approximations when dissipation is applied."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics", "authors": "Hanlin Zhu, Baihe Huang, Shaolun Zhang, Michael Jordan, Jiantao Jiao, Yuandong Tian, Stuart Russell", "subjects": "Subjects:\nMachine Learning (cs.LG); Computation and Language (cs.CL)", "abstract": "Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on ''A is B'', LLM fails to directly conclude ''B is A'' during inference, which is known as the ''reversal curse'' (Berglund et al., 2023). In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers using the framework of Tian et al. (2023a). Our analysis reveals a core reason why the reversal curse happens: the (effective) weights of both auto-regressive models show asymmetry, i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT) (Wei et al., 2022b). We show the necessity of COT, i.e., a model trained on ''$A \\to B$'' and ''$B \\to C$'' fails to directly conclude ''$A \\to C$'' without COT (also empirically observed by Allen-Zhu and Li (2023)), for one-layer transformers via training dynamics, which provides a new perspective different from previous work (Feng et al., 2024) that focuses on expressivity. Finally, we also conduct experiments to validate our theory on multi-layer transformers under different settings."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Interpretable Tensor Fusion", "authors": "Saurabh Varshneya, Antoine Ledent, Philipp Liznerski, Andriy Balinskyy, Purvanshi Mehta, Waleed Mustafa, Marius Kloft", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "Conventional machine learning methods are predominantly designed to predict outcomes based on a single data type. However, practical applications may encompass data of diverse types, such as text, images, and audio. We introduce interpretable tensor fusion (InTense), a multimodal learning method for training neural networks to simultaneously learn multimodal data representations and their interpretable fusion. InTense can separately capture both linear combinations and multiplicative interactions of diverse data types, thereby disentangling higher-order interactions from the individual effects of each modality. InTense provides interpretability out of the box by assigning relevance scores to modalities and their associations. The approach is theoretically grounded and yields meaningful relevance scores on multiple synthetic and real-world datasets. Experiments on six real-world datasets show that InTense outperforms existing state-of-the-art multimodal interpretable approaches in terms of accuracy and interpretability."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Towards Accurate and Efficient Document Analytics with Large Language Models", "authors": "Yiming Lin, Madelon Hulsebos, Ruiying Ma, Shreya Shankar, Sepanta Zeigham, Aditya G. Parameswaran, Eugene Wu", "subjects": "Subjects:\nDatabases (cs.DB)", "abstract": "Unstructured data formats account for over 80% of the data currently stored, and extracting value from such formats remains a considerable challenge. In particular, current approaches for managing unstructured documents do not support ad-hoc analytical queries on document collections. Moreover, Large Language Models (LLMs) directly applied to the documents themselves, or on portions of documents through a process of Retrieval-Augmented Generation (RAG), fail to provide high accuracy query results, and in the LLM-only case, additionally incur high costs. Since many unstructured documents in a collection often follow similar templates that impart a common semantic structure, we introduce ZenDB, a document analytics system that leverages this semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document collections. ZenDB efficiently extracts semantic hierarchical structures from such templatized documents, and introduces a novel query engine that leverages these structures for accurate and cost-effective query execution. Users can impose a schema on their documents, and query it, all via SQL. Extensive experiments on three real-world document collections demonstrate ZenDB's benefits, achieving up to 30% cost savings compared to LLM-based baselines, while maintaining or improving accuracy, and surpassing RAG-based baselines by up to 61% in precision and 80% in recall, at a marginally higher cost."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          TexControl: Sketch-Based Two-Stage Fashion Image Generation Using Diffusion Model", "authors": "Yongming Zhang, Tianyu Zhang, Haoran Xie", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)", "abstract": "Deep learning-based sketch-to-clothing image generation provides the initial designs and inspiration in the fashion design processes. However, clothing generation from freehand drawing is challenging due to the sparse and ambiguous information from the drawn sketches. The current generation models may have difficulty generating detailed texture information. In this work, we propose TexControl, a sketch-based fashion generation framework that uses a two-stage pipeline to generate the fashion image corresponding to the sketch input. First, we adopt ControlNet to generate the fashion image from sketch and keep the image outline stable. Then, we use an image-to-image method to optimize the detailed textures of the generated images and obtain the final results. The evaluation results show that TexControl can generate fashion images with high-quality texture as fine-grained image generation."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Responding to Generative AI Technologies with Research-through-Design: The Ryelands AI Lab as an Exploratory Study", "authors": "Jesse Josua Benjamin, Joseph Lindley, Elizabeth Edwards, Elisa Rubegni, Tim Korjakow, David Grist, Rhiannon Sharkey", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "abstract": "Generative AI technologies demand new practical and critical competencies, which call on design to respond to and foster these. We present an exploratory study guided by Research-through-Design, in which we partnered with a primary school to develop a constructionist curriculum centered on students interacting with a generative AI technology. We provide a detailed account of the design of and outputs from the curriculum and learning materials, finding centrally that the reflexive and prolonged `hands-on' approach led to a co-development of students' practical and critical competencies. From the study, we contribute guidance for designing constructionist approaches to generative AI technology education; further arguing to do so with `critical responsivity.' We then discuss how HCI researchers may leverage constructionist strategies in designing interactions with generative AI technologies; and suggest that Research-through-Design can play an important role as a `rapid response methodology' capable of reacting to fast-evolving, disruptive technologies such as generative AI."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Pipe Routing with Topology Control for UAV Networks", "authors": "Shreyas Devaraju, Shivam Garg, Alexander Ihler, Sunil Kumar", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI); Multiagent Systems (cs.MA); Robotics (cs.RO)", "abstract": "Routing protocols help in transmitting the sensed data from UAVs monitoring the targets (called target UAVs) to the BS. However, the highly dynamic nature of an autonomous, decentralized UAV network leads to frequent route breaks or traffic disruptions. Traditional routing schemes cannot quickly adapt to dynamic UAV networks and/or incur large control overhead and delays. To establish stable, high-quality routes from target UAVs to the BS, we design a hybrid reactive routing scheme called pipe routing that is mobility, congestion, and energy-aware. The pipe routing scheme discovers routes on-demand and proactively switches to alternate high-quality routes within a limited region around the active routes (called the pipe) when needed, reducing the number of route breaks and increasing data throughput. We then design a novel topology control-based pipe routing scheme to maintain robust connectivity in the pipe region around the active routes, leading to improved route stability and increased throughput with minimal impact on the coverage performance of the UAV network."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation", "authors": "Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Recent advances in diffusion-based generative modeling have led to the development of text-to-video (T2V) models that can generate high-quality videos conditioned on a text prompt. Most of these T2V models often produce single-scene video clips that depict an entity performing a particular action (e.g., `a red panda climbing a tree'). However, it is pertinent to generate multi-scene videos since they are ubiquitous in the real-world (e.g., `a red panda climbing a tree' followed by `the red panda sleeps on the top of the tree'). To generate multi-scene videos from the pretrained T2V model, we introduce Time-Aligned Captions (TALC) framework. Specifically, we enhance the text-conditioning mechanism in the T2V architecture to recognize the temporal alignment between the video scenes and scene descriptions. For instance, we condition the visual features of the earlier and later scenes of the generated video with the representations of the first scene description (e.g., `a red panda climbing a tree') and second scene description (e.g., `the red panda sleeps on the top of the tree'), respectively. As a result, we show that the T2V model can generate multi-scene videos that adhere to the multi-scene text descriptions and be visually consistent (e.g., entity and background). Further, we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework. We show that the TALC-finetuned model outperforms the baseline methods by 15.5 points in the overall score, which averages visual consistency and text adherence using human evaluation. The project website is this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking", "authors": "Emre Can Acikgoz, Mete Erdogan, Deniz Yuret", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Large Language Models (LLMs) are becoming crucial across various fields, emphasizing the urgency for high-quality models in underrepresented languages. This study explores the unique challenges faced by low-resource languages, such as data scarcity, model selection, evaluation, and computational limitations, with a special focus on Turkish. We conduct an in-depth analysis to evaluate the impact of training strategies, model choices, and data availability on the performance of LLMs designed for underrepresented languages. Our approach includes two methodologies: (i) adapting existing LLMs originally pretrained in English to understand Turkish, and (ii) developing a model from the ground up using Turkish pretraining data, both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities. The relative performance of these methods is evaluated through the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that assess different reasoning and knowledge skills. Furthermore, we conducted experiments on data and model scaling, both during pretraining and fine-tuning, simultaneously emphasizing the capacity for knowledge transfer across languages and addressing the challenges of catastrophic forgetting encountered during fine-tuning on a different language. Our goal is to offer a detailed guide for advancing the LLM framework in low-resource linguistic contexts, thereby making natural language processing (NLP) benefits more globally accessible."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Towards Human-AI Mutual Learning: A New Research Paradigm", "authors": "Xiaomei Wang, Xiaoyu Chen", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "This paper describes a new research paradigm for studying human-AI collaboration, named \"human-AI mutual learning\", defined as the process where humans and AI agents preserve, exchange, and improve knowledge during human-AI collaboration. We describe relevant methodologies, motivations, domain examples, benefits, challenges, and future research agenda under this paradigm."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Carbon Filter: Real-time Alert Triage Using Large Scale Clustering and Fast Search", "authors": "Jonathan Oliver, Raghav Batta, Adam Bates, Muhammad Adil Inam, Shelly Mehta, Shugao Xia", "subjects": "Subjects:\nCryptography and Security (cs.CR); Machine Learning (cs.LG)", "abstract": "\"Alert fatigue\" is one of the biggest challenges faced by the Security Operations Center (SOC) today, with analysts spending more than half of their time reviewing false alerts. Endpoint detection products raise alerts by pattern matching on event telemetry against behavioral rules that describe potentially malicious behavior, but can suffer from high false positives that distract from actual attacks. While alert triage techniques based on data provenance may show promise, these techniques can take over a minute to inspect a single alert, while EDR customers may face tens of millions of alerts per day; the current reality is that these approaches aren't nearly scalable enough for production environments. We present Carbon Filter, a statistical learning based system that dramatically reduces the number of alerts analysts need to manually review. Our approach is based on the observation that false alert triggers can be efficiently identified and separated from suspicious behaviors by examining the process initiation context (e.g., the command line) that launched the responsible process. Through the use of fast-search algorithms for training and inference, our approach scales to millions of alerts per day. Through batching queries to the model, we observe a theoretical maximum throughput of 20 million alerts per hour. Based on the analysis of tens of million alerts from customer deployments, our solution resulted in a 6-fold improvement in the Signal-to-Noise ratio without compromising on alert triage performance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Nearly Tight Bounds on Approximate Equilibria in Spatial Competition on the Line", "authors": "Umang Bhaskar, Soumyajit Pyne", "subjects": "Subjects:\nComputer Science and Game Theory (cs.GT)", "abstract": "In Hotelling's model of spatial competition, a unit mass of voters is distributed in the interval $[0,1]$ (with their location corresponding to their political persuasion), and each of $m$ candidates selects as a strategy his distinct position in this interval. Each voter votes for the nearest candidate, and candidates choose their strategy to maximize their votes. It is known that if there are more than two candidates, equilibria may not exist in this model. It was unknown, however, how close to an equilibrium one could get. Our work studies approximate equilibria in this model, where a strategy profile is an (additive) $\\epsilon$-equilibria if no candidate can increase their votes by $\\epsilon$, and provides tight or nearly-tight bounds on the approximation $\\epsilon$ achievable. We show that for 3 candidates, for any distribution of the voters, $\\epsilon \\ge 1/12$. Thus, somewhat surprisingly, for any distribution of the voters and any strategy profile of the candidates, at least $1/12$th of the total votes is always left ``on the table.'' Extending this, we show that in the worst case, there exist voter distributions for which $\\epsilon \\ge 1/6$, and this is tight: one can always compute a $1/6$-approximate equilibria. We then study the general case of $m$ candidates, and show that as $m$ grows large, we get closer to an exact equilibrium: one can always obtain an $1/(m+1)$-approximate equilibria in polynomial time. We show this bound is asymptotically tight, by giving voter distributions for which $\\epsilon \\ge 1/(m+3)$."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Existential Theory of the Reals with Summation Operators", "authors": "Markus Bl\u00e4ser, Julian D\u00f6rfler, Maciej Liskiewicz, Benito van der Zander", "subjects": "Subjects:\nComputational Complexity (cs.CC); Logic in Computer Science (cs.LO)", "abstract": "To characterize the computational complexity of satisfiability problems for probabilistic and causal reasoning within the Pearl's Causal Hierarchy, arXiv:2305.09508 [cs.AI] introduce a new natural class, named succ-$\\exists$R. This class can be viewed as a succinct variant of the well-studied class $\\exists$R based on the Existential Theory of the Reals (ETR). Analogously to $\\exists$R, succ-$\\exists$R is an intermediate class between NEXP and EXPSPACE, the exponential versions of NP and PSPACE. The main contributions of this work are threefold. Firstly, we characterize the class succ-$\\exists$R in terms of nondeterministic real RAM machines and develop structural complexity theoretic results for real RAMs, including translation and hierarchy theorems. Notably, we demonstrate the separation of $\\exists$R and succ-$\\exists$R. Secondly, we examine the complexity of model checking and satisfiability of fragments of existential second-order logic and probabilistic independence logic. We show succ-$\\exists$R- completeness of several of these problems, for which the best-known complexity lower and upper bounds were previously NEXP-hardness and EXPSPACE, respectively. Thirdly, while succ-$\\exists$R is characterized in terms of ordinary (non-succinct) ETR instances enriched by exponential sums and a mechanism to index exponentially many variables, in this paper, we prove that when only exponential sums are added, the corresponding class $\\exists$R^{\\Sigma} is contained in PSPACE. We conjecture that this inclusion is strict, as this class is equivalent to adding a VNP-oracle to a polynomial time nondeterministic real RAM. Conversely, the addition of exponential products to ETR, yields PSPACE. Additionally, we study the satisfiability problem for probabilistic reasoning, with the additional requirement of a small model and prove that this problem is complete for $\\exists$R^{\\Sigma}."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory Architectures", "authors": "Ruiyang Qin, Zheyu Yan, Dewen Zeng, Zhenge Jia, Dancheng Liu, Jianbo Liu, Zhi Zheng, Ningyuan Cao, Kai Ni, Jinjun Xiong, Yiyu Shi", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Information Retrieval (cs.IR)", "abstract": "Large Language Models (LLMs) deployed on edge devices learn through fine-tuning and updating a certain portion of their parameters. Although such learning methods can be optimized to reduce resource utilization, the overall required resources remain a heavy burden on edge devices. Instead, Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method, can improve the quality of the LLM-generated content without updating model parameters. However, the RAG-based LLM may involve repetitive searches on the profile data in every user-LLM interaction. This search can lead to significant latency along with the accumulation of user data. Conventional efforts to decrease latency result in restricting the size of saved user data, thus reducing the scalability of RAG as user data continuously grows. It remains an open question: how to free RAG from the constraints of latency and scalability on edge devices? In this paper, we propose a novel framework to accelerate RAG via Computing-in-Memory (CiM) architectures. It accelerates matrix multiplications by performing in-situ computation inside the memory while avoiding the expensive data transfer between the computing unit and memory. Our framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive learning-based training method and noise-aware training, can enable RAG to efficiently search profile data with CiM. To the best of our knowledge, this is the first work utilizing CiM to accelerate RAG."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Mitigating Negative Side Effects in Multi-Agent Systems Using Blame Assignment", "authors": "Pulkit Rustagi, Sandhya Saisubramanian", "subjects": "Subjects:\nMultiagent Systems (cs.MA); Robotics (cs.RO)", "abstract": "When agents that are independently trained (or designed) to complete their individual tasks are deployed in a shared environment, their joint actions may produce negative side effects (NSEs). As their training does not account for the behavior of other agents or their joint action effects on the environment, the agents have no prior knowledge of the NSEs of their actions. We model the problem of mitigating NSEs in a cooperative multi-agent system as a Lexicographic Decentralized Markov Decision Process with two objectives. The agents must optimize the completion of their assigned tasks while mitigating NSEs. We assume independence of transitions and rewards with respect to the agents' tasks but the joint NSE penalty creates a form of dependence in this setting. To improve scalability, the joint NSE penalty is decomposed into individual penalties for each agent using credit assignment, which facilitates decentralized policy computation. Our results in simulation on three domains demonstrate the effectiveness and scalability of our approach in mitigating NSEs by updating the policies of a subset of agents in the system."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Guiding the Way: A Comprehensive Examination of AI Guidelines in Global Media", "authors": "M.F. de-Lima-Santos, W.N. Yeung, T. Dodds", "subjects": "Subjects:\nComputers and Society (cs.CY)", "abstract": "With the increasing adoption of artificial intelligence (AI) technologies in the news industry, media organizations have begun publishing guidelines that aim to promote the responsible, ethical, and unbiased implementation of AI-based technologies. These guidelines are expected to serve journalists and media workers by establishing best practices and a framework that helps them navigate ever-evolving AI tools. Drawing on institutional theory and digital inequality concepts, this study analyzes 37 AI guidelines for media purposes in 17 countries. Our analysis reveals key thematic areas, such as transparency, accountability, fairness, privacy, and the preservation of journalistic values. Results highlight shared principles and best practices that emerge from these guidelines, including the importance of human oversight, explainability of AI systems, disclosure of automated content, and protection of user data. However, the geographical distribution of these guidelines, highlighting the dominance of Western nations, particularly North America and Europe, can further ongoing concerns about power asymmetries in AI adoption and consequently isomorphism outside these regions. Our results may serve as a resource for news organizations, policymakers, and stakeholders looking to navigate the complex AI development toward creating a more inclusive and equitable digital future for the media industry worldwide."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A wearable anti-gravity supplement to therapy does not improve arm function in chronic stroke: a randomized trial", "authors": "Courtney Celian, Partha Ryali, Valentino Wilson, Adith Srivatsaa, James L. Patton", "subjects": "Subjects:\nRobotics (cs.RO); Medical Physics (physics.med-ph)", "abstract": "Background: Gravity confounds arm movement ability in post-stroke hemiparesis. Reducing its influence allows effective practice leading to recovery. Yet, there is a scarcity of wearable devices suitable for personalized use across diverse therapeutic activities in the clinic. Objective: In this study, we investigated the safety, feasibility, and efficacy of anti-gravity therapy using the ExoNET device in post-stroke participants. Methods: Twenty chronic stroke survivors underwent six, 45-minute occupational therapy sessions while wearing the ExoNET, randomized into either the treatment (ExoNET tuned to gravity-support) or control group (ExoNET tuned to slack condition). Clinical outcomes were evaluated by a blinded-rater at baseline, post, and six-week follow-up sessions. Kinetic, kinematic, and patient experience outcomes were also assessed. Results: Mixed-effect models showed a significant improvement in Box and Blocks scores in the post-intervention session for the treatment group (effect size: 2.1, p = .04). No significant effects were found between the treatment and control groups for ARAT scores and other clinical metrics. Direct kinetic effects revealed a significant reduction in muscle activity during free exploration with an effect size of (-7.12%, p< 005). There were no significant longitudinal kinetic or kinematic trends. Subject feedback suggested a generally positive perception of the anti-gravity therapy. Conclusions: Anti-gravity therapy with the ExoNET is a safe and feasible treatment for post-stroke rehabilitation. The device provided anti-gravity forces, did not encumber range of motion, and clinical metrics of anti-gravity therapy demonstrated improvements in gross manual dexterity. Further research is required to explore potential benefits in broader clinical metrics."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Untangling Lariats: Subgradient Following of Variationally Penalized Objectives", "authors": "Kai-Chia Mo, Shai Shalev-Shwartz, Nis\u00e6l Sh\u00e1rtov", "subjects": "Subjects:\nMachine Learning (cs.LG); Optimization and Control (math.OC)", "abstract": "We describe a novel subgradient following apparatus for calculating the optimum of convex problems with variational penalties. In this setting, we receive a sequence $y_i,\\ldots,y_n$ and seek a smooth sequence $x_1,\\ldots,x_n$. The smooth sequence attains the minimum Bregman divergence to an input sequence with additive variational penalties in the general form of $\\sum_i g_i(x_{i+1}-x_i)$. We derive, as special cases of our apparatus, known algorithms for the fused lasso and isotonic regression. Our approach also facilitates new variational penalties such as non-smooth barrier functions. We next derive and analyze multivariate problems in which $\\mathbf{x}_i,\\mathbf{y}_i\\in\\mathbb{R}^d$ and variational penalties that depend on $\\|\\mathbf{x}_{i+1}-\\mathbf{x}_i\\|$. The norms we consider are $\\ell_2$ and $\\ell_\\infty$ which promote group sparsity. Last but not least, we derive a lattice-based subgradient following for variational penalties characterized through the output of arbitrary convolutional filters. This paradigm yields efficient solvers for problems in which sparse high-order discrete derivatives such as acceleration and jerk are desirable."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Community detection in multi-layer bipartite networks", "authors": "Huan Qing", "subjects": "Subjects:\nSocial and Information Networks (cs.SI); Physics and Society (physics.soc-ph); Methodology (stat.ME)", "abstract": "The problem of community detection in multi-layer undirected networks has received considerable attention in recent years. However, practical scenarios often involve multi-layer bipartite networks, where each layer consists of two distinct types of nodes. Existing community detection algorithms tailored for multi-layer undirected networks are not directly applicable to multi-layer bipartite networks. To address this challenge, this paper introduces a novel multi-layer degree-corrected stochastic co-block model specifically designed to capture the underlying community structure within multi-layer bipartite networks. Within this framework, we propose an efficient debiased spectral co-clustering algorithm for detecting nodes' communities. We establish the consistent estimation property of our proposed algorithm and demonstrate that an increased number of layers in bipartite networks improves the accuracy of community detection. Through extensive numerical experiments, we showcase the superior performance of our algorithm compared to existing methods. Additionally, we validate our algorithm by applying it to real-world multi-layer network datasets, yielding meaningful and insightful results."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Enhancing Knowledge Retrieval with Topic Modeling for Knowledge-Grounded Dialogue", "authors": "Nhat Tran, Diane Litman", "subjects": "Subjects:\nInformation Retrieval (cs.IR)", "abstract": "Knowledge retrieval is one of the major challenges in building a knowledge-grounded dialogue system. A common method is to use a neural retriever with a distributed approximate nearest-neighbor database to quickly find the relevant knowledge sentences. In this work, we propose an approach that utilizes topic modeling on the knowledge base to further improve retrieval accuracy and as a result, improve response generation. Additionally, we experiment with a large language model, ChatGPT, to take advantage of the improved retrieval performance to further improve the generation results. Experimental results on two datasets show that our approach can increase retrieval and generation performance. The results also indicate that ChatGPT is a better response generator for knowledge-grounded dialogue when relevant knowledge is provided."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          RACER: Epistemic Risk-Sensitive RL Enables Fast Driving with Fewer Crashes", "authors": "Kyle Stachowicz, Sergey Levine", "subjects": "Subjects:\nRobotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Reinforcement learning provides an appealing framework for robotic control due to its ability to learn expressive policies purely through real-world interaction. However, this requires addressing real-world constraints and avoiding catastrophic failures during training, which might severely impede both learning progress and the performance of the final policy. In many robotics settings, this amounts to avoiding certain \"unsafe\" states. The high-speed off-road driving task represents a particularly challenging instantiation of this problem: a high-return policy should drive as aggressively and as quickly as possible, which often requires getting close to the edge of the set of \"safe\" states, and therefore places a particular burden on the method to avoid frequent failures. To both learn highly performant policies and avoid excessive failures, we propose a reinforcement learning framework that combines risk-sensitive control with an adaptive action space curriculum. Furthermore, we show that our risk-sensitive objective automatically avoids out-of-distribution states when equipped with an estimator for epistemic uncertainty. We implement our algorithm on a small-scale rally car and show that it is capable of learning high-speed policies for a real-world off-road driving task. We show that our method greatly reduces the number of safety violations during the training process, and actually leads to higher-performance policies in both driving and non-driving simulation environments with similar challenges."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Physics-based deep learning reveals rising heating demand heightens air pollution in Norwegian cities", "authors": "Cong Cao, Ramit Debnath, R. Michael Alvarez", "subjects": "Subjects:\nComputers and Society (cs.CY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)", "abstract": "Policymakers frequently analyze air quality and climate change in isolation, disregarding their interactions. This study explores the influence of specific climate factors on air quality by contrasting a regression model with K-Means Clustering, Hierarchical Clustering, and Random Forest techniques. We employ Physics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine the air pollution predictions. Our analysis utilizes ten years (2009-2018) of daily traffic, weather, and air pollution data from three major cities in Norway. Findings from feature selection reveal a correlation between rising heating degree days and heightened air pollution levels, suggesting increased heating activities in Norway are a contributing factor to worsening air quality. PBDL demonstrates superior accuracy in air pollution predictions compared to LSTM. This paper contributes to the growing literature on PBDL methods for more accurate air pollution predictions using environmental variables, aiding policymakers in formulating effective data-driven climate policies."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Remote Diffusion", "authors": "Kunal Sunil Kasodekar", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "I explored adapting Stable Diffusion v1.5 for generating domain-specific satellite and aerial images in remote sensing. Recognizing the limitations of existing models like Midjourney and Stable Diffusion, trained primarily on natural RGB images and lacking context for remote sensing, I used the RSICD dataset to train a Stable Diffusion model with a loss of 0.2. I incorporated descriptive captions from the dataset for text-conditioning. Additionally, I created a synthetic dataset for a Land Use Land Classification (LULC) task, employing prompting techniques with RAG and ChatGPT and fine-tuning a specialized remote sensing LLM. However, I faced challenges with prompt quality and model performance. I trained a classification model (ResNet18) on the synthetic dataset achieving 49.48% test accuracy in TorchGeo to create a baseline. Quantitative evaluation through FID scores and qualitative feedback from domain experts assessed the realism and quality of the generated images and dataset. Despite extensive fine-tuning and dataset iterations, results indicated subpar image quality and realism, as indicated by high FID scores and domain-expert evaluation. These findings call attention to the potential of diffusion models in remote sensing while highlighting significant challenges related to insufficient pretraining data and computational resources."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Metaverse Survey & Tutorial: Exploring Key Requirements, Technologies, Standards, Applications, Challenges, and Perspectives", "authors": "Danda B. Rawat, Hassan El alami, Desta Haileselassie Hagos", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "In this paper, we present a comprehensive survey of the metaverse, envisioned as a transformative dimension of next-generation Internet technologies. This study not only outlines the structural components of our survey but also makes a substantial scientific contribution by elucidating the foundational concepts underlying the emergence of the metaverse. We analyze its architecture by defining key characteristics and requirements, thereby illuminating the nascent reality set to revolutionize digital interactions. Our analysis emphasizes the importance of collaborative efforts in developing metaverse standards, thereby fostering a unified understanding among industry stakeholders, organizations, and regulatory bodies. We extend our scrutiny to critical technologies integral to the metaverse, including interactive experiences, communication technologies, ubiquitous computing, digital twins, artificial intelligence, and cybersecurity measures. For each technological domain, we rigorously assess current contributions, principal techniques, and representative use cases, providing a nuanced perspective on their potential impacts. Furthermore, we delve into the metaverse's diverse applications across education, healthcare, business, social interactions, industrial sectors, defense, and mission-critical operations, highlighting its extensive utility. Each application is thoroughly analyzed, demonstrating its value and addressing associated challenges. The survey concludes with an overview of persistent challenges and future directions, offering insights into essential considerations and strategies necessary to harness the full potential of the metaverse. Through this detailed investigation, our goal is to articulate the scientific contributions of this survey paper, transcending a mere structural overview to highlight the transformative implications of the metaverse."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Detecting and Refining HiRISE Image Patches Obscured by Atmospheric Dust", "authors": "Kunal Sunil Kasodekar", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "HiRISE (High-Resolution Imaging Science Experiment) is a camera onboard the Mars Reconnaissance orbiter responsible for photographing vast areas of the Martian surface in unprecedented detail. It can capture millions of incredible closeup images in minutes. However, Mars suffers from frequent regional and local dust storms hampering this data-collection process, and pipeline, resulting in loss of effort and crucial flight time. Removing these images manually requires a large amount of manpower. I filter out these images obstructed by atmospheric dust automatically by using a Dust Image Classifier fine-tuned on Resnet-50 with an accuracy of 94.05%. To further facilitate the seamless filtering of Images I design a prediction pipeline that classifies and stores these dusty patches. I also denoise partially obstructed images using an Auto Encoder-based denoiser and Pix2Pix GAN with 0.75 and 0.99 SSIM Index respectively."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Learning Phonotactics from Linguistic Informants", "authors": "Canaan Breiss, Alexis Ross, Amani Maina-Kilaas, Roger Levy, Jacob Andreas", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "We propose an interactive approach to language learning that utilizes linguistic acceptability judgments from an informant (a competent language user) to learn a grammar. Given a grammar formalism and a framework for synthesizing data, our model iteratively selects or synthesizes a data-point according to one of a range of information-theoretic policies, asks the informant for a binary judgment, and updates its own parameters in preparation for the next query. We demonstrate the effectiveness of our model in the domain of phonotactics, the rules governing what kinds of sound-sequences are acceptable in a language, and carry out two experiments, one with typologically-natural linguistic data and another with a range of procedurally-generated languages. We find that the information-theoretic policies that our model uses to select items to query the informant achieve sample efficiency comparable to, and sometimes greater than, fully supervised approaches."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          LLMs Can Patch Up Missing Relevance Judgments in Evaluation", "authors": "Shivani Upadhyay, Ehsan Kamalloo, Jimmy Lin", "subjects": "Subjects:\nInformation Retrieval (cs.IR)", "abstract": "Unjudged documents or holes in information retrieval benchmarks are considered non-relevant in evaluation, yielding no gains in measuring effectiveness. However, these missing judgments may inadvertently introduce biases into the evaluation as their prevalence for a retrieval model is heavily contingent on the pooling process. Thus, filling holes becomes crucial in ensuring reliable and accurate evaluation. Collecting human judgment for all documents is cumbersome and impractical. In this paper, we aim at leveraging large language models (LLMs) to automatically label unjudged documents. Our goal is to instruct an LLM using detailed instructions to assign fine-grained relevance judgments to holes. To this end, we systematically simulate scenarios with varying degrees of holes by randomly dropping relevant documents from the relevance judgment in TREC DL tracks. Our experiments reveal a strong correlation between our LLM-based method and ground-truth relevance judgments. Based on our simulation experiments conducted on three TREC DL datasets, in the extreme scenario of retaining only 10% of judgments, our method achieves a Kendall tau correlation of 0.87 and 0.92 on an average for Vicu\u00f1a-7B and GPT-3.5 Turbo respectively."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          S-EQA: Tackling Situational Queries in Embodied Question Answering", "authors": "Vishnu Sashank Dorbala, Prasoon Goyal, Robinson Piramuthu, Michael Johnston, Dinesh Manocha, Reza Ghanadhan", "subjects": "Subjects:\nRobotics (cs.RO); Artificial Intelligence (cs.AI)", "abstract": "We present and tackle the problem of Embodied Question Answering (EQA) with Situational Queries (S-EQA) in a household environment. Unlike prior EQA work tackling simple queries that directly reference target objects and quantifiable properties pertaining them, EQA with situational queries (such as \"Is the bathroom clean and dry?\") is more challenging, as the agent needs to figure out not just what the target objects pertaining to the query are, but also requires a consensus on their states to be answerable. Towards this objective, we first introduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an LLM's output to create a dataset of unique situational queries, corresponding consensus object information, and predicted answers. PGE maintains uniqueness among the generated queries, using multiple forms of semantic similarity. We validate the generated dataset via a large scale user-study conducted on M-Turk, and introduce it as S-EQA, the first dataset tackling EQA with situational queries. Our user study establishes the authenticity of S-EQA with a high 97.26% of the generated queries being deemed answerable, given the consensus object data. Conversely, we observe a low correlation of 46.2% on the LLM-predicted answers to human-evaluated ones; indicating the LLM's poor capability in directly answering situational queries, while establishing S-EQA's usability in providing a human-validated consensus for an indirect solution. We evaluate S-EQA via Visual Question Answering (VQA) on VirtualHome, which unlike other simulators, contains several objects with modifiable states that also visually appear different upon modification -- enabling us to set a quantitative benchmark for S-EQA. To the best of our knowledge, this is the first work to introduce EQA with situational queries, and also the first to use a generative approach for query creation."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          One-Bit Phase Retrieval: Optimal Rates and Efficient Algorithms", "authors": "Junren Chen, Ming Yuan", "subjects": "Subjects:\nInformation Theory (cs.IT)", "abstract": "In this paper, we study the sample complexity and develop efficient optimal algorithms for 1-bit phase retrieval: recovering a signal $\\mathbf{x}\\in\\mathbb{R}^n$ from $m$ phaseless bits $\\{\\mathrm{sign}(|\\mathbf{a}_i^\\top\\mathbf{x}|-\\tau)\\}_{i=1}^m$ generated by standard Gaussian $\\mathbf{a}_i$s. By investigating a phaseless version of random hyperplane tessellation, we show that (constrained) hamming distance minimization uniformly recovers all unstructured signals with Euclidean norm bounded away from zero and infinity to the error $\\mathcal{O}((n/m)\\log(m/n))$, and $\\mathcal{O}((k/m)\\log(mn/k^2))$ when restricting to $k$-sparse signals. Both error rates are shown to be information-theoretically optimal, up to a logarithmic factor. Intriguingly, the optimal rate for sparse recovery matches that of 1-bit compressed sensing, suggesting that the phase information is non-essential for 1-bit compressed sensing. We also develop efficient algorithms for 1-bit (sparse) phase retrieval that can achieve these error rates. Specifically, we prove that (thresholded) gradient descent with respect to the one-sided $\\ell_1$-loss, when initialized via spectral methods, converges linearly and attains the near optimal reconstruction error, with sample complexity $\\mathcal{O}(n)$ for unstructured signals and $\\mathcal{O}(k^2\\log(n)\\log^2(m/k))$ for $k$-sparse signals. Our proof is based upon the observation that a certain local (restricted) approximate invertibility condition is respected by Gaussian measurements. To show this, we utilize a delicate covering argument and derive tight concentration bounds for the directional gradients by properly conditioning on the index set of phaseless hyperplane separations, which may be of independent interests and useful for other related problems."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Cryptanalysis of the SIMON Cypher Using Neo4j", "authors": "Jonathan Cook, Sabih ur Rehman, M. Arif Khan", "subjects": "Subjects:\nCryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Information Retrieval (cs.IR)", "abstract": "The exponential growth in the number of Internet of Things (IoT) devices has seen the introduction of several Lightweight Encryption Algorithms (LEA). While LEAs are designed to enhance the integrity, privacy and security of data collected and transmitted by IoT devices, it is hazardous to assume that all LEAs are secure and exhibit similar levels of protection. To improve encryption strength, cryptanalysts and algorithm designers routinely probe LEAs using various cryptanalysis techniques to identify vulnerabilities and limitations of LEAs. Despite recent improvements in the efficiency of cryptanalysis utilising heuristic methods and a Partial Difference Distribution Table (PDDT), the process remains inefficient, with the random nature of the heuristic inhibiting reproducible results. However, the use of a PDDT presents opportunities to identify relationships between differentials utilising knowledge graphs, leading to the identification of efficient paths throughout the PDDT. This paper introduces the novel use of knowledge graphs to identify intricate relationships between differentials in the SIMON LEA, allowing for the identification of optimal paths throughout the differentials, and increasing the effectiveness of the differential security analyses of SIMON."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          All in One Framework for Multimodal Re-identification in the Wild", "authors": "He Li, Mang Ye, Ming Zhang, Bo Du", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "In Re-identification (ReID), recent advancements yield noteworthy progress in both unimodal and cross-modal retrieval tasks. However, the challenge persists in developing a unified framework that could effectively handle varying multimodal data, including RGB, infrared, sketches, and textual information. Additionally, the emergence of large-scale models shows promising performance in various vision tasks but the foundation model in ReID is still blank. In response to these challenges, a novel multimodal learning paradigm for ReID is introduced, referred to as All-in-One (AIO), which harnesses a frozen pre-trained big model as an encoder, enabling effective multimodal retrieval without additional fine-tuning. The diverse multimodal data in AIO are seamlessly tokenized into a unified space, allowing the modality-shared frozen encoder to extract identity-consistent features comprehensively across all modalities. Furthermore, a meticulously crafted ensemble of cross-modality heads is designed to guide the learning trajectory. AIO is the \\textbf{first} framework to perform all-in-one ReID, encompassing four commonly used modalities. Experiments on cross-modal and multimodal ReID reveal that AIO not only adeptly handles various modal data but also excels in challenging contexts, showcasing exceptional performance in zero-shot and domain generalization scenarios."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Off-Road Autonomy Validation Using Scalable Digital Twin Simulations Within High-Performance Computing Clusters", "authors": "Tanmay Vilas Samak, Chinmay Vilas Samak, Joey Binz, Jonathon Smereka, Mark Brudnak, David Gorsich, Feng Luo, Venkat Krovi", "subjects": "Subjects:\nRobotics (cs.RO); Distributed, Parallel, and Cluster Computing (cs.DC)", "abstract": "Off-road autonomy validation presents unique challenges due to the unpredictable and dynamic nature of off-road environments. Traditional methods focusing on sequentially sweeping across the parameter space for variability analysis struggle to comprehensively assess the performance and safety of off-road autonomous systems within the imposed time constraints. This paper proposes leveraging scalable digital twin simulations within high-performance computing (HPC) clusters to address this challenge. By harnessing the computational power of HPC clusters, our approach aims to provide a scalable and efficient means to validate off-road autonomy algorithms, enabling rapid iteration and testing of autonomy algorithms under various conditions. We demonstrate the effectiveness of our framework through performance evaluations of the HPC cluster in terms of simulation parallelization and present the systematic variability analysis of a candidate off-road autonomy algorithm to identify potential vulnerabilities in the autonomy stack's perception, planning and control modules."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          SVD-AE: Simple Autoencoders for Collaborative Filtering", "authors": "Seoyoung Hong, Jeongwhan Choi, Yeon-Chang Lee, Srijan Kumar, Noseong Park", "subjects": "Subjects:\nInformation Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Collaborative filtering (CF) methods for recommendation systems have been extensively researched, ranging from matrix factorization and autoencoder-based to graph filtering-based methods. Recently, lightweight methods that require almost no training have been recently proposed to reduce overall computation. However, existing methods still have room to improve the trade-offs among accuracy, efficiency, and robustness. In particular, there are no well-designed closed-form studies for \\emph{balanced} CF in terms of the aforementioned trade-offs. In this paper, we design SVD-AE, a simple yet effective singular vector decomposition (SVD)-based linear autoencoder, whose closed-form solution can be defined based on SVD for CF. SVD-AE does not require iterative training processes as its closed-form solution can be calculated at once. Furthermore, given the noisy nature of the rating matrix, we explore the robustness against such noisy interactions of existing CF methods and our SVD-AE. As a result, we demonstrate that our simple design choice based on truncated SVD can be used to strengthen the noise robustness of the recommendation while improving efficiency. Code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          AttacKG+:Boosting Attack Knowledge Graph Construction with Large Language Models", "authors": "Yongheng Zhang, Tingwen Du, Yunshan Ma, Xiang Wang, Yi Xie, Guozheng Yang, Yuliang Lu, Ee-Chien Chang", "subjects": "Subjects:\nCryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "abstract": "Attack knowledge graph construction seeks to convert textual cyber threat intelligence (CTI) reports into structured representations, portraying the evolutionary traces of cyber attacks. Even though previous research has proposed various methods to construct attack knowledge graphs, they generally suffer from limited generalization capability to diverse knowledge types as well as requirement of expertise in model design and tuning. Addressing these limitations, we seek to utilize Large Language Models (LLMs), which have achieved enormous success in a broad range of tasks given exceptional capabilities in both language understanding and zero-shot task fulfillment. Thus, we propose a fully automatic LLM-based framework to construct attack knowledge graphs named: AttacKG+. Our framework consists of four consecutive modules: rewriter, parser, identifier, and summarizer, each of which is implemented by instruction prompting and in-context learning empowered by LLMs. Furthermore, we upgrade the existing attack knowledge schema and propose a comprehensive version. We represent a cyber attack as a temporally unfolding event, each temporal step of which encapsulates three layers of representation, including behavior graph, MITRE TTP labels, and state summary. Extensive evaluation demonstrates that: 1) our formulation seamlessly satisfies the information needs in threat event analysis, 2) our construction framework is effective in faithfully and accurately extracting the information defined by AttacKG+, and 3) our attack graph directly benefits downstream security practices such as attack reconstruction. All the code and datasets will be released upon acceptance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Conditional Local Feature Encoding for Graph Neural Networks", "authors": "Yongze Wang, Haimin Zhang, Qiang Wu, Min Xu", "subjects": "Subjects:\nMachine Learning (cs.LG); Social and Information Networks (cs.SI)", "abstract": "Graph neural networks (GNNs) have shown great success in learning from graph-based data. The key mechanism of current GNNs is message passing, where a node's feature is updated based on the information passing from its local neighbourhood. A limitation of this mechanism is that node features become increasingly dominated by the information aggregated from the neighbourhood as we use more rounds of message passing. Consequently, as the GNN layers become deeper, adjacent node features tends to be similar, making it more difficult for GNNs to distinguish adjacent nodes, thereby, limiting the performance of GNNs. In this paper, we propose conditional local feature encoding (CLFE) to help prevent the problem of node features being dominated by the information from local neighbourhood. The idea of our method is to extract the node hidden state embedding from message passing process and concatenate it with the nodes feature from previous stage, then we utilise linear transformation to form a CLFE based on the concatenated vector. The CLFE will form the layer output to better preserve node-specific information, thus help to improve the performance of GNN models. To verify the feasibility of our method, we conducted extensive experiments on seven benchmark datasets for four graph domain tasks: super-pixel graph classification, node classification, link prediction, and graph regression. The experimental results consistently demonstrate that our method improves model performance across a variety of baseline GNN models for all four tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "authors": "Chu Fei Luo, Ahmad Ghawanmeh, Xiaodan Zhu, Faiza Khan Khattak", "subjects": "Subjects:\nComputation and Language (cs.CL); Machine Learning (cs.LG)", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also learn social biases, which has a significant potential for societal harm. There have been many mitigation strategies proposed for LLM safety, but it is unclear how effective they are for eliminating social biases. In this work, we propose a new methodology for attacking language models with knowledge graph augmented generation. We refactor natural language stereotypes into a knowledge graph, and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models. We find our method increases bias in all models, even those trained with safety guardrails. This demonstrates the need for further research in AI safety, and further work in this new adversarial space."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Communication-efficient and Differentially-private Distributed Nash Equilibrium Seeking with Linear Convergence", "authors": "Xiaomeng Chen, Wei Huo, Kemi Ding, Subhrakanti Dey, Ling Shi", "subjects": "Subjects:\nSystems and Control (eess.SY); Computer Science and Game Theory (cs.GT)", "abstract": "The distributed computation of a Nash equilibrium (NE) for non-cooperative games is gaining increased attention recently. Due to the nature of distributed systems, privacy and communication efficiency are two critical concerns. Traditional approaches often address these critical concerns in isolation. This work introduces a unified framework, named CDP-NES, designed to improve communication efficiency in the privacy-preserving NE seeking algorithm for distributed non-cooperative games over directed graphs. Leveraging both general compression operators and the noise adding mechanism, CDP-NES perturbs local states with Laplacian noise and applies difference compression prior to their exchange among neighbors. We prove that CDP-NES not only achieves linear convergence to a neighborhood of the NE in games with restricted monotone mappings but also guarantees $\\epsilon$-differential privacy, addressing privacy and communication efficiency simultaneously. Finally, simulations are provided to illustrate the effectiveness of the proposed method."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Honeyfile Camouflage: Hiding Fake Files in Plain Sight", "authors": "Roelien C. Timmer, David Liebowitz, Surya Nepal, Salil S. Kanhere", "subjects": "Subjects:\nCryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "abstract": "Honeyfiles are a particularly useful type of honeypot: fake files deployed to detect and infer information from malicious behaviour. This paper considers the challenge of naming honeyfiles so they are camouflaged when placed amongst real files in a file system. Based on cosine distances in semantic vector spaces, we develop two metrics for filename camouflage: one based on simple averaging and one on clustering with mixture fitting. We evaluate and compare the metrics, showing that both perform well on a publicly available GitHub software repository dataset."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Multi-Label Out-of-Distribution Detection with Spectral Normalized Joint Energy", "authors": "Yihan Mei, Xinyu Wang, Dell Zhang, Xiaoling Wang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "In today's interconnected world, achieving reliable out-of-distribution (OOD) detection poses a significant challenge for machine learning models. While numerous studies have introduced improved approaches for multi-class OOD detection tasks, the investigation into multi-label OOD detection tasks has been notably limited. We introduce Spectral Normalized Joint Energy (SNoJoE), a method that consolidates label-specific information across multiple labels through the theoretically justified concept of an energy-based function. Throughout the training process, we employ spectral normalization to manage the model's feature space, thereby enhancing model efficacy and generalization, in addition to bolstering robustness. Our findings indicate that the application of spectral normalization to joint energy scores notably amplifies the model's capability for OOD detection. We perform OOD detection experiments utilizing PASCAL-VOC as the in-distribution dataset and ImageNet-22K or Texture as the out-of-distribution datasets. Our experimental results reveal that, in comparison to prior top performances, SNoJoE achieves 11% and 54% relative reductions in FPR95 on the respective OOD datasets, thereby defining the new state of the art in this field of study."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Large Language Models for Cyber Security: A Systematic Literature Review", "authors": "HanXiang Xu, ShenAo Wang, Ningke Li, Yanjie Zhao, Kai Chen, Kailong Wang, Yang Liu, Ting Yu, HaoYu Wang", "subjects": "Subjects:\nCryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "abstract": "The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in various domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity (LLM4Security). By comprehensively collecting over 30K relevant papers and systematically analyzing 127 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain. Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to a wide range of cybersecurity tasks, including vulnerability detection, malware analysis, network intrusion detection, and phishing detection. Second, we find that the datasets used for training and evaluating LLMs in these tasks are often limited in size and diversity, highlighting the need for more comprehensive and representative datasets. Third, we identify several promising techniques for adapting LLMs to specific cybersecurity domains, such as fine-tuning, transfer learning, and domain-specific pre-training. Finally, we discuss the main challenges and opportunities for future research in LLM4Security, including the need for more interpretable and explainable models, the importance of addressing data privacy and security concerns, and the potential for leveraging LLMs for proactive defense and threat hunting. Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Nearly-Optimal Consensus Tolerating Adaptive Omissions: Why is a Lot of Randomness is Needed?", "authors": "Mohammad T. Hajiaghayi, Dariusz R. Kowalski, Jan Olkowski", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC); Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS)", "abstract": "We study the problem of reaching agreement in a synchronous distributed system by $n$ autonomous parties, when the communication links from/to faulty parties can omit messages. The faulty parties are selected and controlled by an adaptive, full-information, computationally unbounded adversary. We design a randomized algorithm that works in $O(\\sqrt{n}\\log^2 n)$ rounds and sends $O(n^2\\log^3 n)$ communication bits, where the number of faulty parties is $\\Theta(n)$. Our result is simultaneously tight for both these measures within polylogarithmic factors: due to the $\\Omega(n^2)$ lower bound on communication by Abraham et al. (PODC'19) and $\\Omega(\\sqrt{n/\\log n})$ lower bound on the number of rounds by Bar-Joseph and Ben-Or (PODC'98). We also quantify how much randomness is necessary and sufficient to reduce time complexity to a certain value, while keeping the communication complexity (nearly) optimal. We prove that no MC algorithm can work in less than $\\Omega(\\frac{n^2}{\\max\\{R,n\\}\\log n})$ rounds if it uses less than $O(R)$ calls to a random source, assuming a constant fraction of faulty parties. This can be contrasted with a long line of work on consensus against an {\\em adversary limited to polynomial computation time}, thus unable to break cryptographic primitives, culminating in a work by Ghinea et al. (EUROCRYPT'22), where an optimal $O(r)$-round solution with probability $1-(cr)^{-r}$ is given. Our lower bound strictly separates these two regimes, by excluding such results if the adversary is computationally unbounded. On the upper bound side, we show that for $R\\in\\tilde{O}(n^{3/2})$ there exists an algorithm solving consensus in $\\tilde{O}(\\frac{n^2}{R})$ rounds with high probability, where tilde notation hides a polylogarithmic factor. The communication complexity of the algorithm does not depend on the amount of randomness $R$ and stays optimal within polylogarithmic factor."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          When Foresight Pruning Meets Zeroth-Order Optimization: Efficient Federated Learning for Low-Memory Devices", "authors": "Pengyu Zhang, Yingjie Liu, Yingbo Zhou, Xiao Du, Xian Wei, Ting Wang, Mingsong Chen", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)", "abstract": "Although Federated Learning (FL) enables collaborative learning in Artificial Intelligence of Things (AIoT) design, it fails to work on low-memory AIoT devices due to its heavy memory usage. To address this problem, various federated pruning methods are proposed to reduce memory usage during inference. However, few of them can substantially mitigate the memory burdens during pruning and training. As an alternative, zeroth-order or backpropagation-free (BP-Free) methods can partially alleviate the memory consumption, but they suffer from scaling up and large computation overheads, since the gradient estimation error and floating point operations (FLOPs) increase as the dimensionality of the model parameters grows. In this paper, we propose a federated foresight pruning method based on Neural Tangent Kernel (NTK), which can seamlessly integrate with federated BP-Free training frameworks. We present an approximation to the computation of federated NTK by using the local NTK matrices. Moreover, we demonstrate that the data-free property of our method can substantially reduce the approximation error in extreme data heterogeneity scenarios. Since our approach improves the performance of the vanilla BP-Free method with fewer FLOPs and truly alleviates memory pressure during training and inference, it makes FL more friendly to low-memory devices. Comprehensive experimental results obtained from simulation- and real test-bed-based platforms show that our federated foresight-pruning method not only preserves the ability of the dense model with a memory reduction up to 9x but also boosts the performance of the vanilla BP-Free method with dramatically fewer FLOPs."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Test-Time Augmentation for Traveling Salesperson Problem", "authors": "Ryo Ishiyama, Takahiro Shirakawa, Seiichi Uchida, Shinnosuke Matsuo", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "We propose Test-Time Augmentation (TTA) as an effective technique for addressing combinatorial optimization problems, including the Traveling Salesperson Problem. In general, deep learning models possessing the property of invariance, where the output is uniquely determined regardless of the node indices, have been proposed to learn graph structures efficiently. In contrast, we interpret the permutation of node indices, which exchanges the elements of the distance matrix, as a TTA scheme. The results demonstrate that our method is capable of obtaining shorter solutions than the latest models. Furthermore, we show that the probability of finding a solution closer to an exact solution increases depending on the augmentation size."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches", "authors": "Qing Yu, Mikihiro Tanaka, Kent Fujiwara", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "To build a cross-modal latent space between 3D human motion and language, acquiring large-scale and high-quality human motion data is crucial. However, unlike the abundance of image data, the scarcity of motion data has limited the performance of existing motion-language models. To counter this, we introduce \"motion patches\", a new representation of motion sequences, and propose using Vision Transformers (ViT) as motion encoders via transfer learning, aiming to extract useful knowledge from the image domain and apply it to the motion domain. These motion patches, created by dividing and sorting skeleton joints based on body parts in motion sequences, are robust to varying skeleton structures, and can be regarded as color image patches in ViT. We find that transfer learning with pre-trained weights of ViT obtained through training with 2D image data can boost the performance of motion analysis, presenting a promising direction for addressing the issue of limited motion data. Our extensive experiments show that the proposed motion patches, used jointly with ViT, achieve state-of-the-art performance in the benchmarks of text-to-motion retrieval, and other novel challenging tasks, such as cross-skeleton recognition, zero-shot motion classification, and human interaction recognition, which are currently impeded by the lack of data."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Hypergraph-enhanced Dual Semi-supervised Graph Classification", "authors": "Wei Ju, Zhengyang Mao, Siyu Yi, Yifang Qin, Yiyang Gu, Zhiping Xiao, Yifan Wang, Xiao Luo, Ming Zhang", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Social and Information Networks (cs.SI)", "abstract": "In this paper, we study semi-supervised graph classification, which aims at accurately predicting the categories of graphs in scenarios with limited labeled graphs and abundant unlabeled graphs. Despite the promising capability of graph neural networks (GNNs), they typically require a large number of costly labeled graphs, while a wealth of unlabeled graphs fail to be effectively utilized. Moreover, GNNs are inherently limited to encoding local neighborhood information using message-passing mechanisms, thus lacking the ability to model higher-order dependencies among nodes. To tackle these challenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classification, which captures graph semantics from the perspective of the hypergraph and the line graph, respectively. Specifically, to better explore the higher-order relationships among nodes, we design a hypergraph structure learning to adaptively learn complex node dependencies beyond pairwise relations. Meanwhile, based on the learned hypergraph, we introduce a line graph to capture the interaction between hyperedges, thereby better mining the underlying semantic structures. Finally, we develop a relational consistency learning to facilitate knowledge transfer between the two branches and provide better mutual guidance. Extensive experiments on real-world graph datasets verify the effectiveness of the proposed method against existing state-of-the-art methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Determining Recoverable Consensus Numbers", "authors": "Sean Ovens", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC)", "abstract": "Herlihy's wait-free consensus hierarchy classifies the power of object types in asynchronous shared memory systems where processes can permanently crash (i.e. stop taking steps). In this hierarchy, a type has consensus number $n$ if objects of that type can be used along with (read/write) registers to solve consensus among $n$ processes that can permanently crash, but not among $n+1$ or more processes. In systems where processes can recover after crashing, the power of an object type to solve consensus may be different. Golab's recoverable consensus hierarchy classifies the power of object types in such a system. In the recoverable consensus hierarchy, a type has recoverable consensus number $n$ if objects of that type can be used along with registers to solve consensus among $n$ processes that can recover after crashing, but not among $n+1$ or more processes. In this paper, we prove that the recoverable consensus hierarchy of deterministic, readable types is robust, i.e., if consensus can be solved among $n$ processes that can recover after crashing using a collection of objects of deterministic, readable types, then one of these types has recoverable consensus number at least $n$. This is important for comparing the relative computational power of different deterministic, readable types, because it implies that one cannot combine various objects to obtain an algorithm that is better at solving recoverable consensus than any of the individual object types. Our result can be used to show that, for all $n \\geq 4$, there exists a readable type with consensus number $n$ and recoverable consensus number $n-2$. We also show that, for all $n > n' \\geq 1$, there exists a non-readable type that has consensus number $n$ and recoverable consensus number $n'$."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Chain of Thoughtlessness: An Analysis of CoT in Planning", "authors": "Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati", "subjects": "Subjects:\nArtificial Intelligence (cs.AI)", "abstract": "Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated by modifying prompts to include examples with chains of thought--demonstrations of solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examine the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations and depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially because of the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Empathy Through Multimodality in Conversational Interfaces", "authors": "Mahyar Abbasian, Iman Azimi, Mohammad Feli, Amir M. Rahmani, Ramesh Jain", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Agents represent one of the most emerging applications of Large Language Models (LLMs) and Generative AI, with their effectiveness hinging on multimodal capabilities to navigate complex user environments. Conversational Health Agents (CHAs), a prime example of this, are redefining healthcare by offering nuanced support that transcends textual analysis to incorporate emotional intelligence. This paper introduces an LLM-based CHA engineered for rich, multimodal dialogue-especially in the realm of mental health support. It adeptly interprets and responds to users' emotional states by analyzing multimodal cues, thus delivering contextually aware and empathetically resonant verbal responses. Our implementation leverages the versatile openCHA framework, and our comprehensive evaluation involves neutral prompts expressed in diverse emotional tones: sadness, anger, and joy. We evaluate the consistency and repeatability of the planning capability of the proposed CHA. Furthermore, human evaluators critique the CHA's empathic delivery, with findings revealing a striking concordance between the CHA's outputs and evaluators' assessments. These results affirm the indispensable role of vocal (soon multimodal) emotion recognition in strengthening the empathetic connection built by CHAs, cementing their place at the forefront of interactive, compassionate digital health solutions."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          CourseGPT-zh: an Educational Large Language Model Based on Knowledge Distillation Incorporating Prompt Optimization", "authors": "Zheyan Qu, Lu Yin, Zitong Yu, Wenbo Wang, Xing zhang", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Large language models (LLMs) have demonstrated astonishing capabilities in natural language processing (NLP) tasks, sparking interest in their application to professional domains with higher specialized requirements. However, restricted access to closed-source LLMs via APIs and the difficulty in collecting massive high-quality datasets pose obstacles to the development of large language models in education fields of various courses. Given these challenges, we propose CourseGPT-zh, a course-oriented education LLM that supports customization and low-cost deployment. To address the comprehensiveness and diversity requirements of course-specific corpora, we design a high-quality question-answering corpus distillation framework incorporating prompt optimization, which effectively mines textbook knowledge and enhances its diversity. Moreover, considering the alignment of LLM responses with user needs, a novel method for discrete prompt optimization based on LLM-as-Judge is introduced. During optimization, this framework leverages the LLM's ability to reflect on and exploit error feedback and patterns, allowing for prompts that meet user needs and preferences while saving response length. Lastly, we obtain CourseGPT-zh based on the open-source LLM using parameter-efficient fine-tuning. Experimental results show that our discrete prompt optimization framework effectively improves the response quality of ChatGPT, and CourseGPT-zh exhibits strong professional capabilities in specialized knowledge question-answering, significantly outperforming comparable open-source models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Dual-Image Enhanced CLIP for Zero-Shot Anomaly Detection", "authors": "Zhaoxiang Zhang, Hanqiu Deng, Jinan Bao, Xingyu Li", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Image Anomaly Detection has been a challenging task in Computer Vision field. The advent of Vision-Language models, particularly the rise of CLIP-based frameworks, has opened new avenues for zero-shot anomaly detection. Recent studies have explored the use of CLIP by aligning images with normal and prompt descriptions. However, the exclusive dependence on textual guidance often falls short, highlighting the critical importance of additional visual references. In this work, we introduce a Dual-Image Enhanced CLIP approach, leveraging a joint vision-language scoring system. Our methods process pairs of images, utilizing each as a visual reference for the other, thereby enriching the inference process with visual context. This dual-image strategy markedly enhanced both anomaly classification and localization performances. Furthermore, we have strengthened our model with a test-time adaptation module that incorporates synthesized anomalies to refine localization capabilities. Our approach significantly exploits the potential of vision-language joint anomaly detection and demonstrates comparable performance with current SOTA methods across various datasets."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          GoalGrasp: Grasping Goals in Partially Occluded Scenarios without Grasp Training", "authors": "Shun Gui, Yan Luximon", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "We present GoalGrasp, a simple yet effective 6-DOF robot grasp pose detection method that does not rely on grasp pose annotations and grasp training. Our approach enables user-specified object grasping in partially occluded scenes. By combining 3D bounding boxes and simple human grasp priors, our method introduces a novel paradigm for robot grasp pose detection. First, we employ a 3D object detector named RCV, which requires no 3D annotations, to achieve rapid 3D detection in new scenes. Leveraging the 3D bounding box and human grasp priors, our method achieves dense grasp pose detection. The experimental evaluation involves 18 common objects categorized into 7 classes based on shape. Without grasp training, our method generates dense grasp poses for 1000 scenes. We compare our method's grasp poses to existing approaches using a novel stability metric, demonstrating significantly higher grasp pose stability. In user-specified robot grasping experiments, our approach achieves a 94% grasp success rate. Moreover, in user-specified grasping experiments under partial occlusion, the success rate reaches 92%."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Impact of Human Aspects on the Interactions Between Software Developers and End-Users in Software Engineering: A Systematic Literature Review", "authors": "Hashini Gunatilake, John Grundy, Rashina Hoda, Ingo Mueller", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "Context: Research on human aspects within the field of software engineering (SE) has been steadily gaining prominence in recent years. These human aspects have a significant impact on SE due to the inherently interactive and collaborative nature of the discipline. Objective: In this paper, we present a systematic literature review (SLR) on human aspects affecting developer-user interactions. The objective of this SLR is to plot the current landscape of primary studies by examining the human aspects that influence developer-user interactions, their implications, interrelationships, and how existing studies address these implications. Method: We conducted this SLR following the guidelines proposed by Kitchenham et al. We performed a comprehensive search in six digital databases, and an exhaustive backward and forward snowballing process. We selected 46 primary studies for data extraction. Results: We identified various human aspects affecting developer-user interactions in SE, assessed their interrelationships, identified their positive impacts and mitigation strategies for negative effects. We present specific recommendations derived from the identified research gaps. Conclusion: Our findings suggest the importance of leveraging positive effects and addressing negative effects in developer-user interactions through the implementation of effective mitigation strategies. These insights may benefit software practitioners for effective user interactions, and the recommendations proposed by this SLR may aid the research community in further human aspects related studies."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DiffMatch: Visual-Language Guidance Makes Better Semi-supervised Change Detector", "authors": "Kaiyu Li, Xiangyong Cao, Yupeng Deng, Deyu Meng", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Change Detection (CD) aims to identify pixels with semantic changes between images. However, annotating massive numbers of pixel-level images is labor-intensive and costly, especially for multi-temporal images, which require pixel-wise comparisons by human experts. Considering the excellent performance of visual language models (VLMs) for zero-shot, open-vocabulary, etc. with prompt-based reasoning, it is promising to utilize VLMs to make better CD under limited labeled data. In this paper, we propose a VLM guidance-based semi-supervised CD method, namely DiffMatch. The insight of DiffMatch is to synthesize free change labels using VLMs to provide additional supervision signals for unlabeled data. However, almost all current VLMs are designed for single-temporal images and cannot be directly applied to bi- or multi-temporal images. Motivated by this, we first propose a VLM-based mixed change event generation (CEG) strategy to yield pseudo labels for unlabeled CD data. Since the additional supervised signals provided by these VLM-driven pseudo labels may conflict with the pseudo labels from the consistency regularization paradigm (e.g. FixMatch), we propose the dual projection head for de-entangling different signal sources. Further, we explicitly decouple the bi-temporal images semantic representation through two auxiliary segmentation decoders, which are also guided by VLM. Finally, to make the model more adequately capture change representations, we introduce metric-aware supervision by feature-level contrastive loss in auxiliary branches. Extensive experiments show the advantage of DiffMatch. For instance, DiffMatch improves the FixMatch baseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In addition, our CEG strategy, in an un-supervised manner, can achieve performance far superior to state-of-the-art un-supervised CD methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Impact of Perceived Tone, Age, and Gender on Voice Assistant Persuasiveness in the Context of Product Recommendations", "authors": "Sabid Bin Habib Pias, Ran Huang, Donald Williamson, Minjeong Kim, Apu Kapadia", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC)", "abstract": "Voice Assistants (VAs) can assist users in various everyday tasks, but many users are reluctant to rely on VAs for intricate tasks like online shopping. This study aims to examine whether the vocal characteristics of VAs can serve as an effective tool to persuade users and increase user engagement with VAs in online shopping. Prior studies have demonstrated that the perceived tone, age, and gender of a voice influence the perceived persuasiveness of the speaker in interpersonal interactions. Furthermore, persuasion in product communication has been shown to affect purchase decisions in online shopping. We investigate whether variations in a VA voice's perceived tone, age, and gender characteristics can persuade users, and ultimately affect their purchase decisions. Our experimental study showed that participants were more persuaded to make purchase decisions by VA voices having positive or neutral tones as well as middle-aged male or younger female voices. Our results suggest that VA designers should offer users the ability to easily customize VA voices with a range of tones, ages, and genders. This customization can enhance user comfort and enjoyment, potentially leading to higher engagement with VAs. Additionally, we discuss the boundaries of ethical persuasion, emphasizing the importance of safeguarding users' interests against unwarranted manipulation."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Zero-shot LLM-guided Counterfactual Generation for Text", "authors": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets. Collecting and annotating such datasets for counterfactual generation is labor intensive and therefore, infeasible in practice. Therefore, in this work, we focus on a novel problem setting: \\textit{zero-shot counterfactual generation}. To this end, we propose a structured way to utilize large language models (LLMs) as general purpose counterfactual example generators. We hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero-shot manner, without requiring any training or fine-tuning. Through comprehensive experiments on various downstream tasks in natural language processing (NLP), we demonstrate the efficacy of LLMs as zero-shot counterfactual generators in evaluating and explaining black-box NLP models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Variational Schr\\\"odinger Diffusion Models", "authors": "Wei Deng, Weijian Luo, Yixin Tan, Marin Bilo\u0161, Yu Chen, Yuriy Nevmyvaka, Ricky T. Q. Chen", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "Schr\u00f6dinger bridge (SB) has emerged as the go-to method for optimizing transportation plans in diffusion models. However, SB requires estimating the intractable forward score functions, inevitably resulting in the costly implicit training loss based on simulated trajectories. To improve the scalability while preserving efficient transportation plans, we leverage variational inference to linearize the forward score functions (variational scores) of SB and restore simulation-free properties in training backward scores. We propose the variational Schr\u00f6dinger diffusion model (VSDM), where the forward process is a multivariate diffusion and the variational scores are adaptively optimized for efficient transport. Theoretically, we use stochastic approximation to prove the convergence of the variational scores and show the convergence of the adaptively generated samples based on the optimal variational scores. Empirically, we test the algorithm in simulated examples and observe that VSDM is efficient in generations of anisotropic shapes and yields straighter sample trajectories compared to the single-variate diffusion. We also verify the scalability of the algorithm in real-world data and achieve competitive unconditional generation performance in CIFAR10 and conditional generation in time series modeling. Notably, VSDM no longer depends on warm-up initializations and has become tuning-friendly in training large-scale experiments."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control", "authors": "Yide Shentu, Philipp Wu, Aravind Rajeswaran, Pieter Abbeel", "subjects": "Subjects:\nRobotics (cs.RO); Artificial Intelligence (cs.AI)", "abstract": "Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. \\method~uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that \\method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DeepDamageNet: A two-step deep-learning model for multi-disaster building damage segmentation and classification using satellite imagery", "authors": "Irene Alisjahbana, Jiawei Li, Ben (Mullet)Strong, Yue Zhang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "Satellite imagery has played an increasingly important role in post-disaster building damage assessment. Unfortunately, current methods still rely on manual visual interpretation, which is often time-consuming and can cause very low accuracy. To address the limitations of manual interpretation, there has been a significant increase in efforts to automate the process. We present a solution that performs the two most important tasks in building damage assessment, segmentation and classification, through deep-learning models. We show our results submitted as part of the xView2 Challenge, a competition to design better models for identifying buildings and their damage level after exposure to multiple kinds of natural disasters. Our best model couples a building identification semantic segmentation convolutional neural network (CNN) to a building damage classification CNN, with a combined F1 score of 0.66, surpassing the xView2 challenge baseline F1 score of 0.28. We find that though our model was able to identify buildings with relatively high accuracy, building damage classification across various disaster types is a difficult task due to the visual similarity between different damage levels and different damage distribution between disaster types, highlighting the fact that it may be important to have a probabilistic prior estimate regarding disaster damage in order to obtain accurate predictions."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Blockchains for Internet of Things: Fundamentals, Applications, and Challenges", "authors": "Yusen Wu, Ye Hu, Mingzhe Chen, Yelena Yesha, M\u00e9rouane Debbah", "subjects": "Subjects:\nCryptography and Security (cs.CR); Networking and Internet Architecture (cs.NI)", "abstract": "Internet of Things (IoT) services necessitate the storage, transmission, and analysis of diverse data for inference, autonomy, and control. Blockchains, with their inherent properties of decentralization and security, offer efficient database solutions for these devices through consensus-based data sharing. However, it's essential to recognize that not every blockchain system is suitable for specific IoT applications, and some might be more beneficial when excluded with privacy concerns. For example, public blockchains are not suitable for storing sensitive data. This paper presents a detailed review of three distinct blockchains tailored for enhancing IoT applications. We initially delve into the foundational aspects of three blockchain systems, highlighting their strengths, limitations, and implementation needs. Additionally, we discuss the security issues in different blockchains. Subsequently, we explore the blockchain's application in three pivotal IoT areas: edge AI, communications, and healthcare. We underscore potential challenges and the future directions for integrating different blockchains in IoT. Ultimately, this paper aims to offer a comprehensive perspective on the synergies between blockchains and the IoT ecosystem, highlighting the opportunities and complexities involved."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          WixUp: A General Data Augmentation Framework for Wireless Perception in Tracking of Humans", "authors": "Yin Li, Rajalakshmi Nandakumar", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI)", "abstract": "Recent advancements in wireless perception technologies, including mmWave, WiFi, and acoustics, have expanded their application in human motion tracking and health monitoring. They are promising alternatives to traditional camera-based perception systems, thanks to their efficacy under diverse conditions or occlusions, and enhanced privacy. However, the integration of deep learning within this field introduces new challenges such as the need for extensive training data and poor model generalization, especially with sparse and noisy wireless point clouds. As a remedy, data augmentation is one solution well-explored in other deep learning fields, but they are not directly applicable to the unique characteristics of wireless signals. This motivates us to propose a custom data augmentation framework, WixUp, tailored for wireless perception. Moreover, we aim to make it a general framework supporting various datasets, model architectures, sensing modalities, and tasks; while previous wireless data augmentation or generative simulations do not exhibit this generalizability, only limited to certain use cases. More specifically, WixUp can reverse-transform lossy coordinates into dense range profiles using Gaussian mixture and probability tricks, making it capable of in-depth data diversity enhancement; and its mixing-based method enables unsupervised domain adaptation via self-training, allowing training of the model with no labels from new users or environments in practice. In summary, our extensive evaluation experiments show that WixUp provides consistent performance improvement across various scenarios and outperforms the baselines."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A leadless power transfer and wireless telemetry solutions for an endovascular electrocorticography", "authors": "Zhangyu Xu, Majid Khazaee, Nhan Duy Truong, Deniel Havenga, Armin Nikpour, Arman Ahnood, Omid Kavehei", "subjects": "Subjects:\nSystems and Control (eess.SY)", "abstract": "Endovascular brain-computer interfaces (eBCIs) offer a minimally invasive way to connect the brain to external devices, merging neuroscience, engineering, and medical technology. Achieving wireless data and power transmission is crucial for the clinical viability of these implantable devices. Typically, solutions for endovascular electrocorticography (ECoG) include a sensing stent with multiple electrodes (e.g. in the superior sagittal sinus) in the brain, a subcutaneous chest implant for wireless energy harvesting and data telemetry, and a long (tens of centimetres) cable with a set of wires in between. This long cable presents risks and limitations, especially for younger patients or those with fragile vasculature. This work introduces a wireless and leadless telemetry and power transfer solution for endovascular ECoG. The proposed solution includes an optical telemetry module and a focused ultrasound (FUS) power transfer system. The proposed system can be miniaturised to fit in an endovascular stent. Our solution uses optical telemetry for high-speed data transmission (over 2 Mbit/s, capable of transmitting 41 ECoG channels at a 2 kHz sampling rate and 24-bit resolution) and the proposed power transferring scheme provides up to 10mW power budget into the site of the endovascular implants under the safety limit. Tests on bovine tissues confirmed the system's effectiveness, suggesting that future custom circuit designs could further enhance eBCI applications by removing wires and auxiliary implants, minimising complications."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Transformer Architecture for NetsDB", "authors": "Subodh Kamble, Kunal Sunil Kasodekar", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "HiRISE (High-Resolution Imaging Science Experiment) is a camera onboard the Mars Reconnaissance orbiter responsible for photographing vast areas of the Martian surface in unprecedented detail. It can capture millions of incredible closeup images in minutes. However, Mars suffers from frequent regional and local dust storms hampering this data-collection process, and pipeline, resulting in loss of effort and crucial flight time. Removing these images manually requires a large amount of manpower. I filter out these images obstructed by atmospheric dust automatically by using a Dust Image Classifier fine-tuned on Resnet-50 with an accuracy of 94.05%. To further facilitate the seamless filtering of Images I design a prediction pipeline that classifies and stores these dusty patches. I also denoise partially obstructed images using an Auto Encoder-based denoiser and Pix2Pix GAN with 0.75 and 0.99 SSIM Index respectively"}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A general error analysis for randomized low-rank approximation with application to data assimilation", "authors": "Alexandre Scotto Di Perrotolo, Youssef Diouane, Selime G\u00fcrol, Xavier Vasseur", "subjects": "Subjects:\nNumerical Analysis (math.NA); Machine Learning (stat.ML)", "abstract": "Randomized algorithms have proven to perform well on a large class of numerical linear algebra problems. Their theoretical analysis is critical to provide guarantees on their behaviour, and in this sense, the stochastic analysis of the randomized low-rank approximation error plays a central role. Indeed, several randomized methods for the approximation of dominant eigen- or singular modes can be rewritten as low-rank approximation methods. However, despite the large variety of algorithms, the existing theoretical frameworks for their analysis rely on a specific structure for the covariance matrix that is not adapted to all the algorithms. We propose a general framework for the stochastic analysis of the low-rank approximation error in Frobenius norm for centered and non-standard Gaussian matrices. Under minimal assumptions on the covariance matrix, we derive accurate bounds both in expectation and probability. Our bounds have clear interpretations that enable us to derive properties and motivate practical choices for the covariance matrix resulting in efficient low-rank approximation algorithms. The most commonly used bounds in the literature have been demonstrated as a specific instance of the bounds proposed here, with the additional contribution of being tighter. Numerical experiments related to data assimilation further illustrate that exploiting the problem structure to select the covariance matrix improves the performance as suggested by our bounds."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          General Place Recognition Survey: Towards Real-World Autonomy", "authors": "Peng Yin, Jianhao Jiao, Shiqi Zhao, Lingyun Xu, Guoquan Huang, Howie Choset, Sebastian Scherer, Jianda Han", "subjects": "Subjects:\nRobotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)", "abstract": "In the realm of robotics, the quest for achieving real-world autonomy, capable of executing large-scale and long-term operations, has positioned place recognition (PR) as a cornerstone technology. Despite the PR community's remarkable strides over the past two decades, garnering attention from fields like computer vision and robotics, the development of PR methods that sufficiently support real-world robotic systems remains a challenge. This paper aims to bridge this gap by highlighting the crucial role of PR within the framework of Simultaneous Localization and Mapping (SLAM) 2.0. This new phase in robotic navigation calls for scalable, adaptable, and efficient PR solutions by integrating advanced artificial intelligence (AI) technologies. For this goal, we provide a comprehensive review of the current state-of-the-art (SOTA) advancements in PR, alongside the remaining challenges, and underscore its broad applications in robotics. This paper begins with an exploration of PR's formulation and key research challenges. We extensively review literature, focusing on related methods on place representation and solutions to various PR challenges. Applications showcasing PR's potential in robotics, key PR datasets, and open-source libraries are discussed. We also emphasizes our open-source package, aimed at new development and benchmark for general PR. We conclude with a discussion on PR's future directions, accompanied by a summary of the literature covered and access to our open-source library, available to the robotics community at: this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Novel Technique for Query Plan Representation Based on Graph Neural Networks", "authors": "Baoming Chang, Amin Kamali, Verena Kantere", "subjects": "Subjects:\nDatabases (cs.DB); Artificial Intelligence (cs.AI)", "abstract": "Learning representations for query plans play a pivotal role in machine learning-based query optimizers of database management systems. To this end, particular model architectures are proposed in the literature to convert the tree-structured query plans into representations with formats learnable by downstream machine learning models. However, existing research rarely compares and analyzes the query plan representation capabilities of these tree models and their direct impact on the performance of the overall optimizer. To address this problem, we perform a comparative study to explore the effect of using different state-of-the-art tree models on the optimizer's cost estimation and plan selection performance in relatively complex workloads. Additionally, we explore the possibility of using graph neural networks (GNN) in the query plan representation task. We propose a novel tree model combining directed GNN with Gated Recurrent Units (GRU) and demonstrate experimentally that the new tree model provides significant improvements to cost estimation tasks and relatively excellent plan selection performance compared to the state-of-the-art tree models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Proportion Estimation by Masked Learning from Label Proportion", "authors": "Takumi Okuo, Kazuya Nishimura, Hiroaki Ito, Kazuhiro Terada, Akihiko Yoshizawa, Ryoma Bise", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "The PD-L1 rate, the number of PD-L1 positive tumor cells over the total number of all tumor cells, is an important metric for immunotherapy. This metric is recorded as diagnostic information with pathological images. In this paper, we propose a proportion estimation method with a small amount of cell-level annotation and proportion annotation, which can be easily collected. Since the PD-L1 rate is calculated from only `tumor cells' and not using `non-tumor cells', we first detect tumor cells with a detection model. Then, we estimate the PD-L1 proportion by introducing a masking technique to `learning from label proportion.' In addition, we propose a weighted focal proportion loss to address data imbalance problems. Experiments using clinical data demonstrate the effectiveness of our method. Our method achieved the best performance in comparisons."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation", "authors": "Ana Brassard, Benjamin Heinzerling, Keito Kudo, Keisuke Sakaguchi, Kentaro Inui", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Evaluating free-text explanations is a multifaceted, subjective, and labor-intensive task. Large language models (LLMs) present an appealing alternative due to their potential for consistency, scalability, and cost-efficiency. In this work, we present ACORN, a new dataset of 3,500 free-text explanations and aspect-wise quality ratings, and use it to gain insights into how LLMs evaluate explanations. We observed that replacing one of the human ratings sometimes maintained, but more often lowered the inter-annotator agreement across different settings and quality aspects, suggesting that their judgments are not always consistent with human raters. We further quantified this difference by comparing the correlation between LLM-generated ratings with majority-voted human ratings across different quality aspects. With the best system, Spearman's rank correlation ranged between 0.53 to 0.95, averaging 0.72 across aspects, indicating moderately high but imperfect alignment. Finally, we considered the alternative of using an LLM as an additional rater when human raters are scarce, and measured the correlation between majority-voted labels with a limited human pool and LLMs as an additional rater, compared to the original gold labels. While GPT-4 improved the outcome when there were only two human raters, in all other observed cases, LLMs were neutral to detrimental when there were three or more human raters. We publicly release the dataset to support future improvements in LLM-in-the-loop evaluation here: this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "authors": "Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)", "abstract": "Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          APrompt4EM: Augmented Prompt Tuning for Generalized Entity Matching", "authors": "Yikuan Xia, Jiazun Chen, Xinchi Li, Jun Gao", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)", "abstract": "Generalized Entity Matching (GEM), which aims at judging whether two records represented in different formats refer to the same real-world entity, is an essential task in data management. The prompt tuning paradigm for pre-trained language models (PLMs), including the recent PromptEM model, effectively addresses the challenges of low-resource GEM in practical applications, offering a robust solution when labeled data is scarce. However, existing prompt tuning models for GEM face the challenges of prompt design and information gap. This paper introduces an augmented prompt tuning framework for the challenges, which consists of two main improvements. The first is an augmented contextualized soft token-based prompt tuning method that extracts a guiding soft token benefit for the PLMs' prompt tuning, and the second is a cost-effective information augmentation strategy leveraging large language models (LLMs). Our approach performs well on the low-resource GEM challenges. Extensive experiments show promising advancements of our basic model without information augmentation over existing methods based on moderate-size PLMs (average 5.24%+), and our model with information augmentation achieves comparable performance compared with fine-tuned LLMs, using less than 14% of the API fee."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ATDM:An Anthropomorphic Aerial Tendon-driven Manipulator with Low-Inertia and High-Stiffness", "authors": "Quman Xu, Zhan Li, Hai Li, Xinghu Yu, Yipeng Yang", "subjects": "Subjects:\nRobotics (cs.RO); Systems and Control (eess.SY)", "abstract": "Aerial Manipulator Systems (AMS) have garnered significant interest for their utility in aerial operations. Nonetheless, challenges related to the manipulator's limited stiffness and the coupling disturbance with manipulator movement persist. This paper introduces the Aerial Tendon-Driven Manipulator (ATDM), an innovative AMS that integrates a hexrotor Unmanned Aerial Vehicle (UAV) with a 4-degree-of-freedom (4-DOF) anthropomorphic tendon-driven manipulator. The design of the manipulator is anatomically inspired, emulating the human arm anatomy from the shoulder joint downward. To enhance the structural integrity and performance, finite element topology optimization and lattice optimization are employed on the links to replicate the radially graded structure characteristic of bone, this approach effectively reduces weight and inertia while simultaneously maximizing stiffness. A novel tensioning mechanism with adjustable tension is introduced to address cable relaxation, and a Tension-amplification tendon mechanism is implemented to increase the manipulator's overall stiffness and output. The paper presents a kinematic model based on virtual coupled joints, a comprehensive workspace analysis, and detailed calculations of output torques and stiffness for individual arm joints. The prototype arm has a total weight of 2.7 kg, with the end effector contributing only 0.818 kg. By positioning all actuators at the base, coupling disturbance are minimized. The paper includes a detailed mechanical design and validates the system's performance through semi-physical multi-body dynamics simulations, confirming the efficacy of the proposed design."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Counting Cohesive Subgraphs with Hereditary Properties", "authors": "Rong-Hua Li, Xiaowei Ye, Fusheng Jin, Yu-Ping Wang, Ye Yuan, Guoren Wang", "subjects": "Subjects:\nData Structures and Algorithms (cs.DS)", "abstract": "Counting small cohesive subgraphs in a graph is a fundamental operation with numerous applications in graph analysis. Previous studies on cohesive subgraph counting are mainly based on the clique model, which aim to count the number of $k$-cliques in a graph with a small $k$. However, the clique model often proves too restrictive for practical use. To address this issue, we investigate a new problem of counting cohesive subgraphs that adhere to the hereditary property. Here the hereditary property means that if a graph $G$ has a property $\\mathcal{P}$, then any induced subgraph of $G$ also has a property $\\mathcal{P}$. To count these hereditary cohesive subgraphs (\\hcss), we propose a new listing-based framework called \\hcslist, which employs a backtracking enumeration procedure to count all \\hcss. A notable limitation of \\hcslist is that it requires enumerating all \\hcss, making it intractable for large and dense graphs due to the exponential growth in the number of \\hcss with respect to graph size. To overcome this limitation, we propose a novel pivot-based framework called \\hcspivot, which can count most \\hcss in a combinatorial manner without explicitly listing them. Two additional noteworthy features of \\hcspivot is its ability to (1) simultaneously count \\hcss of any size and (2) simultaneously count \\hcss for each vertex or each edge, while \\hcslist is only capable of counting a specific size of \\hcs and obtaining a total count of \\hcss in a graph. We focus specifically on two \\hcs: $s$-defective clique and $s$-plex, with several non-trivial pruning techniques to enhance the efficiency. We conduct extensive experiments on 8 large real-world graphs, and the results demonstrate the high efficiency and effectiveness of our solutions."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Quantum-Edge Cloud Computing: A Future Paradigm for IoT Applications", "authors": "Mohammad Ikbal Hossain, Shaharier Arafat Sumon, Habib Md. Hasan, Fatema Akter, Md Bahauddin Badhon, Mohammad Nahid Ul Islam", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "The Internet of Things (IoT) is expanding rapidly, which has created a need for sophisticated computational frameworks that can handle the data and security requirements inherent in modern IoT applications. However, traditional cloud computing frameworks have struggled with latency, scalability, and security vulnerabilities. Quantum-Edge Cloud Computing (QECC) is a new paradigm that effectively addresses these challenges by combining the computational power of quantum computing, the low-latency benefits of edge computing, and the scalable resources of cloud computing. This study has been conducted based on a published literature review, performance improvements, and metrics data from Bangladesh on smart city infrastructure, healthcare monitoring, and the industrial IoT sector. We have discussed the integration of quantum cryptography to enhance data integrity, the role of edge computing in reducing response times, and how cloud computing's resource abundance can support large IoT networks. We examine case studies, such as the use of quantum sensors in self-driving vehicles, to illustrate the real-world impact of QECC. Furthermore, the paper identifies future research directions, including developing quantum-resistant encryption and optimizing quantum algorithms for edge computing. The convergence of these technologies in QECC promises to overcome the existing limitations of IoT frameworks and set a new standard for the future of IoT applications."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution", "authors": "Shuo Shao, Yiming Li, Hongwei Yao, Yiling He, Zhan Qin, Kui Ren", "subjects": "Subjects:\nCryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Ownership verification is currently the most critical and widely adopted post-hoc method to safeguard model copyright. In general, model owners exploit it to identify whether a given suspicious third-party model is stolen from them by examining whether it has particular properties `inherited' from their released models. Currently, backdoor-based model watermarks are the primary and cutting-edge methods to implant such properties in the released models. However, backdoor-based methods have two fatal drawbacks, including harmfulness and ambiguity. The former indicates that they introduce maliciously controllable misclassification behaviors ($i.e.$, backdoor) to the watermarked released models. The latter denotes that malicious users can easily pass the verification by finding other misclassified samples, leading to ownership ambiguity. In this paper, we argue that both limitations stem from the `zero-bit' nature of existing watermarking schemes, where they exploit the status ($i.e.$, misclassified) of predictions for verification. Motivated by this understanding, we design a new watermarking paradigm, $i.e.$, Explanation as a Watermark (EaaW), that implants verification behaviors into the explanation of feature attribution instead of model predictions. Specifically, EaaW embeds a `multi-bit' watermark into the feature attribution explanation of specific trigger samples without changing the original prediction. We correspondingly design the watermark embedding and extraction algorithms inspired by explainable artificial intelligence. In particular, our approach can be used for different tasks ($e.g.$, image classification and text generation). Extensive experiments verify the effectiveness and harmlessness of our EaaW and its resistance to potential attacks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Adaptive Whole-body Robotic Tool-use Learning on Low-rigidity Plastic-made Humanoids Using Vision and Tactile Sensors", "authors": "Kento Kawaharazuka, Kei Okada, Masayuki Inaba", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "Various robots have been developed so far; however, we face challenges in modeling the low-rigidity bodies of some robots. In particular, the deflection of the body changes during tool-use due to object grasping, resulting in significant shifts in the tool-tip position and the body's center of gravity. Moreover, this deflection varies depending on the weight and length of the tool, making these models exceptionally complex. However, there is currently no control or learning method that takes all of these effects into account. In this study, we propose a method for constructing a neural network that describes the mutual relationship among joint angle, visual information, and tactile information from the feet. We aim to train this network using the actual robot data and utilize it for tool-tip control. Additionally, we employ Parametric Bias to capture changes in this mutual relationship caused by variations in the weight and length of tools, enabling us to understand the characteristics of the grasped tool from the current sensor information. We apply this approach to the whole-body tool-use on KXR, a low-rigidity plastic-made humanoid robot, to validate its effectiveness."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ChuXin: 1.6B Technical Report", "authors": "Xiaomin Zhuang, Yufan Jiang, Qiaozhi He, Zhihua Wu", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "In this report, we present ChuXin, an entirely open-source language model with a size of 1.6 billion parameters. Unlike the majority of works that only open-sourced the model weights and architecture, we have made everything needed to train a model available, including the training data, the training process, and the evaluation code. Our goal is to empower and strengthen the open research community, fostering transparency and enabling a new wave of innovation in the field of language modeling. Furthermore, we extend the context length to 1M tokens through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. The weights for both models are available at Hugging Face to download and use."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Fine-tuning Pre-trained Named Entity Recognition Models For Indian Languages", "authors": "Sankalp Bahad, Pruthwik Mishra, Karunesh Arora, Rakesh Chandra Balabantaray, Dipti Misra Sharma, Parameswari Krishnamurthy", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Named Entity Recognition (NER) is a useful component in Natural Language Processing (NLP) applications. It is used in various tasks such as Machine Translation, Summarization, Information Retrieval, and Question-Answering systems. The research on NER is centered around English and some other major languages, whereas limited attention has been given to Indian languages. We analyze the challenges and propose techniques that can be tailored for Multilingual Named Entity Recognition for Indian Languages. We present a human annotated named entity corpora of 40K sentences for 4 Indian languages from two of the major Indian language families. Additionally,we present a multilingual model fine-tuned on our dataset, which achieves an F1 score of 0.80 on our dataset on average. We achieve comparable performance on completely unseen benchmark datasets for Indian languages which affirms the usability of our model."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          FlexEControl: Flexible and Efficient Multimodal Control for Text-to-Image Generation", "authors": "Xuehai He, Jian Zheng, Jacob Zhiyuan Fang, Robinson Piramuthu, Mohit Bansal, Vicente Ordonez, Gunnar A Sigurdsson, Nanyun Peng, Xin Eric Wang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Controllable text-to-image (T2I) diffusion models generate images conditioned on both text prompts and semantic inputs of other modalities like edge maps. Nevertheless, current controllable T2I methods commonly face challenges related to efficiency and faithfulness, especially when conditioning on multiple inputs from either the same or diverse modalities. In this paper, we propose a novel Flexible and Efficient method, FlexEControl, for controllable T2I generation. At the core of FlexEControl is a unique weight decomposition strategy, which allows for streamlined integration of various input types. This approach not only enhances the faithfulness of the generated image to the control, but also significantly reduces the computational overhead typically associated with multimodal conditioning. Our approach achieves a reduction of 41% in trainable parameters and 30% in memory usage compared with Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate images under the guidance of multiple input conditions of various modalities."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Enhancing Data Integrity and Traceability in Industry Cyber Physical Systems (ICPS) through Blockchain Technology: A Comprehensive Approach", "authors": "Mohammad Ikbal Hossain, Dr. Tanja Steigner, Muhammad Imam Hussain, Afroja Akther", "subjects": "Subjects:\nCryptography and Security (cs.CR); Systems and Control (eess.SY)", "abstract": "Blockchain technology, heralded as a transformative innovation, has far-reaching implications beyond its initial application in cryptocurrencies. This study explores the potential of blockchain in enhancing data integrity and traceability within Industry Cyber-Physical Systems (ICPS), a crucial aspect in the era of Industry 4.0. ICPS, integrating computational and physical components, is pivotal in managing critical infrastructure like manufacturing, power grids, and transportation networks. However, they face challenges in security, privacy, and reliability. With its inherent immutability, transparency, and distributed consensus, blockchain presents a groundbreaking approach to address these challenges. It ensures robust data reliability and traceability across ICPS, enhancing transaction transparency and facilitating secure data sharing. This research unearths various blockchain applications in ICPS, including supply chain management, quality control, contract management, and data sharing. Each application demonstrates blockchain's capacity to streamline processes, reduce fraud, and enhance system efficiency. In supply chain management, blockchain provides real-time auditing and compliance. For quality control, it establishes tamper-proof records, boosting consumer confidence. In contract management, smart contracts automate execution, enhancing efficiency. Blockchain also fosters secure collaboration in ICPS, which is crucial for system stability and safety. This study emphasizes the need for further research on blockchain's practical implementation in ICPS, focusing on challenges like scalability, system integration, and security vulnerabilities. It also suggests examining blockchain's economic and organizational impacts in ICPS to understand its feasibility and long-term advantages."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Federated Adaptation for Foundation Model-based Recommendations", "authors": "Chunxu Zhang, Guodong Long, Hongkuan Guo, Xiao Fang, Yang Song, Zhaojie Liu, Guorui Zhou, Zijian Zhang, Yang Liu, Bo Yang", "subjects": "Subjects:\nInformation Retrieval (cs.IR)", "abstract": "With the recent success of large language models, particularly foundation models with generalization abilities, applying foundation models for recommendations becomes a new paradigm to improve existing recommendation systems. It becomes a new open challenge to enable the foundation model to capture user preference changes in a timely manner with reasonable communication and computation costs while preserving privacy. This paper proposes a novel federated adaptation mechanism to enhance the foundation model-based recommendation system in a privacy-preserving manner. Specifically, each client will learn a lightweight personalized adapter using its private data. The adapter then collaborates with pre-trained foundation models to provide recommendation service efficiently with fine-grained manners. Importantly, users' private behavioral data remains secure as it is not shared with the server. This data localization-based privacy preservation is embodied via the federated learning framework. The model can ensure that shared knowledge is incorporated into all adapters while simultaneously preserving each user's personal preferences. Experimental results on four benchmark datasets demonstrate our method's superior performance. Implementation code is available to ease reproducibility."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          xMTrans: Temporal Attentive Cross-Modality Fusion Transformer for Long-Term Traffic Prediction", "authors": "Huy Quang Ung, Hao Niu, Minh-Son Dao, Shinya Wada, Atsunori Minamikawa", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Traffic predictions play a crucial role in intelligent transportation systems. The rapid development of IoT devices allows us to collect different kinds of data with high correlations to traffic predictions, fostering the development of efficient multi-modal traffic prediction models. Until now, there are few studies focusing on utilizing advantages of multi-modal data for traffic predictions. In this paper, we introduce a novel temporal attentive cross-modality transformer model for long-term traffic predictions, namely xMTrans, with capability of exploring the temporal correlations between the data of two modalities: one target modality (for prediction, e.g., traffic congestion) and one support modality (e.g., people flow). We conducted extensive experiments to evaluate our proposed model on traffic congestion and taxi demand predictions using real-world datasets. The results showed the superiority of xMTrans against recent state-of-the-art methods on long-term traffic predictions. In addition, we also conducted a comprehensive ablation study to further analyze the effectiveness of each module in xMTrans."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Effective alpha theory certification using interval arithmetic: alpha theory over regions", "authors": "Kisun Lee", "subjects": "Subjects:\nSymbolic Computation (cs.SC); Algebraic Geometry (math.AG); Numerical Analysis (math.NA)", "abstract": "We reexamine Smale's alpha theory as a way to certify a numerical solution to an analytic system. For a given point and a system, Smale's alpha theory determines whether Newton's method applied to this point shows the quadratic convergence to an exact solution. We introduce the alpha theory computation using interval arithmetic to avoid costly exact arithmetic. As a straightforward variation of the alpha theory, our work improves computational efficiency compared to software employing the traditional alpha theory."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems", "authors": "Kai Zheng, Haijun Zhao, Rui Huang, Beichuan Zhang, Na Mou, Yanan Niu, Yang Song, Hongning Wang, Kun Gai", "subjects": "Subjects:\nInformation Retrieval (cs.IR)", "abstract": "The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems. The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility. Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system. Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper). The \\emph{selection bias} inherent in the model of each stage significantly influences the results that are ultimately presented to users. To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users. We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank. Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best complies with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output. We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms. The algorithm is proved to be effective in both retrieval and ranking stages. Since deployed, the algorithm has brought consistent and significant performance gain to the platform."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Sorting multibay block stacking storage systems", "authors": "Jakob Pfrommer, Thomas B\u00f6mer, Daniyar Akizhanov, Anne Meyer", "subjects": "Subjects:\nData Structures and Algorithms (cs.DS)", "abstract": "Autonomous mobile robots (AMRs) are increasingly used to automate operations in intralogistics. One crucial feature of AMRs is their availability, allowing them to operate 24/7. This work addresses the multibay unit load pre-marshalling problem, which extends pre-marshalling from a single bay to larger warehouse configurations with multiple bays. Pre-marshalling leverages off-peak time intervals to sort a block stacking warehouse in anticipation of future orders. These larger warehouse configurations require not only the minimization of the number of moves but also the consideration of distance or time when making sorting decisions. Our proposed solution for the multibay unit load pre-marshalling problem is based on our two-step approach that first determines the access direction for each stack and then finds a sequence of moves to sort the warehouse. In addition to adapting the existing approach that integrates a network flow model and an extended A* algorithm, we additionally present an exact constraint programming approach for the second stage of the problem-solving process. The results demonstrate that the presented solution approach effectively enhances the access time of unit loads and reduces the sorting effort for block stacking warehouses with multiple bays."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Explaining Clustering of Ecological Momentary Assessment Data Through Temporal and Feature Attention", "authors": "Mandani Ntekouli, Gerasimos Spanakis, Lourens Waldorp, Anne Roefs", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "In the field of psychopathology, Ecological Momentary Assessment (EMA) studies offer rich individual data on psychopathology-relevant variables (e.g., affect, behavior, etc) in real-time. EMA data is collected dynamically, represented as complex multivariate time series (MTS). Such information is crucial for a better understanding of mental disorders at the individual- and group-level. More specifically, clustering individuals in EMA data facilitates uncovering and studying the commonalities as well as variations of groups in the population. Nevertheless, since clustering is an unsupervised task and true EMA grouping is not commonly available, the evaluation of clustering is quite challenging. An important aspect of evaluation is clustering explainability. Thus, this paper proposes an attention-based interpretable framework to identify the important time-points and variables that play primary roles in distinguishing between clusters. A key part of this study is to examine ways to analyze, summarize, and interpret the attention weights as well as evaluate the patterns underlying the important segments of the data that differentiate across clusters. To evaluate the proposed approach, an EMA dataset of 187 individuals grouped in 3 clusters is used for analyzing the derived attention-based importance attributes. More specifically, this analysis provides the distinct characteristics at the cluster-, feature- and individual level. Such clustering explanations could be beneficial for generalizing existing concepts of mental disorders, discovering new insights, and even enhancing our knowledge at an individual level."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Pedestrian Attribute Recognition as Label-balanced Multi-label Learning", "authors": "Yibo Zhou, Hai-Miao Hu, Yirong Xiang, Xiaokang Zhang, Haotian Wu", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Rooting in the scarcity of most attributes, realistic pedestrian attribute datasets exhibit unduly skewed data distribution, from which two types of model failures are delivered: (1) label imbalance: model predictions lean greatly towards the side of majority labels; (2) semantics imbalance: model is easily overfitted on the under-represented attributes due to their insufficient semantic diversity. To render perfect label balancing, we propose a novel framework that successfully decouples label-balanced data re-sampling from the curse of attributes co-occurrence, i.e., we equalize the sampling prior of an attribute while not biasing that of the co-occurred others. To diversify the attributes semantics and mitigate the feature noise, we propose a Bayesian feature augmentation method to introduce true in-distribution novelty. Handling both imbalances jointly, our work achieves best accuracy on various popular benchmarks, and importantly, with minimal computational budget."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Guarding Force: Safety-Critical Compliant Control for Robot-Environment Interaction", "authors": "Xinming Wang, Jun Yang, Jianliang Mao, Jinzhuo Liang, Shihua Li, Yunda Yan", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "In this study, we propose a safety-critical compliant control strategy designed to strictly enforce interaction force constraints during the physical interaction of robots with unknown environments. The interaction force constraint is interpreted as a new force-constrained control barrier function (FC-CBF) by exploiting the generalized contact model and the prior information of the environment, i.e., the prior stiffness and rest position, for robot kinematics. The difference between the real environment and the generalized contact model is approximated by constructing a tracking differentiator, and its estimation error is quantified based on Lyapunov theory. By interpreting strict interaction safety specification as a dynamic constraint, restricting the desired joint angular rates in kinematics, the proposed approach modifies nominal compliant controllers using quadratic programming, ensuring adherence to interaction force constraints in unknown environments. The strict force constraint and the stability of the closed-loop system are rigorously analyzed. Experimental tests using a UR3e industrial robot with different environments verify the effectiveness of the proposed method in achieving the force constraints in unknown environments."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Concolic Testing of Quantum Programs", "authors": "Shangzhou Xia, Jianjun Zhao, Fuyuan Zhang, Xiaoyu Guo", "subjects": "Subjects:\nSoftware Engineering (cs.SE); Quantum Physics (quant-ph)", "abstract": "This paper presents the first concolic testing framework specifically designed for quantum programs. The framework defines quantum conditional statements that quantify quantum states and presents a symbolization method for quantum variables. Utilizing this framework, we generate path constraints for each concrete execution path of a quantum program. These constraints guide the exploration of new paths, with a quantum constraint solver determining the outcomes to generate novel input samples and enhance branch coverage. We implemented this framework in Python and integrated it with Qiskit for practical evaluation. Experimental results demonstrate that our concolic testing framework significantly improves branch coverage and the quality of quantum input samples, demonstrating its effectiveness and efficiency in quantum software testing."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Insights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations", "authors": "SiQi Wang, Xing Hu, Bei Wang, WenXin Yao, Xin Xia, XingYu Wang", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "With the rapid development of deep learning, the implementation of intricate algorithms and substantial data processing have become standard elements of deep learning projects. As a result, the code has become progressively complex as the software evolves, which is difficult to maintain and understand. Existing studies have investigated the impact of refactoring on software quality within traditional software. However, the insight of code refactoring in the context of deep learning is still unclear. This study endeavors to fill this knowledge gap by empirically examining the current state of code refactoring in deep learning realm, and practitioners' views on refactoring. We first manually analyzed the commit history of five popular and well-maintained deep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in historical commits and measured how different types and elements of refactoring operations are distributed and found that refactoring operation types' distribution in deep learning projects is different from it in traditional Java software. We then surveyed 159 practitioners about their views of code refactoring in deep learning projects and their expectations of current refactoring tools. The result of the survey showed that refactoring research and the development of related tools in the field of deep learning are crucial for improving project maintainability and code quality, and that current refactoring tools do not adequately meet the needs of practitioners. Lastly, we provided our perspective on the future advancement of refactoring tools and offered suggestions for developers' development practices."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Regime Learning for Differentiable Particle Filters", "authors": "John-Joseph Brady, Yuhui Luo, Wenwu Wang, Victor Elvira, Yunpeng Li", "subjects": "Subjects:\nMachine Learning (cs.LG); Signal Processing (eess.SP)", "abstract": "Differentiable particle filters are an emerging class of models that combine sequential Monte Carlo techniques with the flexibility of neural networks to perform state space inference. This paper concerns the case where the system may switch between a finite set of state-space models, i.e. regimes. No prior approaches effectively learn both the individual regimes and the switching process simultaneously. In this paper, we propose the neural network based regime learning differentiable particle filter (RLPF) to address this problem. We further design a training procedure for the RLPF and other related algorithms. We demonstrate competitive performance compared to the previous state-of-the-art algorithms on a pair of numerical experiments."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Systematic review, analysis, and characterisation of malicious industrial network traffic datasets for aiding Machine Learning algorithm performance testing", "authors": "Martin Dobler, Michael Hellwig, Nuno Lopes, Ken Oakley, Mike Winterburn", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "The adoption of the Industrial Internet of Things (IIoT) as a complementary technology to Operational Technology (OT) has enabled a new level of standardised data access and process visibility. This convergence of Information Technology (IT), OT, and IIoT has also created new cybersecurity vulnerabilities and risks that must be managed. Artificial Intelligence (AI) is emerging as a powerful tool to monitor OT/IIoT networks for malicious activity and is a highly active area of research. AI researchers are applying advanced Machine Learning (ML) and Deep Learning (DL) techniques to the detection of anomalous or malicious activity in network traffic. They typically use datasets derived from IoT/IIoT/OT network traffic captures to measure the performance of their proposed approaches. Therefore, there is a widespread need for datasets for algorithm testing. This work systematically reviews publicly available network traffic capture-based datasets, including categorisation of contained attack types, review of metadata, and statistical as well as complexity analysis. Each dataset is analysed to provide researchers with metadata that can be used to select the best dataset for their research question. This results in an added benefit to the community as researchers can select the best dataset for their research more easily and according to their specific Machine Learning goals."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Enhancing Geometric Ontology Embeddings for $\\mathcal{EL}^{++}$ with Negative Sampling and Deductive Closure Filtering", "authors": "Olga Mashkova, Fernando Zhapa-Camacho, Robert Hoehndorf", "subjects": "Subjects:\nArtificial Intelligence (cs.AI)", "abstract": "Ontology embeddings map classes, relations, and individuals in ontologies into $\\mathbb{R}^n$, and within $\\mathbb{R}^n$ similarity between entities can be computed or new axioms inferred. For ontologies in the Description Logic $\\mathcal{EL}^{++}$, several embedding methods have been developed that explicitly generate models of an ontology. However, these methods suffer from some limitations; they do not distinguish between statements that are unprovable and provably false, and therefore they may use entailed statements as negatives. Furthermore, they do not utilize the deductive closure of an ontology to identify statements that are inferred but not asserted. We evaluated a set of embedding methods for $\\mathcal{EL}^{++}$ ontologies based on high-dimensional ball representation of concept descriptions, incorporating several modifications that aim to make use of the ontology deductive closure. In particular, we designed novel negative losses that account both for the deductive closure and different types of negatives. We demonstrate that our embedding methods improve over the baseline ontology embedding in the task of knowledge base or ontology completion."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Logical Negation Augmenting and Debiasing for Prompt-based Methods", "authors": "Yitian Li, Jidong Tian, Hao He, Yaohui Jin", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)", "abstract": "Prompt-based methods have gained increasing attention on NLP and shown validity on many downstream tasks. Many works have focused on mining these methods' potential for knowledge extraction, but few explore their ability to make logical reasoning. In this work, we focus on the effectiveness of the prompt-based methods on first-order logical reasoning and find that the bottleneck lies in logical negation. Based on our analysis, logical negation tends to result in spurious correlations to negative answers, while propositions without logical negation correlate to positive answers. To solve the problem, we propose a simple but effective method, Negation Augmenting and Negation Debiasing (NAND), which introduces negative propositions to prompt-based methods without updating parameters. Specifically, these negative propositions can counteract spurious correlations by providing \"not\" for all instances so that models cannot make decisions only by whether expressions contain a logical negation. Experiments on three datasets show that NAND not only solves the problem of calibrating logical negation but also significantly enhances prompt-based methods of logical reasoning without model retraining."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration", "authors": "Emily Wong, Juan S\u00e1nchez Esquivel, Jens Emil Gr\u00f8nb\u00e6k, Germ\u00e1n Leiva, Eduardo Velloso", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC)", "abstract": "Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Critical Infrastructure Protection: Generative AI, Challenges, and Opportunities", "authors": "Yagmur Yigit, Mohamed Amine Ferrag, Iqbal H. Sarker, Leandros A. Maglaras, Christos Chrysoulas, Naghmeh Moradpoor, Helge Janicke", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "Critical National Infrastructure (CNI) encompasses a nation's essential assets that are fundamental to the operation of society and the economy, ensuring the provision of vital utilities such as energy, water, transportation, and communication. Nevertheless, growing cybersecurity threats targeting these infrastructures can potentially interfere with operations and seriously risk national security and public safety. In this paper, we examine the intricate issues raised by cybersecurity risks to vital infrastructure, highlighting these systems' vulnerability to different types of cyberattacks. We analyse the significance of trust, privacy, and resilience for Critical Infrastructure Protection (CIP), examining the diverse standards and regulations to manage these domains. We also scrutinise the co-analysis of safety and security, offering innovative approaches for their integration and emphasising the interdependence between these fields. Furthermore, we introduce a comprehensive method for CIP leveraging Generative AI and Large Language Models (LLMs), giving a tailored lifecycle and discussing specific applications across different critical infrastructure sectors. Lastly, we discuss potential future directions that promise to enhance the security and resilience of critical infrastructures. This paper proposes innovative strategies for CIP from evolving attacks and enhances comprehension of cybersecurity concerns related to critical infrastructure."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          SCALA: Split Federated Learning with Concatenated Activations and Logit Adjustments", "authors": "Jiarong Yang, Yuan Liu", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Split Federated Learning (SFL) is a distributed machine learning framework which strategically divides the learning process between a server and clients and collaboratively trains a shared model by aggregating local models updated based on data from distributed clients. However, data heterogeneity and partial client participation result in label distribution skew, which severely degrades the learning performance. To address this issue, we propose SFL with Concatenated Activations and Logit Adjustments (SCALA). Specifically, the activations from the client-side models are concatenated as the input of the server-side model so as to centrally adjust label distribution across different clients, and logit adjustments of loss functions on both server-side and client-side models are performed to deal with the label distribution variation across different subsets of participating clients. Theoretical analysis and experimental results verify the superiority of the proposed SCALA on public datasets."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Need Of Trustworthy Announcements To Achieve Driving Comfort", "authors": "Rezvi Shahariar, Chris Phillips", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "An Intelligent Transport System (ITS) is more demanding nowadays and it can be achieved through deploying Vehicular Ad Hoc Networks (VANETs). Vehicles and Roadside Units (RSUs) exchange traffic events. Malicious drivers generate false events. Thus, they need to be identified to maintain trustworthy communication. When an authorised user acts maliciously, the security scheme typically fails. However, a trust model can isolate false messages. In this paper, the significance of trustworthy announcements for VANETs is analysed. To this end, a series of experiments is conducted in Veins to illustrate how the trustworthiness of announcements affects travel time. A traffic scenario is created where vehicles detour to an alternate route with an announcement from the leading vehicle. Both true and false announcements are considered. Results confirm that false announcements and refraining from announcements increase travel time. However, the travel time is reduced with trustworthy announcements. From this analysis, it can be concluded that trustworthy announcements facilitate driver comfort."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio", "authors": "Yuankun Xie, Yi Lu, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Jianhua Tao, Xin Qi, Xiaopeng Wang, Yukun Liu, Haonan Cheng, Long Ye, Yi Sun", "subjects": "Subjects:\nSound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)", "abstract": "With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for effective detection methods. Unlike traditional deepfake audio generation, which often involves multi-step processes culminating in vocoder usage, ALM directly utilizes neural codec methods to decode discrete codes into audio. Moreover, driven by large-scale data, ALMs exhibit remarkable robustness and versatility, posing a significant challenge to current audio deepfake detection (ADD) models. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially construct the Codecfake dataset, an open-source large-scale dataset, including two languages, millions of audio samples, and various test conditions, tailored for ALM-based audio detection. Additionally, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original SAM, we propose the CSAM strategy to learn a domain balanced and generalized minima. Experiment results demonstrate that co-training on Codecfake dataset and vocoded dataset with CSAM strategy yield the lowest average Equal Error Rate (EER) of 0.616% across all test conditions compared to baseline models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          G\\\"odel Number based Clustering Algorithm with Decimal First Degree Cellular Automata", "authors": "Vicky Vikrant, Narodia Parth P, Kamalika Bhattacharjee", "subjects": "Subjects:\nFormal Languages and Automata Theory (cs.FL); Emerging Technologies (cs.ET); Machine Learning (cs.LG)", "abstract": "In this paper, a decimal first degree cellular automata (FDCA) based clustering algorithm is proposed where clusters are created based on reachability. Cyclic spaces are created and configurations which are in the same cycle are treated as the same cluster. Here, real-life data objects are encoded into decimal strings using G\u00f6del number based encoding. The benefits of the scheme is, it reduces the encoded string length while maintaining the features properties. Candidate CA rules are identified based on some theoretical criteria such as self-replication and information flow. An iterative algorithm is developed to generate the desired number of clusters over three stages. The results of the clustering are evaluated based on benchmark clustering metrics such as Silhouette score, Davis Bouldin, Calinski Harabasz and Dunn Index. In comparison with the existing state-of-the-art clustering algorithms, our proposed algorithm gives better performance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion", "authors": "Zehan Wang, Ziang Zhang, Xize Cheng, Rongjie Huang, Luping Liu, Zhenhui Ye, Haifeng Huang, Yang Zhao, Tao Jin, Peng Gao, Zhou Zhao", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Unified multi-model representation spaces are the foundation of multimodal understanding and generation. However, the billions of model parameters and catastrophic forgetting problems make it challenging to further enhance pre-trained unified spaces. In this work, we propose Molecule-Space, an idea that treats multimodal representation spaces as \"molecules\", and augments pre-trained unified space by integrating knowledge from extra expert spaces via \"molecules space reactions\". Specifically, we introduce two kinds of basic space reactions: 1) Space Displacement Reaction and 2) Space Combination Reaction. Based on these defined basic reactions, we design Complex Sequential & Parallel Reactions to effectively integrate multiple spaces simultaneously. Benefiting from the modularization concept, we further propose a coarse-to-fine customized inference strategy to flexibly adjust the enhanced unified space for different purposes. Experimentally, we fuse the audio-image-text space of ImageBind with the image-text and audio-text expert spaces. The resulting space outperforms ImageBind on 5 downstream tasks across 9 datasets. Moreover, via customized inference, it even surpasses the used image-text and audio-text expert spaces."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A trust management framework for vehicular ad hoc networks", "authors": "Rezvi Shahariar, Chris Phillips", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "Vehicular Ad Hoc Networks (VANETs) enable road users and public infrastructure to share information that improves the operation of roads and driver experience. However, these are vulnerable to poorly behaved authorized users. Trust management is used to address attacks from authorized users in accordance with their trust score. By removing the dissemination of trust metrics in the validation process, communication overhead and response time are lowered. In this paper, we propose a new Tamper-Proof Device (TPD) based trust management framework for controlling trust at the sender side vehicle that regulates driver behaviour. Moreover, the dissemination of feedback is only required when there is conflicting information in the VANET. If a conflict arises, the Road-Side Unit (RSU) decides, using the weighted voting system, whether the originator is to be believed, or not. The framework is evaluated against a centralized reputation approach and the results demonstrate that it outperforms the latter."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Fast LiDAR Upsampling using Conditional Diffusion Models", "authors": "Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim T\u00f8rresen, Ryo Kurazume", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "abstract": "The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          GISR: Geometric Initialization and Silhouette-based Refinement for Single-View Robot Pose and Configuration Estimation", "authors": "Ivan Bili\u0107, Filip Mari\u0107, Fabio Bonsignorio, Ivan Petrovi\u0107", "subjects": "Subjects:\nRobotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "abstract": "For autonomous robotics applications, it is crucial that robots are able to accurately measure their potential state and perceive their environment, including other agents within it (e.g., cobots interacting with humans). The redundancy of these measurements is important, as it allows for planning and execution of recovery protocols in the event of sensor failure or external disturbances. Visual estimation can provide this redundancy through the use of low-cost sensors and server as a standalone source of proprioception when no encoder-based sensing is available. Therefore, we estimate the configuration of the robot jointly with its pose, which provides a complete spatial understanding of the observed robot. We present GISR - a method for deep configuration and robot-to-camera pose estimation that prioritizes real-time execution. GISR is comprised of two modules: (i) a geometric initialization module, efficiently computing an approximate robot pose and configuration, and (ii) an iterative silhouette-based refinement module that refines the initial solution in only a few iterations. We evaluate our method on a publicly available dataset and show that GISR performs competitively with existing state-of-the-art approaches, while being significantly faster compared to existing methods of the same class. Our code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A fuzzy reward and punishment scheme for vehicular ad hoc networks", "authors": "Rezvi Shahariar, Chris Phillips", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "Trust management is an important security approach for the successful implementation of Vehicular Ad Hoc Networks (VANETs). Trust models evaluate messages to assign reward or punishment. This can be used to influence a driver's future behaviour. In the author's previous work, a sender side based trust management framework is developed which avoids the receiver evaluation of messages. However, this does not guarantee that a trusted driver will not lie. These \"untrue attacks\" are resolved by the RSUs using collaboration to rule on a dispute, providing a fixed amount of reward and punishment. The lack of sophistication is addressed in this paper with a novel fuzzy RSU controller considering the severity of incident, driver past behaviour, and RSU confidence to determine the reward or punishment for the conflicted drivers. Although any driver can lie in any situation, it is expected that trustworthy drivers are more likely to remain so, and vice versa. This behaviour is captured in a Markov chain model for sender and reporter drivers where their lying characteristics depend on trust score and trust state. Each trust state defines the driver's likelihood of lying using different probability distribution. An extensive simulation is performed to evaluate the performance of the fuzzy assessment and examine the Markov chain driver behaviour model with changing the initial trust score of all or some drivers in Veins simulator. The fuzzy and the fixed RSU assessment schemes are compared, and the result shows that the fuzzy scheme can encourage drivers to improve their behaviour."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Verified authors shape X/Twitter discursive communities", "authors": "Stefano Guarino, Ayoub Mounim, Guido Caldarelli, Fabio Saracco", "subjects": "Subjects:\nSocial and Information Networks (cs.SI); Computers and Society (cs.CY)", "abstract": "Community detection algorithms try to extract a mesoscale structure from the available network data, generally avoiding any explicit assumption regarding the quantity and quality of information conveyed by specific sets of edges. In this paper, we show that the core of ideological/discursive communities on X/Twitter can be effectively identified by uncovering the most informative interactions in an authors-audience bipartite network through a maximum-entropy null model. The analysis is performed considering three X/Twitter datasets related to the main political events of 2022 in Italy, using as benchmarks four state-of-the-art algorithms - three descriptive, one inferential -, and manually annotating nearly 300 verified users based on their political affiliation. In terms of information content, the communities obtained with the entropy-based algorithm are comparable to those obtained with some of the benchmarks. However, such a methodology on the authors-audience bipartite network: uses just a small sample of the available data to identify the central users of each community; returns a neater partition of the user set in just a few, easy to interpret, communities; clusters well-known political figures in a way that better matches the political alliances when compared with the benchmarks. Our results provide an important insight into online debates, highlighting that online interaction networks are mostly shaped by the activity of a small set of users who enjoy public visibility even outside social media."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset", "authors": "Paul Jideani, Aurona Gerber", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)", "abstract": "Recent social media posts on the cholera outbreak in Hammanskraal have highlighted the diverse range of emotions people experienced in response to such an event. The extent of people's opinions varies greatly depending on their level of knowledge and information about the disease. The documented re-search about Cholera lacks investigations into the classification of emotions. This study aims to examine the emotions expressed in social media posts about Chol-era. A dataset of 23,000 posts was extracted and pre-processed. The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer library was applied to deter-mine the emotional significance of each text. Additionally, Machine Learning (ML) models were applied for emotion classification, including Long short-term memory (LSTM), Logistic regression, Decision trees, and the Bidirectional En-coder Representations from Transformers (BERT) model. The results of this study demonstrated that LSTM achieved the highest accuracy of 75%. Emotion classification presents a promising tool for gaining a deeper understanding of the impact of Cholera on society. The findings of this study might contribute to the development of effective interventions in public health strategies."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          MoveTouch: Robotic Motion Capturing System with Wearable Tactile Display to Achieve Safe HRI", "authors": "Ali Alabbas, Miguel Altamirano Cabrera, Mohamed Sayed, Oussama Alyounes, Qian Liu, Dzmitry Tsetserukou", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "The collaborative robot market is flourishing as there is a trend towards simplification, modularity, and increased flexibility on the production line. But when humans and robots are collaborating in a shared environment, the safety of humans should be a priority. We introduce a novel wearable robotic system to enhance safety during Human Robot Interaction (HRI). The proposed wearable robot is designed to hold a fiducial marker and maintain its visibility to the tracking system, which, in turn, localizes the user's hand with good accuracy and low latency and provides haptic feedback on the user's wrist. The haptic feedback guides the user's hand movement during collaborative tasks in order to increase safety and enhance collaboration efficiency. A user study was conducted to assess the recognition and discriminability of ten designed haptic patterns applied to the volar and dorsal parts of the user's wrist. As a result, four patterns with a high recognition rate were chosen to be incorporated into our system. A second experiment was carried out to evaluate the system integration into real-world collaborative tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Self-supervised Gait-based Emotion Representation Learning from Selective Strongly Augmented Skeleton Sequences", "authors": "Cheng Song, Lu Lu, Zhen Ke, Long Gao, Shuai Ding", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Emotion recognition is an important part of affective computing. Extracting emotional cues from human gaits yields benefits such as natural interaction, a nonintrusive nature, and remote detection. Recently, the introduction of self-supervised learning techniques offers a practical solution to the issues arising from the scarcity of labeled data in the field of gait-based emotion recognition. However, due to the limited diversity of gaits and the incompleteness of feature representations for skeletons, the existing contrastive learning methods are usually inefficient for the acquisition of gait emotions. In this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for self-supervised gait-based emotion representation, which aims to derive effective representations from limited labeled gait data. First, we propose an SSA method for the gait emotion recognition task, which includes upper body jitter and random spatiotemporal mask. The goal of SSA is to generate more diverse and targeted positive samples and prompt the model to learn more distinctive and robust feature representations. Then, we design a complementary feature fusion network (CFFN) that facilitates the integration of cross-domain information to acquire topological structural and global adaptive features. Finally, we implement the distributional divergence minimization loss to supervise the representation learning of the generally and strongly augmented queries. Our approach is validated on the Emotion-Gait (E-Gait) and Emilya datasets and outperforms the state-of-the-art methods under different evaluation protocols."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Imbalanced Graph Classification with Multi-scale Oversampling Graph Neural Networks", "authors": "Rongrong Ma, Guansong Pang, Ling Chen", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "One main challenge in imbalanced graph classification is to learn expressive representations of the graphs in under-represented (minority) classes. Existing generic imbalanced learning methods, such as oversampling and imbalanced learning loss functions, can be adopted for enabling graph representation learning models to cope with this challenge. However, these methods often directly operate on the graph representations, ignoring rich discriminative information within the graphs and their interactions. To tackle this issue, we introduce a novel multi-scale oversampling graph neural network (MOSGNN) that learns expressive minority graph representations based on intra- and inter-graph semantics resulting from oversampled graphs at multiple scales - subgraph, graph, and pairwise graphs. It achieves this by jointly optimizing subgraph-level, graph-level, and pairwise-graph learning tasks to learn the discriminative information embedded within and between the minority graphs. Extensive experiments on 16 imbalanced graph datasets show that MOSGNN i) significantly outperforms five state-of-the-art models, and ii) offers a generic framework, in which different advanced imbalanced learning loss functions can be easily plugged in and obtain significantly improved classification performance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Empowering Wireless Networks with Artificial Intelligence Generated Graph", "authors": "Jiacheng Wang, Yinqiu Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Haibo Zhou, Dong In Kim", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI)", "abstract": "In wireless communications, transforming network into graphs and processing them using deep learning models, such as Graph Neural Networks (GNNs), is one of the mainstream network optimization approaches. While effective, the generative AI (GAI) shows stronger capabilities in graph analysis, processing, and generation, than conventional methods such as GNN, offering a broader exploration space for graph-based network optimization. Therefore, this article proposes to use GAI-based graph generation to support wireless networks. Specifically, we first explore applications of graphs in wireless networks. Then, we introduce and analyze common GAI models from the perspective of graph generation. On this basis, we propose a framework that incorporates the conditional diffusion model and an evaluation network, which can be trained with reward functions and conditions customized by network designers and users. Once trained, the proposed framework can create graphs based on new conditions, helping to tackle problems specified by the user in wireless networks. Finally, using the link selection in integrated sensing and communication (ISAC) as an example, the effectiveness of the proposed framework is validated."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Traj-LLM: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models", "authors": "Zhengxing Lan, Hongbo Li, Lingshan Liu, Bo Fan, Yisheng Lv, Yilong Ren, Zhiyong Cui", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "abstract": "Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving. Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of the complex traffic semantics. This paper proposes Traj-LLM, the first to investigate the potential of using Large Language Models (LLMs) without explicit prompt engineering to generate future motion from agents' past/observed trajectories and scene semantics. Traj-LLM starts with sparse context joint coding to dissect the agent and scene features into a form that LLMs understand. On this basis, we innovatively explore LLMs' powerful comprehension abilities to capture a spectrum of high-level scene knowledge and interactive information. Emulating the human-like lane focus cognitive function and enhancing Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the pioneering Mamba module. Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant multi-modal predictions. Extensive experiments manifest that Traj-LLM, fortified by LLMs' strong prior knowledge and understanding prowess, together with lane-aware probability learning, outstrips state-of-the-art methods across evaluation metrics. Moreover, the few-shot analysis further substantiates Traj-LLM's performance, wherein with just 50% of the dataset, it outperforms the majority of benchmarks relying on complete data utilization. This study explores equipping the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent motion in a new way."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Learning with Posterior Sampling for Revenue Management under Time-varying Demand", "authors": "Kazuma Shimizu, Junya Honda, Shinji Ito, Shinji Nakadai", "subjects": "Subjects:\nMachine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "This paper discusses the revenue management (RM) problem to maximize revenue by pricing items or services. One challenge in this problem is that the demand distribution is unknown and varies over time in real applications such as airline and retail industries. In particular, the time-varying demand has not been well studied under scenarios of unknown demand due to the difficulty of jointly managing the remaining inventory and estimating the demand. To tackle this challenge, we first introduce an episodic generalization of the RM problem motivated by typical application scenarios. We then propose a computationally efficient algorithm based on posterior sampling, which effectively optimizes prices by solving linear programming. We derive a Bayesian regret upper bound of this algorithm for general models where demand parameters can be correlated between time periods, while also deriving a regret lower bound for generic algorithms. Our empirical study shows that the proposed algorithm performs better than other benchmark algorithms and comparably to the optimal policy in hindsight. We also propose a heuristic modification of the proposed algorithm, which further efficiently learns the pricing policy in the experiments."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Weakly-supervised Semantic Segmentation via Dual-stream Contrastive Learning of Cross-image Contextual Information", "authors": "Qi Lai, Chi-Man Vong", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Weakly supervised semantic segmentation (WSSS) aims at learning a semantic segmentation model with only image-level tags. Despite intensive research on deep learning approaches over a decade, there is still a significant performance gap between WSSS and full semantic segmentation. Most current WSSS methods always focus on a limited single image (pixel-wise) information while ignoring the valuable inter-image (semantic-wise) information. From this perspective, a novel end-to-end WSSS framework called DSCNet is developed along with two innovations: i) pixel-wise group contrast and semantic-wise graph contrast are proposed and introduced into the WSSS framework; ii) a novel dual-stream contrastive learning (DSCL) mechanism is designed to jointly handle pixel-wise and semantic-wise context information for better WSSS performance. Specifically, the pixel-wise group contrast learning (PGCL) and semantic-wise graph contrast learning (SGCL) tasks form a more comprehensive solution. Extensive experiments on PASCAL VOC and MS COCO benchmarks verify the superiority of DSCNet over SOTA approaches and baseline models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Delve into Base-Novel Confusion: Redundancy Exploration for Few-Shot Class-Incremental Learning", "authors": "Haichen Zhou, Yixiong Zou, Ruixuan Li, Yuhua Li, Kui Xiao", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "abstract": "Few-shot class-incremental learning (FSCIL) aims to acquire knowledge from novel classes with limited samples while retaining information about base classes. Existing methods address catastrophic forgetting and overfitting by freezing the feature extractor during novel-class learning. However, these methods usually tend to cause the confusion between base and novel classes, i.e., classifying novel-class samples into base classes. In this paper, we delve into this phenomenon to study its cause and solution. We first interpret the confusion as the collision between the novel-class and the base-class region in the feature space. Then, we find the collision is caused by the label-irrelevant redundancies within the base-class feature and pixel space. Through qualitative and quantitative experiments, we identify this redundancy as the shortcut in the base-class training, which can be decoupled to alleviate the collision. Based on this analysis, to alleviate the collision between base and novel classes, we propose a method for FSCIL named Redundancy Decoupling and Integration (RDI). RDI first decouples redundancies from base-class space to shrink the intra-base-class feature space. Then, it integrates the redundancies as a dummy class to enlarge the inter-base-class feature space. This process effectively compresses the base-class feature space, creating buffer space for novel classes and alleviating the model's confusion between the base and novel classes. Extensive experiments across benchmark datasets, including CIFAR-100, miniImageNet, and CUB-200-2011 demonstrate that our method achieves state-of-the-art performance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DataSP: A Differential All-to-All Shortest Path Algorithm for Learning Costs and Predicting Paths with Context", "authors": "Alan A. Lahoud, Erik Schaffernicht, Johannes A. Stork", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Learning latent costs of transitions on graphs from trajectories demonstrations under various contextual features is challenging but useful for path planning. Yet, existing methods either oversimplify cost assumptions or scale poorly with the number of observed trajectories. This paper introduces DataSP, a differentiable all-to-all shortest path algorithm to facilitate learning latent costs from trajectories. It allows to learn from a large number of trajectories in each learning step without additional computation. Complex latent cost functions from contextual features can be represented in the algorithm through a neural network approximation. We further propose a method to sample paths from DataSP in order to reconstruct/mimic observed paths' distributions. We prove that the inferred distribution follows the maximum entropy principle. We show that DataSP outperforms state-of-the-art differentiable combinatorial solver and classical machine learning approaches in predicting paths on graphs."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Power-Domain Interference Graph Estimation for Full-Duplex Millimeter-Wave Backhauling", "authors": "Haorui Li, Daqian Ding, Yibo Pi, Xudong Wang", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI)", "abstract": "Traditional wisdom for network resource management allocates separate frequency-time resources for measurement and data transmission tasks. As a result, the two types of tasks have to compete for resources, and a heavy measurement task inevitably reduces available resources for data transmission. This prevents interference graph estimation (IGE), a heavy yet important measurement task, from being widely used in practice. To resolve this issue, we propose to use power as a new dimension for interference measurement in full-duplex millimeter-wave backhaul networks, such that data transmission and measurement can be done simultaneously using the same frequency-time resources. Our core insight is to consider the mmWave network as a linear system, where the received power of a node is a linear combination of the channel gains. By controlling the powers of transmitters, we can find unique solutions for the channel gains of interference links and use them to estimate the interference. To accomplish resource allocation and IGE simultaneously, we jointly optimize resource allocation and IGE with power control. Extensive simulations show that significant links in the interference graph can be accurately estimated with minimal extra power consumption, independent of the time and carrier frequency offsets between nodes."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Enabling Roll-up and Drill-down Operations in News Exploration with Knowledge Graphs for Due Diligence and Risk Management", "authors": "Sha Wang, Yuchen Li, Hanhua Xiao, Zhifeng Bao, Lambert Deng, Yanfei Dong", "subjects": "Subjects:\nInformation Retrieval (cs.IR)", "abstract": "Efficient news exploration is crucial in real-world applications, particularly within the financial sector, where numerous control and risk assessment tasks rely on the analysis of public news reports. The current processes in this domain predominantly rely on manual efforts, often involving keywordbased searches and the compilation of extensive keyword lists. In this paper, we introduce NCEXPLORER, a framework designed with OLAP-like operations to enhance the news exploration experience. NCEXPLORER empowers users to use roll-up operations for a broader content overview and drill-down operations for detailed insights. These operations are achieved through integration with external knowledge graphs (KGs), encompassing both fact-based and ontology-based structures. This integration significantly augments exploration capabilities, offering a more comprehensive and efficient approach to unveiling the underlying structures and nuances embedded in news content. Extensive empirical studies through master-qualified evaluators on Amazon Mechanical Turk demonstrate NCEXPLORER's superiority over existing state-of-the-art news search methodologies across an array of topic domains, using real-world news datasets."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          FIGRET: Fine-Grained Robustness-Enhanced Traffic Engineering", "authors": "Ximeng Liu, Shizhen Zhao, Yong Cui", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI)", "abstract": "Traffic Engineering (TE) is critical for improving network performance and reliability. A key challenge in TE is the management of sudden traffic bursts. Existing TE schemes often struggle to accurately determine the extent of focus required for these surges, thereby facing difficulties in achieving a balance between performance under normal and peak traffic conditions. To address this issue, we introduce FIGRET, a Fine-Grained Robustness-Enhanced TE Scheme. FIGRET offers a novel approach to TE by providing varying levels of robustness enhancements, customized according to the distinct traffic characteristics of various source-destination pairs. By leveraging a sophisticated loss function and advanced deep learning techniques, FIGRET is capable of generating high-quality TE solutions efficiently. Our evaluations of real-world production networks, including Wide Area Networks and data centers, demonstrate that FIGRET significantly outperforms existing TE schemes. Compared to the TE scheme currently deployed in the Jupiter network of Google, FIGRET achieves a 9\\%-34\\% reduction in average Maximum Link Utilization and improves solution speed by $35\\times$-$1800 \\times$. Against DOTE, a state-of-the-art deep learning-based TE method, FIGRET substantially lowers the occurrence of significant congestion events triggered by traffic bursts by 41\\%-53.9\\% in topologies characterized by high traffic dynamics."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          SPSW: Database Watermarking Based on Fake Tuples and Sparse Priority Strategy", "authors": "Zhiwen Ren, Zehua Ma, Weiming Zhang, Nenghai Yu", "subjects": "Subjects:\nDatabases (cs.DB)", "abstract": "Databases play a crucial role in storing and managing vast amounts of data in various organizations and industries. Yet the risk of database leakage poses a significant threat to data privacy and security. To trace the source of database leakage, researchers have proposed many database watermarking schemes. Among them, fake-tuples-based database watermarking shows great potential as it does not modify the original data of the database, ensuring the seamless usability of the watermarked database. However, the existing fake-tuple-based database watermarking schemes need to insert a large number of fake tuples for the embedding of each watermark bit, resulting in low watermark transparency. Therefore, we propose a novel database watermarking scheme based on fake tuples and sparse priority strategy, named SPSW, which achieves the same watermark capacity with a lower number of inserted fake tuples compared to the existing embedding strategy. Specifically, for a database about to be watermarked, we prioritize embedding the sparsest watermark sequence, i.e., the sequence containing the most `0' bits among the currently available watermark sequences. For each bit in the sparse watermark sequence, when it is set to `1', SPSW will embed the corresponding set of fake tuples into the database. Otherwise, no modifications will be made to the database. Through theoretical analysis, the proposed sparse priority strategy not only improves transparency but also enhances the robustness of the watermark. The comparative experimental results with other database watermarking schemes further validate the superior performance of the proposed SPSW, aligning with the theoretical analysis."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Developing trustworthy AI applications with foundation models", "authors": "Michael Mock (1), Sebastian Schmidt (1), Felix M\u00fcller (2 and 1), Rebekka G\u00f6rge (1), Anna Schmitz (1), Elena Haedecke (2 and 1), Angelika Voss (1), Dirk Hecker (1), Maximillian Poretschkin (1 and 2) ((1) Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS Sankt Augustin, Germany, (2) University of Bonn, Bonn, Germany)", "subjects": "Subjects:\nArtificial Intelligence (cs.AI)", "abstract": "The trustworthiness of AI applications has been the subject of recent research and is also addressed in the EU's recently adopted AI Regulation. The currently emerging foundation models in the field of text, speech and image processing offer completely new possibilities for developing AI applications. This whitepaper shows how the trustworthiness of an AI application developed with foundation models can be evaluated and ensured. For this purpose, the application-specific, risk-based approach for testing and ensuring the trustworthiness of AI applications, as developed in the 'AI Assessment Catalog - Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is transferred to the context of foundation models. Special consideration is given to the fact that specific risks of foundation models can have an impact on the AI application and must also be taken into account when checking trustworthiness. Chapter 1 of the white paper explains the fundamental relationship between foundation models and AI applications based on them in terms of trustworthiness. Chapter 2 provides an introduction to the technical construction of foundation models and Chapter 3 shows how AI applications can be developed based on them. Chapter 4 provides an overview of the resulting risks regarding trustworthiness. Chapter 5 shows which requirements for AI applications and foundation models are to be expected according to the draft of the European Union's AI Regulation and Chapter 6 finally shows the system and procedure for meeting trustworthiness requirements."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Fault Identification Enhancement with Reinforcement Learning (FIERL)", "authors": "Valentina Zaccaria, Davide Sartor, Simone Del Favero, Gian Antonio Susto", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "This letter presents a novel approach in the field of Active Fault Detection (AFD), by explicitly separating the task into two parts: Passive Fault Detection (PFD) and control input design. This formulation is very general, and most existing AFD literature can be viewed through this lens. By recognizing this separation, PFD methods can be leveraged to provide components that make efficient use of the available information, while the control input is designed in order to optimize the gathering of information. The core contribution of this work is FIERL, a general simulation-based approach for the design of such control strategies, using Constrained Reinforcement Learning (CRL) to optimize the performance of arbitrary passive detectors. The control policy is learned without the need of knowing the passive detector inner workings, making FIERL broadly applicable. However, it is especially useful when paired with the design of an efficient passive component. Unlike most AFD approaches, FIERL can handle fairly complex scenarios such as continuous sets of fault modes. The effectiveness of FIERL is tested on a benchmark problem for actuator fault diagnosis, where FIERL is shown to be fairly robust, being able to generalize to fault dynamics not seen in training."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID", "authors": "Wentao Tan, Changxing Ding, Jiayu Jiang, Fei Wang, Yibing Zhan, Dapeng Tao", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Text-to-image person re-identification (ReID) retrieves pedestrian images according to textual descriptions. Manually annotating textual descriptions is time-consuming, restricting the scale of existing datasets and therefore the generalization ability of ReID models. As a result, we study the transferable text-to-image ReID problem, where we train a model on our proposed large-scale database and directly deploy it to various datasets for evaluation. We obtain substantial training data via Multi-modal Large Language Models (MLLMs). Moreover, we identify and address two key challenges in utilizing the obtained textual descriptions. First, an MLLM tends to generate descriptions with similar structures, causing the model to overfit specific sentence patterns. Thus, we propose a novel method that uses MLLMs to caption images according to various templates. These templates are obtained using a multi-turn dialogue with a Large Language Model (LLM). Therefore, we can build a large-scale dataset with diverse textual descriptions. Second, an MLLM may produce incorrect descriptions. Hence, we introduce a novel method that automatically identifies words in a description that do not correspond with the image. This method is based on the similarity between one text and all patch token embeddings in the image. Then, we mask these words with a larger probability in the subsequent training epoch, alleviating the impact of noisy textual descriptions. The experimental results demonstrate that our methods significantly boost the direct transfer text-to-image ReID performance. Benefiting from the pre-trained model weights, we also achieve state-of-the-art performance in the traditional evaluation settings."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs", "authors": "Eline M. Bovy, Marnix Suilen, Sebastian Junges, Nils Jansen", "subjects": "Subjects:\nArtificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)", "abstract": "Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known. Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets. While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods. We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values. These novel semantics for RPOMDPS give access to results for the widely studied POSG model; concretely, we show the existence of a Nash equilibrium. Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Dual-domain Collaborative Denoising for Social Recommendation", "authors": "Wenjie Chen, Yi Zhang, Honghao Li, Lei Sang, Yiwen Zhang", "subjects": "Subjects:\nInformation Retrieval (cs.IR); Social and Information Networks (cs.SI)", "abstract": "Social recommendation leverages social network to complement user-item interaction data for recommendation task, aiming to mitigate the data sparsity issue in recommender systems. However, existing social recommendation methods encounter the following challenge: both social network and interaction data contain substaintial noise, and the propagation of such noise through Graph Neural Networks (GNNs) not only fails to enhance recommendation performance but may also interfere with the model's normal training. Despite the importance of denoising for social network and interaction data, only a limited number of studies have considered the denoising for social network and all of them overlook that for interaction data, hindering the denoising effect and recommendation performance. Based on this, we propose a novel model called Dual-domain Collaborative Denoising for Social Recommendation ($\\textbf{DCDSR}$). DCDSR comprises two primary modules: the structure-level collaborative denoising module and the embedding-space collaborative denoising module. In the structure-level collaborative denoising module, information from interaction domain is first employed to guide social network denoising. Subsequently, the denoised social network is used to supervise the denoising for interaction data. The embedding-space collaborative denoising module devotes to resisting the noise cross-domain diffusion problem through contrastive learning with dual-domain embedding collaborative perturbation. Additionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is introduced to better harness the denoising capability of contrastive learning. Evaluating our model on three real-world datasets verifies that DCDSR has a considerable denoising effect, thus outperforms the state-of-the-art social recommendation methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Unsupervised Skin Feature Tracking with Deep Neural Networks", "authors": "Jose Chang, Torbj\u00f6rn E.M. Nordling", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Facial feature tracking is essential in imaging ballistocardiography for accurate heart rate estimation and enables motor degradation quantification in Parkinson's disease through skin feature tracking. While deep convolutional neural networks have shown remarkable accuracy in tracking tasks, they typically require extensive labeled data for supervised training. Our proposed pipeline employs a convolutional stacked autoencoder to match image crops with a reference crop containing the target feature, learning deep feature encodings specific to the object category in an unsupervised manner, thus reducing data requirements. To overcome edge effects making the performance dependent on crop size, we introduced a Gaussian weight on the residual errors of the pixels when calculating the loss function. Training the autoencoder on facial images and validating its performance on manually labeled face and hand videos, our Deep Feature Encodings (DFE) method demonstrated superior tracking accuracy with a mean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods like SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and CoTracker. Overall, our unsupervised learning approach excels in tracking various skin features under significant motion conditions, providing superior feature descriptors for tracking, matching, and image registration compared to both traditional and state-of-the-art supervised learning methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Sparse Tensor Generator with Efficient Feature Extraction", "authors": "Tugba Torun, Eren Yenigul, Ameer Taweel, Didem Unat", "subjects": "Subjects:\nMathematical Software (cs.MS); Machine Learning (cs.LG)", "abstract": "Sparse tensor operations are gaining attention in emerging applications such as social networks, deep learning, diagnosis, crime, and review analysis. However, a major obstacle for research in sparse tensor operations is the deficiency of a broad-scale sparse tensor dataset. Another challenge in sparse tensor operations is examining the sparse tensor features, which are not only important for revealing its nonzero pattern but also have a significant impact on determining the best-suited storage format, the decomposition algorithm, and the reordering methods. However, due to the large sizes of real tensors, even extracting these features becomes costly without caution. To address these gaps in the literature, we have developed a smart sparse tensor generator that mimics the substantial features of real sparse tensors. Moreover, we propose various methods for efficiently extracting an extensive set of features for sparse tensors. The effectiveness of our generator is validated through the quality of features and the performance of decomposition in the generated tensors. Both the sparse tensor feature extractor and the tensor generator are open source with all the artifacts available at this https URL and this https URL, respectively."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context", "authors": "Yunxin Li, Baotian Hu, Haoyuan Shi, Wei Wang, Longyue Wang, Min Zhang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "abstract": "Large Multimodal Models (LMMs) have achieved impressive success in visual understanding and reasoning, remarkably improving the performance of mathematical reasoning in a visual context. Yet, a challenging type of visual math lies in the multimodal graph theory problem, which demands that LMMs understand the graphical structures accurately and perform multi-step reasoning on the visual graph. Additionally, exploring multimodal graph theory problems will lead to more effective strategies in fields like biology, transportation, and robotics planning. To step forward in this direction, we are the first to design a benchmark named VisionGraph, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems. It encompasses eight complex graph problem tasks, from connectivity to shortest path problems. Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning. Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Evolving R2 to R2+: Optimal, Delayed Line-of-sight Vector-based Path Planning", "authors": "Yan Kai Lai, Prahlad Vadakkepat, Cheng Xiang", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "A vector-based any-angle path planner, R2, is evolved in to R2+ in this paper. By delaying line-of-sight, R2 and R2+ search times are largely unaffected by the distance between the start and goal points, but are exponential in the worst case with respect to the number of collisions during searches. To improve search times, additional discarding conditions in the overlap rule are introduced in R2+. In addition, R2+ resolves interminable chases in R2 by replacing ad hoc points with limited occupied-sector traces from target nodes, and simplifies R2 by employing new abstract structures and ensuring target progression during a trace. R2+ preserves the speed of R2 when paths are expected to detour around few obstacles, and searches significantly faster than R2 in maps with many disjoint obstacles."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Supervised Anomaly Detection for Complex Industrial Images", "authors": "Aimira Baitieva, David Hurych, Victor Besnier, Olivier Bernard", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentation-based Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Improving Long Text Understanding with Knowledge Distilled from Summarization Model", "authors": "Yan Liu, Yazheng Yang, Xiaokang Chen", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)", "abstract": "Long text understanding is important yet challenging for natural language processing. A long article or document usually contains many redundant words that are not pertinent to its gist and sometimes can be regarded as noise. With recent advances of abstractive summarization, we propose our \\emph{Gist Detector} to leverage the gist detection ability of a summarization model and integrate the extracted gist into downstream models to enhance their long text understanding ability. Specifically, Gist Detector first learns the gist detection knowledge distilled from a summarization model, and then produces gist-aware representations to augment downstream models. We evaluate our method on three different tasks: long document classification, distantly supervised open-domain question answering, and non-parallel text style transfer. The experimental results show that our method can significantly improve the performance of baseline models on all tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Editing Mesh Sequences with Varying Connectivity", "authors": "Filip H\u00e1cha, Jan Dvo\u0159\u00e1k, Zuzana K\u00e1\u010derekov\u00e1, Libor V\u00e1\u0161a", "subjects": "Subjects:\nGraphics (cs.GR)", "abstract": "Time-varying connectivity of triangle mesh sequences leads to substantial difficulties in their processing. Unlike editing sequences with constant connectivity, editing sequences with varying connectivity requires addressing the problem of temporal correspondence between the frames of the sequence. We present a method for time-consistent editing of triangle mesh sequences with varying connectivity using sparse temporal correspondence, which can be obtained using existing methods. Our method includes a deformation model based on the usage of the sparse temporal correspondence, which is suitable for the temporal propagation of user-specified deformations of the edited surface with respect to the shape and true topology of the surface while preserving the individual connectivity of each frame. Since there is no other method capable of comparable types of editing on time-varying meshes, we compare our method and the proposed deformation model with a baseline approach and demonstrate the benefits of our framework."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Computation of some dispersive equations through their iterated linearisation", "authors": "Guannan Chen, Arieh Iserles, Karolina Kropielnicka, Pranav Singh", "subjects": "Subjects:\nNumerical Analysis (math.NA); Computational Physics (physics.comp-ph)", "abstract": "It is often the case that, while the numerical solution of the non-linear dispersive equation $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}(u(t),t)u(t)$ represents a formidable challenge, it is fairly easy and cheap to solve closely related linear equations of the form $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}_1(t)u(t)+\\widetilde{\\mathcal H}_2(t)u(t)$, where $\\mathcal{H}_1(t)+\\mathcal{H}_2(v,t)=\\mathcal{H}(v,t)$. In that case we advocate an iterative linearisation procedure that involves fixed-point iteration of the latter equation to solve the former. A typical case is when the original problem is a nonlinear Schr\u00f6dinger or Gross--Pitaevskii equation, while the `easy' equation is linear Schr\u00f6dinger with time-dependent potential. We analyse in detail the iterative scheme and its practical implementation, prove that each iteration increases the order, derive upper bounds on the speed of convergence and discuss in the case of nonlinear Schr\u00f6dinger equation with cubic potential the preservation of structural features of the underlying equation: the $\\mathrm{L}_2$ norm, momentum and Hamiltonian energy. A key ingredient in our approach is the use of the Magnus expansion in conjunction with Hermite quadratures, which allows effective solutions of the linearised but non-autonomous equations in an iterative fashion. The resulting Magnus--Hermite methods can be combined with a wide range of numerical approximations to the matrix exponential. The paper concludes with a number of numerical experiments, demonstrating the power of the proposed approach."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models", "authors": "Guochao Jiang, Zepeng Ding, Yuchen Shi, Deqing Yang", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL). However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself. In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type. With such significant information, the LLM can achieve entity classification more precisely. To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering. Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A posteriori error analysis of hybrid higher order methods for the elliptic obstacle problem", "authors": "Kamana Porwal, Ritesh Singla", "subjects": "Subjects:\nNumerical Analysis (math.NA)", "abstract": "In this article, a posteriori error analysis of the elliptic obstacle problem is addressed using hybrid high-order methods. The method involve cell unknowns represented by degree-$r$ polynomials and face unknowns represented by degree-$s$ polynomials, where $r=0$ and $s$ is either $0$ or $1$. The discrete obstacle constraints are specifically applied to the cell unknowns. The analysis hinges on the construction of a suitable Lagrange multiplier, a residual functional and a linear averaging map. The reliability and the efficiency of the proposed a posteriori error estimator is discussed, and the study is concluded by numerical experiments supporting the theoretical results."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture", "authors": "Yitong Jin, Zhiping Qiu, Yi Shi, Shuangpeng Sun, Chongwu Wang, Donghao Pan, Jiachen Zhao, Zhenghao Liang, Yuan Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai", "subjects": "Subjects:\nMultimedia (cs.MM)", "abstract": "In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Frequency-Assisted Mamba for Remote Sensing Image Super-Resolution", "authors": "Yi Xiao, Qiangqiang Yuan, Kui Jiang, Yuzeng Chen, Qiang Zhang, Chia-Wen Lin", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Recent progress in remote sensing image (RSI) super-resolution (SR) has exhibited remarkable performance using deep neural networks, e.g., Convolutional Neural Networks and Transformers. However, existing SR methods often suffer from either a limited receptive field or quadratic computational overhead, resulting in sub-optimal global representation and unacceptable computational costs in large-scale RSI. To alleviate these issues, we develop the first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR, which specializes in processing large-scale RSI by capturing long-range dependency with linear complexity. To achieve better SR reconstruction, building upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed FMSR, to explore the spatial and frequent correlations. In particular, our FMSR features a multi-level fusion architecture equipped with the Frequency Selection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate Module (HGM) to grasp their merits for effective spatial-frequency fusion. Recognizing that global and local dependencies are complementary and both beneficial for SR, we further recalibrate these multi-level features for accurate feature fusion via learnable scaling adaptors. Extensive experiments on AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms state-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on average, while consuming only 28.05% and 19.08% of its memory consumption and complexity, respectively."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Communication-Efficient Collaborative Perception via Information Filling with Codebook", "authors": "Yue Hu, Juntong Peng, Sifei Liu, Junhao Ge, Si Liu, Siheng Chen", "subjects": "Subjects:\nInformation Theory (cs.IT); Computer Vision and Pattern Recognition (cs.CV); Multiagent Systems (cs.MA)", "abstract": "Collaborative perception empowers each agent to improve its perceptual ability through the exchange of perceptual messages with other agents. It inherently results in a fundamental trade-off between perception ability and communication cost. To address this bottleneck issue, our core idea is to optimize the collaborative messages from two key aspects: representation and selection. The proposed codebook-based message representation enables the transmission of integer codes, rather than high-dimensional feature maps. The proposed information-filling-driven message selection optimizes local messages to collectively fill each agent's information demand, preventing information overflow among multiple agents. By integrating these two designs, we propose CodeFilling, a novel communication-efficient collaborative perception system, which significantly advances the perception-communication trade-off and is inclusive to both homogeneous and heterogeneous collaboration settings. We evaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new simulation dataset, OPV2VH+. Results show that CodeFilling outperforms previous SOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication volume. Our code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A review on discriminative self-supervised learning methods", "authors": "Nikolaos Giakoumoglou, Tania Stathaki", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "abstract": "In the field of computer vision, self-supervised learning has emerged as a method to extract robust features from unlabeled data, where models derive labels autonomously from the data itself, without the need for manual annotation. This paper provides a comprehensive review of discriminative approaches of self-supervised learning within the domain of computer vision, examining their evolution and current status. Through an exploration of various methods including contrastive, self-distillation, knowledge distillation, feature decorrelation, and clustering techniques, we investigate how these approaches leverage the abundance of unlabeled data. Finally, we have comparison of self-supervised learning methods on the standard ImageNet classification benchmark."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Numerical analysis of small-strain elasto-plastic deformation using local Radial Basis Function approximation with Picard iteration", "authors": "Filip Strni\u0161a, Mitja Jan\u010di\u010d, Gregor Kosec", "subjects": "Subjects:\nNumerical Analysis (math.NA)", "abstract": "This paper deals with a numerical analysis of plastic deformation under various conditions, utilizing Radial Basis Function (RBF) approximation. The focus is on the elasto-plastic von Mises problem under plane-strain assumption. Elastic deformation is modelled using the Navier-Cauchy equation. In regions where the von Mises stress surpasses the yield stress, corrections are applied locally through a return mapping algorithm. The non-linear deformation problem in the plastic domain is solved using the Picard iteration. The solutions for the Navier-Cauchy equation are computed using the Radial Basis Function-Generated Finite Differences (RBF-FD) meshless method using only scattered nodes in a strong form. Verification of the method is performed through the analysis of an internally pressurized thick-walled cylinder subjected to varying loading conditions. These conditions induce states of elastic expansion, perfectly-plastic yielding, and plastic yielding with linear hardening. The results are benchmarked against analytical solutions and traditional Finite Element Method (FEM) solutions. The paper also showcases the robustness of this approach by solving case of thick-walled cylinder with cut-outs. The results affirm that the RBF-FD method produces results comparable to those obtained through FEM, while offering substantial benefits in managing complex geometries without the necessity for conventional meshing, along with other benefits of meshless methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          End-to-End Semi-Supervised approach with Modulated Object Queries for Table Detection in Documents", "authors": "Iqraa Ehsan, Tahira Shehzadi, Didier Stricker, Muhammad Zeshan Afzal", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Table detection, a pivotal task in document analysis, aims to precisely recognize and locate tables within document images. Although deep learning has shown remarkable progress in this realm, it typically requires an extensive dataset of labeled data for proficient training. Current CNN-based semi-supervised table detection approaches use the anchor generation process and Non-Maximum Suppression (NMS) in their detection process, limiting training efficiency. Meanwhile, transformer-based semi-supervised techniques adopted a one-to-one match strategy that provides noisy pseudo-labels, limiting overall efficiency. This study presents an innovative transformer-based semi-supervised table detector. It improves the quality of pseudo-labels through a novel matching strategy combining one-to-one and one-to-many assignment techniques. This approach significantly enhances training efficiency during the early stages, ensuring superior pseudo-labels for further training. Our semi-supervised approach is comprehensively evaluated on benchmark datasets, including PubLayNet, ICADR-19, and TableBank. It achieves new state-of-the-art results, with a mAP of 95.7% and 97.9% on TableBank (word) and PubLaynet with 30% label data, marking a 7.4 and 7.6 point improvement over previous semi-supervised table detection approach, respectively. The results clearly show the superiority of our semi-supervised approach, surpassing all existing state-of-the-art methods by substantial margins. This research represents a significant advancement in semi-supervised table detection methods, offering a more efficient and accurate solution for practical document analysis tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision Support", "authors": "Felix Haag, Carlo Stingl, Katrin Zerfass, Konstantin Hopf, Thorsten Staake", "subjects": "Subjects:\nComputers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); General Economics (econ.GN)", "abstract": "Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals' decision-making (e.g., by manipulating purchase decisions). Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions. So far, the potential of these technological advances to overcome anchoring bias remains widely unclear. To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias. Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias. Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Discrepancy-based Diffusion Models for Lesion Detection in Brain MRI", "authors": "Keqiang Fan, Xiaohao Cai, Mahesan Niranjan", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "abstract": "Diffusion probabilistic models (DPMs) have exhibited significant effectiveness in computer vision tasks, particularly in image generation. However, their notable performance heavily relies on labelled datasets, which limits their application in medical images due to the associated high-cost annotations. Current DPM-related methods for lesion detection in medical imaging, which can be categorized into two distinct approaches, primarily rely on image-level annotations. The first approach, based on anomaly detection, involves learning reference healthy brain representations and identifying anomalies based on the difference in inference results. In contrast, the second approach, resembling a segmentation task, employs only the original brain multi-modalities as prior information for generating pixel-level annotations. In this paper, our proposed model - discrepancy distribution medical diffusion (DDMD) - for lesion detection in brain MRI introduces a novel framework by incorporating distinctive discrepancy features, deviating from the conventional direct reliance on image-level annotations or the original brain modalities. In our method, the inconsistency in image-level annotations is translated into distribution discrepancies among heterogeneous samples while preserving information within homogeneous samples. This property retains pixel-wise uncertainty and facilitates an implicit ensemble of segmentation, ultimately enhancing the overall detection performance. Thorough experiments conducted on the BRATS2020 benchmark dataset containing multimodal MRI scans for brain tumour detection demonstrate the great performance of our approach in comparison to state-of-the-art methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Prototype2Code: End-to-end Front-end Code Generation from UI Design Prototypes", "authors": "Shuhong Xiao, Yunnong Chen, Jiazhi Li, Liuqing Chen, Lingyun Sun, Tingting Zhou", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "UI-to-code technology has streamlined the front-end development process, reducing repetitive tasks for engineers. prior research mainly use design prototypes as inputs, with the effectiveness of the generated code heavily dependent on these prototypes' quality, leading to compromised robustness. Moreover, these approaches also exhibit shortcomings in code quality, including issues such as disorganized UI structures and the inability to support responsive layouts. To address these challenges, we introduce Prototype2Code, which achieves end-to-end front-end code generation with business demands. For Prototype2Code, we incorporate design linting into the workflow, addressing the detection of fragmented elements and perceptual groups, enhancing the robustness of the generated outcomes. By optimizing the hierarchical structure and intelligently recognizing UI element types, Prototype2Code generates code that is more readable and structurally clearer. To meet responsive design requirements, Prototype2Code primarily supports flexbox layout model, ensuring code compatibility across various device sizes. To validate the efficacy, we compare Prototype2Code with the commercial code generation platform CodeFun and Screenshot-to-code based on GPT-4 with vision. Employing structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and mean squared error (MSE) for visual similarity assessment, Prototype2Code's rendered UI effects align most closely with the design prototypes, exhibiting the minimal errors. We also conduct a user study with five experienced front-end engineers, inviting them to review and revise code generated by the three methods. As a result, Prototype2Code surpasses other methods in readability, usability, and maintainability, better meeting the business needs of industrial development."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          RF-based Energy Harvesting: Nonlinear Models, Applications and Challenges", "authors": "Ruihong Jiang", "subjects": "Subjects:\nInformation Theory (cs.IT); Signal Processing (eess.SP)", "abstract": "So far, various aspects associated with wireless energy harvesting (EH) have been investigated from diverse perspectives, including energy sources and models, usage protocols, energy scheduling and optimization, and EH implementation in different wireless communication systems. However, a comprehensive survey specifically focusing on models of radio frequency (RF)-based EH behaviors has not yet been presented. To address this gap, this article provides an overview of the mainstream mathematical models that capture the nonlinear behavior of practical EH circuits, serving as a valuable handbook of mathematical models for EH application research. Moreover, we summarize the application of each nonlinear EH model, including the associated challenges and precautions. We also analyze the impact and advancements of each EH model on RF-based EH systems in wireless communication, utilizing artificial intelligence (AI) techniques. Additionally, we highlight emerging research directions in the context of nonlinear RF-based EH. This article aims to contribute to the future application of RF-based EH in novel communication research domains to a significant extent."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Predictive Mapping of Spectral Signatures from RGB Imagery for Off-Road Terrain Analysis", "authors": "Sarvesh Prajapati, Ananya Trivedi, Bruce Maxwell, Taskin Padir", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "Accurate identification of complex terrain characteristics, such as soil composition and coefficient of friction, is essential for model-based planning and control of mobile robots in off-road environments. Spectral signatures leverage distinct patterns of light absorption and reflection to identify various materials, enabling precise characterization of their inherent properties. Recent research in robotics has explored the adoption of spectroscopy to enhance perception and interaction with environments. However, the significant cost and elaborate setup required for mounting these sensors present formidable barriers to widespread adoption. In this study, we introduce RS-Net (RGB to Spectral Network), a deep neural network architecture designed to map RGB images to corresponding spectral signatures. We illustrate how RS-Net can be synergistically combined with Co-Learning techniques for terrain property estimation. Initial results demonstrate the effectiveness of this approach in characterizing spectral signatures across an extensive off-road real-world dataset. These findings highlight the feasibility of terrain property estimation using only RGB cameras."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Dynamic Data Layout Optimization with Worst-case Guarantees", "authors": "Kexin Rong, Paul Liu, Sarah Ashok Sonje, Moses Charikar", "subjects": "Subjects:\nDatabases (cs.DB); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)", "abstract": "Many data analytics systems store and process large datasets in partitions containing millions of rows. By mapping rows to partitions in an optimized way, it is possible to improve query performance by skipping over large numbers of irrelevant partitions during query processing. This mapping is referred to as a data layout. Recent works have shown that customizing the data layout to the anticipated query workload greatly improves query performance, but the performance benefits may disappear if the workload changes. Reorganizing data layouts to accommodate workload drift can resolve this issue, but reorganization costs could exceed query savings if not done carefully. In this paper, we present an algorithmic framework OReO that makes online reorganization decisions to balance the benefits of improved query performance with the costs of reorganization. Our framework extends results from Metrical Task Systems to provide a tight bound on the worst-case performance guarantee for online reorganization, without prior knowledge of the query workload. Through evaluation on real-world datasets and query workloads, our experiments demonstrate that online reorganization with OReO can lead to an up to 32% improvement in combined query and reorganization time compared to using a single, optimized data layout for the entire workload."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          An Artificial Intelligence Approach for Interpreting Creative Combinational Designs", "authors": "Liuqing Chen, Shuhong Xiao, Yunnong Chen, Linyun Sun, Peter R.N. Childs, Ji Han", "subjects": "Subjects:\nArtificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)", "abstract": "Combinational creativity, a form of creativity involving the blending of familiar ideas, is pivotal in design innovation. While most research focuses on how combinational creativity in design is achieved through blending elements, this study focuses on the computational interpretation, specifically identifying the 'base' and 'additive' components that constitute a creative design. To achieve this goal, the authors propose a heuristic algorithm integrating computer vision and natural language processing technologies, and implement multiple approaches based on both discriminative and generative artificial intelligence architectures. A comprehensive evaluation was conducted on a dataset created for studying combinational creativity. Among the implementations of the proposed algorithm, the most effective approach demonstrated a high accuracy in interpretation, achieving 87.5% for identifying 'base' and 80% for 'additive'. We conduct a modular analysis and an ablation experiment to assess the performance of each part in our implementations. Additionally, the study includes an analysis of error cases and bottleneck issues, providing critical insights into the limitations and challenges inherent in the computational interpretation of creative designs."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          TeraPool-SDR: An 1.89TOPS 1024 RV-Cores 4MiB Shared-L1 Cluster for Next-Generation Open-Source Software-Defined Radios", "authors": "Yichao Zhang, Marco Bertuletti, Samuel Riedel, Matheus Cavalcante, Alessandro Vanelli-Coralli, Luca Benini", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC); Hardware Architecture (cs.AR)", "abstract": "Radio Access Networks (RAN) workloads are rapidly scaling up in data processing intensity and throughput as the 5G (and beyond) standards grow in number of antennas and sub-carriers. Offering flexible Processing Elements (PEs), efficient memory access, and a productive parallel programming model, many-core clusters are a well-matched architecture for next-generation software-defined RANs, but staggering performance requirements demand a high number of PEs coupled with extreme Power, Performance and Area (PPA) efficiency. We present the architecture, design, and full physical implementation of Terapool-SDR, a cluster for Software Defined Radio (SDR) with 1024 latency-tolerant, compact RV32 PEs, sharing a global view of a 4MiB, 4096-banked, L1 memory. We report various feasible configurations of TeraPool-SDR featuring an ultra-high bandwidth PE-to-L1-memory interconnect, clocked at 730MHz, 880MHz, and 924MHz (TT/0.80 V/25 \u00b0C) in 12nm FinFET technology. The TeraPool-SDR cluster achieves high energy efficiency on all SDR key kernels for 5G RANs: Fast Fourier Transform (93GOPS/W), Matrix-Multiplication (125GOPS/W), Channel Estimation (96GOPS/W), and Linear System Inversion (61GOPS/W). For all the kernels, it consumes less than 10W, in compliance with industry standards."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Health Index Estimation Through Integration of General Knowledge with Unsupervised Learning", "authors": "Kristupas Bajarunas, Marcia L. Baptista, Kai Goebel, Manuel A. Chao", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Accurately estimating a Health Index (HI) from condition monitoring data (CM) is essential for reliable and interpretable prognostics and health management (PHM) in complex systems. In most scenarios, complex systems operate under varying operating conditions and can exhibit different fault modes, making unsupervised inference of an HI from CM data a significant challenge. Hybrid models combining prior knowledge about degradation with deep learning models have been proposed to overcome this challenge. However, previously suggested hybrid models for HI estimation usually rely heavily on system-specific information, limiting their transferability to other systems. In this work, we propose an unsupervised hybrid method for HI estimation that integrates general knowledge about degradation into the convolutional autoencoder's model architecture and learning algorithm, enhancing its applicability across various systems. The effectiveness of the proposed method is demonstrated in two case studies from different domains: turbofan engines and lithium batteries. The results show that the proposed method outperforms other competitive alternatives, including residual-based methods, in terms of HI quality and their utility for Remaining Useful Life (RUL) predictions. The case studies also highlight the comparable performance of our proposed method with a supervised model trained with HI labels."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          On Stochastic Fundamental Limits in a Downlink Integrated Sensing and Communication Network", "authors": "Marziyeh Soltani, Mahtab Mirmohseni, Rahim Tafazolli", "subjects": "Subjects:\nInformation Theory (cs.IT)", "abstract": "This paper aims to analyze the stochastic performance of a multiple input multiple output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario, where a base station (BS) transmits a dual-functional radar-communication (DFRC) signal matrix, serving the purpose of transmitting communication data to the user while simultaneously sensing the angular location of a target. The channel between the BS and the user is modeled as a random channel with Rayleigh fading distribution, and the azimuth angle of the target is assumed to follow a uniform distribution. Due to the randomness inherent in the network, the challenge is to consider suitable performance metrics for this randomness. To address this issue, for users, we employ the user's rate outage probability (OP) and ergodic rate, while for target, we propose using the OP of the Cram\u00e9r-Rao lower bound (CRLB) for the angle of arrival and the ergodic CRLB. We have obtained the expressions of these metrics for scenarios where the BS employs two different beamforming methods. Our approach to deriving these metrics involves computing the probability density function (PDF) of the signal-to-noise ratio for users and the CRLB for the target. We have demonstrated that the central limit theorem provides a viable approach for deriving these PDFs. In our numerical results, we demonstrate the trade-off between sensing and communication (S \\& C) by characterizing the region of S \\& C metrics and by obtaining the Pareto optimal boundary points, confirmed with simulations."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          NAVRepair: Node-type Aware C/C++ Code Vulnerability Repair", "authors": "Ruoke Wang, Zongjie Li, Chaozheng Wang, Yang Xiao, Cuiyun Gao", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "The rapid advancement of deep learning has led to the development of Large Language Models (LLMs). In the field of vulnerability repair, previous research has leveraged rule-based fixing, pre-trained models, and LLM's prompt engineering. However, existing approaches have limitations in terms of the integration of code structure with error types. Besides, due to certain features of C/C++ language, vulnerability repair in C/C++ proves to be exceptionally challenging. To address these challenges, we propose NAVRepair, a novel framework that combines the node-type information extracted from Abstract Syntax Trees (ASTs) with error types, specifically targeting C/C++ vulnerabilities. Specifically, our approach employs type analysis to localize the minimum edit node (MEN) and customizes context information collection based on different error types. In the offline stage, NAVRepair parses code patches to locate MENs and designs rules to extract relevant contextual information for each MEN type. In the online repairing stage, it analyzes the suspicious code, combines it with vulnerability type templates derived from the Common Weakness Enumeration (CWE), and generates targeted repair prompts. We evaluate NAVRepair on multiple popular LLMs and demonstrate its effectiveness in improving the performance of code vulnerability repair. Notably, our framework is independent of any specific LLMs and can quickly adapt to new vulnerability types. Extensive experiments validate that NAVRepair achieves excellent results in assisting LLMs to accurately detect and fix C/C++ vulnerabilities. We achieve a 26% higher accuracy compared to an existing LLM-based C/C++ vulnerability repair method. We believe our node type-aware approach has promising application prospects for enhancing real-world C/C++ code security."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Bridging the Gap Between Saliency Prediction and Image Quality Assessment", "authors": "Kirillov Alexey, Andrey Moskalenko, Dmitriy Vatolin", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "Over the past few years, deep neural models have made considerable advances in image quality assessment (IQA). However, the underlying reasons for their success remain unclear, owing to the complex nature of deep neural networks. IQA aims to describe how the human visual system (HVS) works and to create its efficient approximations. On the other hand, Saliency Prediction task aims to emulate HVS via determining areas of visual interest. Thus, we believe that saliency plays a crucial role in human perception. In this work, we conduct an empirical study that reveals the relation between IQA and Saliency Prediction tasks, demonstrating that the former incorporates knowledge of the latter. Moreover, we introduce a novel SACID dataset of saliency-aware compressed images and conduct a large-scale comparison of classic and neural-based IQA methods. All supplementary code and data will be available at the time of publication."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Axiomatization of approximate exclusion", "authors": "Matilda H\u00e4ggblom", "subjects": "Subjects:\nLogic in Computer Science (cs.LO); Logic (math.LO)", "abstract": "We define and axiomatize approximate exclusion atoms in the team semantic setting. A team is a set of assignments, which can be seen as a mathematical model of a uni-relational database, and we say that an approximate exclusion atom is satisfied in a team if the corresponding usual exclusion atom is satisfied in a large enough subteam. We consider the implication problem for approximate exclusion atoms and show that it is axiomatizable for consequences with a degree of approximation that is not too large. We prove the completeness theorem for usual exclusion atoms, which is currently missing from the literature, and generalize it to approximate exclusion atoms. We also provide a polynomial time algorithm for the implication problems. The results also apply to exclusion dependencies in database theory."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          HMANet: Hybrid Multi-Axis Aggregation Network for Image Super-Resolution", "authors": "Shu-Chuan Chu, Zhi-Chao Dou, Jeng-Shyang Pan, Shaowei Weng, Junbao Li", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Transformer-based methods have demonstrated excellent performance on super-resolution visual tasks, surpassing conventional convolutional neural networks. However, existing work typically restricts self-attention computation to non-overlapping windows to save computational costs. This means that Transformer-based networks can only use input information from a limited spatial range. Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA) is proposed in this paper to exploit feature potential information better. HMA is constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid Attention Blocks(GAB). On the one side, RHTB combines channel attention and self-attention to enhance non-local feature fusion and produce more attractive visual results. Conversely, GAB is used in cross-domain information interaction to jointly model similar features and obtain a larger perceptual field. For the super-resolution task in the training phase, a novel pre-training method is designed to enhance the model representation capabilities further and validate the proposed model's effectiveness through many experiments. The experimental results show that HMA outperforms the state-of-the-art methods on the benchmark dataset. We provide code and models at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking", "authors": "Pengcheng Shao, Tianyang Xu, Zhangyong Tang, Linze Li, Xiao-Jun Wu, Josef Kittler", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion. However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data. To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity. In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes. The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF). Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively. Our code will be available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Analyzing design principles for competitive evolution strategies in constrained search spaces", "authors": "Michael Hellwig, Hans-Georg Beyer", "subjects": "Subjects:\nNeural and Evolutionary Computing (cs.NE)", "abstract": "In the context of the 2018 IEEE Congress of Evolutionary Computation, the Matrix Adaptation Evolution Strategy for constrained optimization turned out to be notably successful in the competition on constrained single objective real-parameter optimization. Across all considered instances the so-called $\\epsilon$MAg-ES achieved the second rank. However, it can be considered to be the most successful participant in high dimensions. Unfortunately, the competition result does not provide any information about the modus operandi of a successful algorithm or its suitability for problems of a particular shape. To this end, the present paper is concerned with an extensive empirical analysis of the $\\epsilon$MAg-ES working principles that is expected to provide insights about the performance contribution of specific algorithmic components. To avoid rankings with respect to insignificant differences within the algorithm realizations, the paper additionally introduces significance testing into the ranking process."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ADELIE: Aligning Large Language Models on Information Extraction", "authors": "Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks. This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data. In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE. We first collect and construct a high-quality alignment corpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on IEInstruct. We further train ADELIE_SFT with direct preference optimization (DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO) achieve state-of-the-art (SoTA) performance among open-source models. We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline. We will release the code, data, and models to facilitate further research."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields", "authors": "Ning Wang, Lefei Zhang, Angel X Chang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Neural fields (NeRF) have emerged as a promising approach for representing continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs poses a significant challenge for scene decomposition. To address this challenge, we present a single model, Multi-Modal Decomposition NeRF (${M^2D}$NeRF), that is capable of both text-based and visual patch-based edits. Specifically, we use multi-modal feature distillation to integrate teacher features from pretrained visual and language models into 3D semantic feature volumes, thereby facilitating consistent 3D editing. To enforce consistency between the visual and language features in our 3D feature volumes, we introduce a multi-modal similarity constraint. We also introduce a patch-based joint contrastive loss that helps to encourage object-regions to coalesce in the 3D feature space, resulting in more precise boundaries. Experiments on various real-world scenes show superior performance in 3D scene decomposition tasks compared to prior NeRF-based methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Entropy Enigma: Success and Failure of Entropy Minimization", "authors": "Ori Press, Ravid Shwartz-Ziv, Yann LeCun, Matthias Bethge", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time. EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes. In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy. After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\\%$, an improvement of $29.62\\%$ over the previous SoTA on this task. Our code is available at this https URL"}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Concrete Dense Network for Long-Sequence Time Series Clustering", "authors": "Redemptor Jr Laceda Taloma, Patrizio Pisani, Danilo Comminiello", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "Time series clustering is fundamental in data analysis for discovering temporal patterns. Despite recent advancements, learning cluster-friendly representations is still challenging, particularly with long and complex time series. Deep temporal clustering methods have been trying to integrate the canonical k-means into end-to-end training of neural networks but fall back on surrogate losses due to the non-differentiability of the hard cluster assignment, yielding sub-optimal solutions. In addition, the autoregressive strategy used in the state-of-the-art RNNs is subject to error accumulation and slow training, while recent research findings have revealed that Transformers are less effective due to time points lacking semantic meaning, to the permutation invariance of attention that discards the chronological order and high computation cost. In light of these observations, we present LoSTer which is a novel dense autoencoder architecture for the long-sequence time series clustering problem (LSTC) capable of optimizing the k-means objective via the Gumbel-softmax reparameterization trick and designed specifically for accurate and fast clustering of long time series. Extensive experiments on numerous benchmark datasets and two real-world applications prove the effectiveness of LoSTer over state-of-the-art RNNs and Transformer-based deep clustering methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          TGTM: TinyML-based Global Tone Mapping for HDR Sensors", "authors": "Peter Todorov, Julian Hartig, Jan Meyer-Siemon, Martin Fiedler, Gregor Schewior", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "Advanced driver assistance systems (ADAS) relying on multiple cameras are increasingly prevalent in vehicle technology. Yet, conventional imaging sensors struggle to capture clear images in conditions with intense illumination contrast, such as tunnel exits, due to their limited dynamic range. Introducing high dynamic range (HDR) sensors addresses this issue. However, the process of converting HDR content to a displayable range via tone mapping often leads to inefficient computations, when performed directly on pixel data. In this paper, we focus on HDR image tone mapping using a lightweight neural network applied on image histogram data. Our proposed TinyML-based global tone mapping method, termed as TGTM, operates at 9,000 FLOPS per RGB image of any resolution. Additionally, TGTM offers a generic approach that can be incorporated to any classical tone mapping method. Experimental results demonstrate that TGTM outperforms state-of-the-art methods on real HDR camera images by up to 5.85 dB higher PSNR with orders of magnitude less computations."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          6G Software Engineering: A Systematic Mapping Study", "authors": "Ruoyu Su, Xiaozhou Li, Davide Taibi", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "6G will revolutionize the software world allowing faster cellular communications and a massive number of connected devices. 6G will enable a shift towards a continuous edge-to-cloud architecture. Current cloud solutions, where all the data is transferred and computed in the cloud, are not sustainable in such a large network of devices. Current technologies, including development methods, software architectures, and orchestration and offloading systems, still need to be prepared to cope with such requirements. In this paper, we conduct a Systematic Mapping Study to investigate the current research status of 6G Software Engineering. Results show that 18 research papers have been proposed in software process, software architecture, orchestration and offloading methods. Of these, software architecture and software-defined networks are respectively areas and topics that have received the most attention in 6G Software Engineering. In addition, the main types of results of these papers are methods, architectures, platforms, frameworks and algorithms. For the five tools/frameworks proposed, they are new and not currently studied by other researchers. The authors of these findings are mainly from China, India and Saudi Arabia. The results will enable researchers and practitioners to further research and extend for 6G Software Engineering."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          AI-based Dynamic Schedule Calculation in Time Sensitive Networks using GCN-TD3", "authors": "Syed Tasnimul Islam, Anas Bin Muslim", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI)", "abstract": "Offline scheduling in Time Sensitive Networking (TSN) utilizing the Time Aware Shaper (TAS) facilitates optimal deterministic latency and jitter-bounds calculation for Time- Triggered (TT) flows. However, the dynamic nature of traffic in industrial settings necessitates a strategy for adaptively scheduling flows without interrupting existing schedules. Our research identifies critical gaps in current dynamic scheduling methods for TSN and introduces the novel GCN-TD3 approach. This novel approach utilizes a Graph Convolutional Network (GCN) for representing the various relations within different components of TSN and employs the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to dynamically schedule any incoming flow. Additionally, an Integer Linear Programming (ILP) based offline scheduler is used both to initiate the simulation and serve as a fallback mechanism. This mechanism is triggered to recalculate the entire schedule when the predefined threshold of Gate Control List(GCL) length exceeds. Comparative analyses demonstrate that GCN-TD3 outperforms existing methods like Deep Double Q-Network (DDQN) and Deep Deterministic Policy Gradient (DDPG), exhibiting convergence within 4000 epochs with a 90\\% dynamic TT flow admission rate while maintaining deadlines and reducing jitter to as low as 2us. Finally, two modules were developed for the OMNeT++ simulator, facilitating dynamic simulation to evaluate the methodology."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Quantum Circuit Ansatz: Abstraction and Reuse of Quantum Algorithm Design", "authors": "Xiaoyu Guo, Takahiro Muta, Jianjun Zhao", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "Quantum computing holds the potential to revolutionize various fields by efficiently tackling complex problems. At its core are quantum circuits, sequences of quantum gates manipulating quantum states. The selection of the right quantum circuit ansatz, which defines initial circuit structures and serves as the basis for optimization techniques, is crucial in quantum algorithm design.This paper presents a categorized catalog of quantum circuit ansatzes aimed at supporting quantum algorithm design and implementation. Each ansatz is described with details such as intent, motivation, applicability, circuit diagram, implementation, example, and see also. Practical examples are provided to illustrate their application in quantum algorithm design.The catalog aims to assist quantum algorithm designers by offering insights into the strengths and limitations of different ansatzes, thereby facilitating decision-making for specific tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Adversarial Threats to Automatic Modulation Open Set Recognition in Wireless Networks", "authors": "Yandie Yang, Sicheng Zhang, Kuixian Li, Qiao Tian, Yun Lin", "subjects": "Subjects:\nCryptography and Security (cs.CR); Social and Information Networks (cs.SI)", "abstract": "Automatic Modulation Open Set Recognition (AMOSR) is a crucial technological approach for cognitive radio communications, wireless spectrum management, and interference monitoring within wireless networks. Numerous studies have shown that AMR is highly susceptible to minimal perturbations carefully designed by malicious attackers, leading to misclassification of signals. However, the adversarial security issue of AMOSR has not yet been explored. This paper adopts the perspective of attackers and proposes an Open Set Adversarial Attack (OSAttack), aiming at investigating the adversarial vulnerabilities of various AMOSR methods. Initially, an adversarial threat model for AMOSR scenarios is established. Subsequently, by analyzing the decision criteria of both discriminative and generative open set recognition, OSFGSM and OSPGD are proposed to reduce the performance of AMOSR. Finally, the influence of OSAttack on AMOSR is evaluated utilizing a range of qualitative and quantitative indicators. The results indicate that despite the increased resistance of AMOSR models to conventional interference signals, they remain vulnerable to attacks by adversarial examples."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          HackCar: a test platform for attacks and defenses on a cost-contained automotive architecture", "authors": "Dario Stabili, Filip Valgimigli, Edoardo Torrini, Mirco Marchetti", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "In this paper, we introduce the design of HackCar, a testing platform for replicating attacks and defenses on a generic automotive system without requiring access to a complete vehicle. This platform empowers security researchers to illustrate the consequences of attacks targeting an automotive system on a realistic platform, facilitating the development and testing of security countermeasures against both existing and novel attacks. The HackCar platform is built upon an F1-10th model, to which various automotive-grade microcontrollers are connected through automotive communication protocols. This solution is crafted to be entirely modular, allowing for the creation of diverse test scenarios. Researchers and practitioners can thus develop innovative security solutions while adhering to the constraints of automotive-grade microcontrollers. We showcase our design by comparing it with a real, licensed, and unmodified vehicle. Additionally, we analyze the behavior of the HackCar in both an attack-free scenario and a scenario where an attack on in-vehicle communication is deployed."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          StyleMamba : State Space Model for Efficient Text-driven Image Style Transfer", "authors": "Zijia Wang, Zhi-Song Liu", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "abstract": "We present StyleMamba, an efficient image style transfer framework that translates text prompts into corresponding visual styles while preserving the content integrity of the original images. Existing text-guided stylization requires hundreds of training iterations and takes a lot of computing resources. To speed up the process, we propose a conditional State Space Model for Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that sequentially aligns the image features to the target text prompts. To enhance the local and global style consistency between text and image, we propose masked and second-order directional losses to optimize the stylization direction to significantly reduce the training iterations by 5 times and the inference time by 3 times. Extensive experiments and qualitative evaluation confirm the robust and superior stylization performance of our methods compared to the existing baselines."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Stability And Uncertainty Propagation In Power Networks: A Lyapunov-based Approach With Applications To Renewable Resources Allocation", "authors": "Mohamad Kazma, Ahmad F. Taha", "subjects": "Subjects:\nSystems and Control (eess.SY)", "abstract": "The rapid increase in the integration of intermittent and stochastic renewable energy resources (RER) introduces challenging issues related to power system stability. Interestingly, identifying grid nodes that can best support stochastic loads from RER, has gained recent interest. Methods based on Lyapunov stability are commonly exploited to assess the stability of power networks. These strategies approach quantifying system stability while considering: (i) simplified reduced order power system models that do not model power flow constraints, or (ii) datadriven methods that are prone to measurement noise and hence can inaccurately depict stochastic loads as system instability. In this paper, while considering a nonlinear differential algebraic equation (NL-DAE) model, we introduce a new method for assessing the impact of uncertain renewable power injections on the stability of power system nodes/buses. The identification of stable nodes informs the operator/utility on how renewables injections affect the stability of the grid. The proposed method is based on optimizing metrics equivalent to the Lyapunov spectrum of exponents; its underlying properties result in a computationally efficient and scalable stable node identification algorithm for renewable energy resources allocation. The proposed method is validated on the IEEE 9-bus and 200-bus networks"}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Functional Specifications and Testing Requirements of Grid-Forming Type-IV Offshore Wind Power", "authors": "Sulav Ghimire, Gabriel M.G. Guerreiro, Kanakesh V.K., Emerson D. Guest, Kim H. Jensen, Guangya Yang, Xiongfei Wang", "subjects": "Subjects:\nSystems and Control (eess.SY)", "abstract": "Throughout the past few years, various transmission system operators (TSOs) and research institutes have defined several functional specifications for grid-forming (GFM) converters via grid codes, white papers, and technical documents. These institutes and organisations also proposed testing requirements for general inverter-based resources (IBRs) and specific GFM converters. This paper initially reviews functional specifications and testing requirements from several sources to create an understanding of GFM capabilities in general. Furthermore, it proposes an outlook of the defined GFM capabilities, functional specifications, and testing requirements for offshore wind power plant (OF WPP) applications from an original equipment manufacturer (OEM) perspective. Finally, this paper briefly establishes the relevance of new testing methodologies for equipment-level certification and model validation, focusing on GFM functional specifications."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Mitigating Bias Using Model-Agnostic Data Attribution", "authors": "Sander De Coninck, Wei-Cheng Wang, Sam Leroux, Pieter Simoens", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Mitigating bias in machine learning models is a critical endeavor for ensuring fairness and equity. In this paper, we propose a novel approach to address bias by leveraging pixel image attributions to identify and regularize regions of images containing significant information about bias attributes. Our method utilizes a model-agnostic approach to extract pixel attributions by employing a convolutional neural network (CNN) classifier trained on small image patches. By training the classifier to predict a property of the entire image using only a single patch, we achieve region-based attributions that provide insights into the distribution of important information across the image. We propose utilizing these attributions to introduce targeted noise into datasets with confounding attributes that bias the data, thereby constraining neural networks from learning these biases and emphasizing the primary attributes. Our approach demonstrates its efficacy in enabling the training of unbiased classifiers on heavily biased datasets."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Multi-fidelity Hamiltonian Monte Carlo", "authors": "Dhruv V. Patel, Jonghyun Lee, Matthew W. Farthing, Peter K. Kitanidis, Eric F. Darve", "subjects": "Subjects:\nComputational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "Numerous applications in biology, statistics, science, and engineering require generating samples from high-dimensional probability distributions. In recent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a state-of-the-art Markov chain Monte Carlo technique, exploiting the shape of such high-dimensional target distributions to efficiently generate samples. Despite its impressive empirical success and increasing popularity, its wide-scale adoption remains limited due to the high computational cost of gradient calculation. Moreover, applying this method is impossible when the gradient of the posterior cannot be computed (for example, with black-box simulators). To overcome these challenges, we propose a novel two-stage Hamiltonian Monte Carlo algorithm with a surrogate model. In this multi-fidelity algorithm, the acceptance probability is computed in the first stage via a standard HMC proposal using an inexpensive differentiable surrogate model, and if the proposal is accepted, the posterior is evaluated in the second stage using the high-fidelity (HF) numerical solver. Splitting the standard HMC algorithm into these two stages allows for approximating the gradient of the posterior efficiently, while producing accurate posterior samples by using HF numerical solvers in the second stage. We demonstrate the effectiveness of this algorithm for a range of problems, including linear and nonlinear Bayesian inverse problems with in-silico data and experimental data. The proposed algorithm is shown to seamlessly integrate with various low-fidelity and HF models, priors, and datasets. Remarkably, our proposed method outperforms the traditional HMC algorithm in both computational and statistical efficiency by several orders of magnitude, all while retaining or improving the accuracy in computed posterior statistics."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Dissipativity Conditions for Maximum Dynamic Loadability", "authors": "Riley Lawson, Marija Ilic", "subjects": "Subjects:\nSystems and Control (eess.SY)", "abstract": "In this paper we consider a possibility of stabilizing very fast electromagnetic interactions between Inverter Based Resources (IBRs), known as the Control Induced System Stability problems. We propose that when these oscillatory interactions are controlled the ability of the grid to deliver power to loads at high rates will be greatly increased. We refer to this grid property as the dynamic grid loadability. The approach is to start by modeling the dynamical behavior of all components. Next, to avoid excessive complexity, interactions between components are captured in terms of unified technology-agnostic aggregate variables, instantaneous power and rate of change of instantaneous reactive power. Sufficient dissipativity conditions in terms of rate of change of energy conversion in components themselves and bounds on their rate of change of interactions are derived in support of achieving the maximum system loadability. These physically intuitive conditions are then used to derive methods to increase loadability using high switching frequency reactive power sources. Numerical simulations confirm the theoretical calculations, and shows dynamic load-side reactive power support increases stable dynamic loadability regions."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Reviewing Intelligent Cinematography: AI research for camera-based video production", "authors": "Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)", "abstract": "This paper offers a comprehensive review of artificial intelligence (AI) research in the context of real camera content acquisition for entertainment purposes and is aimed at both researchers and cinematographers. Considering the breadth of computer vision research and the lack of review papers tied to intelligent cinematography (IC), this review introduces a holistic view of the IC landscape while providing the technical insight for experts across across disciplines. We preface the main discussion with technical background on generative AI, object detection, automated camera calibration and 3-D content acquisition, and link explanatory articles to assist non-technical readers. The main discussion categorizes work by four production types: General Production, Virtual Production, Live Production and Aerial Production. Note that for Virtual Production we do not discuss research relating to virtual content acquisition, including work on automated video generation, like Stable Diffusion. Within each section, we (1) sub-classify work by the technical field of research - reflected by the subsections, and (2) evaluate the trends and challenge w.r.t to each type of production. In the final chapter, we present our concluding remarks on the greater scope of IC research and outline work that we believe has significant potential to influence the whole industry. We find that work relating to virtual production has the greatest potential to impact other mediums of production, driven by the growing interest in LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture for a virtual modelling of real world scenes and actors. This is the first piece of literature to offer a structured and comprehensive examination of IC research. Consequently, we address ethical and legal concerns regarding the use of creative AI involving artists, actors and the general public, in the..."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Gr\\\"obner Basis Cryptanalysis of Ciminion and Hydra", "authors": "Matthias Johann Steiner", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "Ciminion and Hydra are two recently introduced symmetric key Pseudo-Random Functions for Multi-Party Computation applications. For efficiency both primitives utilize quadratic permutations at round level. Therefore, polynomial system solving-based attacks pose a serious threat to these primitives. For Ciminion we construct a quadratic degree reverse lexicographic (DRL) Gr\u00f6bner basis for the iterated polynomial model via affine transformations. For Hydra we provide a computer-aided proof in SageMath that a quadratic DRL Gr\u00f6bner basis is already contained within the iterated polynomial system for the Hydra heads after affine transformations and a linear change of coordinates. Our Ciminion DRL Gr\u00f6bner basis simplifies cryptanalysis, since one does not need to impose genericity assumptions, like being regular or semi-regular, anymore to derive complexity estimates on key recovery attacks. In the Hydra proposal it was claimed that $r_\\mathcal{H} = 31$ rounds for the heads are sufficient to achieve $128$ bits of security against Gr\u00f6bner basis attacks for key recovery. However, for $r_\\mathcal{H} = 31$ standard term order conversion to a lexicographic (LEX) Gr\u00f6bner basis for our Hydra DRL Gr\u00f6bner basis requires just $126$ bits. Moreover, via the Eigenvalue Method up to $r_\\mathcal{H} = 33$ rounds can be attacked below $128$ bits."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          An adaptive finite element multigrid solver using GPU acceleration", "authors": "Manuel Liebchen, Utku Kaya, Christian Lessig, Thomas Richter", "subjects": "Subjects:\nNumerical Analysis (math.NA)", "abstract": "Adaptive finite elements combined with geometric multigrid solvers are one of the most efficient numerical methods for problems such as the instationary Navier-Stokes equations. Yet despite their efficiency, computations remain expensive and the simulation of, for example, complex flow problems can take many hours or days. GPUs provide an interesting avenue to speed up the calculations due to their very large theoretical peak performance. However, the large degree of parallelism and non-standard API make the use of GPUs in scientific computing challenging. In this work, we develop a GPU acceleration for the adaptive finite element library Gascoigne and study its effectiveness for different systems of partial differential equations. Through the systematic formulation of all computations as linear algebra operations, we can employ GPU-accelerated linear algebra libraries, which simplifies the implementation and ensures the maintainability of the code while achieving very efficient GPU utilizations. Our results for a transport-diffusion equation, linear elasticity, and the instationary Navier-Stokes equations show substantial speedups of up to 20X compared to multi-core CPU implementations."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources", "authors": "Lasse Hyldig Hansen, Nikolaj Andersen, Jack Gallifant, Liam G. McCoy, James K Stone, Nura Izath, Marcela Aguirre-Jerez, Danielle S Bitterman, Judy Gichoya, Leo Anthony Celi", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Background Advancements in Large Language Models (LLMs) hold transformative potential in healthcare, however, recent work has raised concern about the tendency of these models to produce outputs that display racial or gender biases. Although training data is a likely source of such biases, exploration of disease and demographic associations in text data at scale has been limited. Methods We conducted a large-scale textual analysis using a dataset comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl. The study analyzed the context in which various diseases are discussed alongside markers of race and gender. Given that LLMs are pre-trained on similar datasets, this approach allowed us to examine the potential biases that LLMs may learn and internalize. We compared these findings with actual demographic disease prevalence as well as GPT-4 outputs in order to evaluate the extent of bias representation. Results Our findings indicate that demographic terms are disproportionately associated with specific disease concepts in online texts. gender terms are prominently associated with disease concepts, while racial terms are much less frequently associated. We find widespread disparities in the associations of specific racial and gender terms with the 18 diseases analyzed. Most prominently, we see an overall significant overrepresentation of Black race mentions in comparison to population proportions. Conclusions Our results highlight the need for critical examination and transparent reporting of biases in LLM pretraining datasets. Our study suggests the need to develop mitigation strategies to counteract the influence of biased training data in LLMs, particularly in sensitive domains such as healthcare."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Real-Time Motion Detection Using Dynamic Mode Decomposition", "authors": "Marco Mignacca, Simone Brugiapaglia, Jason J. Bramburger", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit timeseries data to a linear dynamical system. In doing so, DMD decomposes dynamic data into spatially coherent modes that evolve in time according to exponential growth/decay or with a fixed frequency of oscillation. A prolific application of DMD has been to video, where one interprets the high-dimensional pixel space evolving through time as the video plays. In this work, we propose a simple and interpretable motion detection algorithm for streaming video data rooted in DMD. Our method leverages the fact that there exists a correspondence between the evolution of important video features, such as foreground motion, and the eigenvalues of the matrix which results from applying DMD to segments of video. We apply the method to a database of test videos which emulate security footage under varying realistic conditions. Effectiveness is analyzed using receiver operating characteristic curves, while we use cross-validation to optimize the threshold parameter that identifies movement."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric Information", "authors": "Lorenzo Montano-Oliv\u00e1n, Julio A. Placed, Luis Montano, Mar\u00eda T. L\u00e1zaro", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "Localization in already mapped environments is a critical component in many robotics and automotive applications, where previously acquired information can be exploited along with sensor fusion to provide robust and accurate localization estimates. In this work, we offer a new perspective on map-based localization by reusing prior topological and metric information. Thus, we reformulate this long-studied problem to go beyond the mere use of metric maps. Our framework seamlessly integrates LiDAR, iner\\-tial and GNSS measurements, and scan-to-map registrations in a sliding window graph fashion, which allows to accommodate the uncertainty of each observation. The modularity of our framework allows it to work with different sensor configurations (\\textit{e.g.,} LiDAR resolutions, GNSS denial) and environmental conditions (\\textit{e.g.,} map-less regions, large environments). We have conducted different validation experiments, including deployment in a real-world automotive application, demonstrating the accuracy, efficiency, and versatility of our system in online localization."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models", "authors": "Aylin Gunal, Baihan Lin, Djallel Bouneffouf", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems. In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals. The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model. We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model's output as synthetic labels for fine-tuning a large language model for the same task. Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Impact of Tone-Aware Explanations in Recommender Systems", "authors": "Ayano Okoso, Keisuke Otaki, Satoshi Koide, Yukino Baba", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Information Retrieval (cs.IR)", "abstract": "In recommender systems, the presentation of explanations plays a crucial role in supporting users' decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Controlling Borda Elections by Adding or Deleting either Votes or Candidates: Complete and Top-Truncated Votes", "authors": "Aizhong Zhou, Fengbo Wang, Jiong Guo", "subjects": "Subjects:\nComputational Complexity (cs.CC); Computer Science and Game Theory (cs.GT)", "abstract": "An election is defined as a pair of a set of candidates C=\\{c_1,\\cdots,c_m\\} and a multiset of votes V=\\{v_1,\\cdots,v_n\\}, where each vote is a linear order of the candidates. The Borda election rule is characterized by a vector \\langle m-1,m-2,\\cdots,0\\rangle, which means that the candidate ranked at the i-th position of a vote v receives a score m-i from v, and the candidate receiving the most score from all votes wins the election. Here, we consider the control problems of a Borda election, where the chair of the election attempts to influence the election outcome by adding or deleting either votes or candidates with the intention to make a special candidate win (constructive control) or lose (destructive control) the election. Control problems have been extensively studied for Borda elections from both classical and parameterized complexity viewpoints. We complete the parameterized complexity picture for Borda control problems by showing W[2]-hardness with the number of additions/deletions as parameter for constructive control by deleting votes, adding candidates, or deleting candidates. The hardness result for deleting votes settles an open problem posed by Liu and Zhu. Following the suggestion by Menon and Larson, we also investigate the impact of introducing top-truncated votes, where each voter ranks only t out of the given m candidates, on the classical and parameterized complexity of Borda control problems. Constructive Borda control problems remain NP-hard even with t being a small constant. Moreover, we prove that in the top-truncated case, constructive control by adding/deleting votes problems are FPT with the number \\ell of additions/deletions and t as parameters, while for every constant t\\geq 2, constructive control by adding/deleting candidates problems are W[2]-hard with respect to \\ell."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Designing Skill-Compatible AI: Methodologies and Frameworks in Chess", "authors": "Karim Hamade, Reid McIlroy-Young, Siddhartha Sen, Jon Kleinberg, Ashton Anderson", "subjects": "Subjects:\nArtificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)", "abstract": "Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Novel Actor-Critic Algorithm for Robust Decision Making of CAV under Delays and Loss of V2X Data", "authors": "Zine el abidine Kherroubi", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Current autonomous driving systems heavily rely on V2X communication data to enhance situational awareness and the cooperation between vehicles. However, a major challenge when using V2X data is that it may not be available periodically because of unpredictable delays and data loss during wireless transmission between road stations and the receiver vehicle. This issue should be considered when designing control strategies for connected and autonomous vehicles. Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm that guarantees robust driving performance in V2X environment with delayed and/or lost data. The novel algorithm incorporates three key mechanisms: a virtual fixed sampling period, a combination of Temporal-Difference and Monte Carlo learning, and a numerical approximation of immediate reward values. To address the temporal aperiodicity problem of V2X data, we first illustrate this challenge. Then, we provide a detailed explanation of the Blind Actor-Critic algorithm where we highlight the proposed components to compensate for the temporal aperiodicity problem of V2X data. We evaluate the performance of our algorithm in a simulation environment and compare it to benchmark approaches. The results demonstrate that training metrics are improved compared to conventional actor-critic algorithms. Additionally, testing results show that our approach provides robust control, even under low V2X network reliability levels."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations", "authors": "Xuyang Zhong, Yixiao Huang, Chen Liu", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "This work studies sparse adversarial perturbations bounded by $l_0$ norm. We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations. Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks. Codes are available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment", "authors": "Simon Weber, Je Hyeong Hong, Daniel Cremers", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Initialization-free bundle adjustment (BA) remains largely uncharted. While Levenberg-Marquardt algorithm is the golden method to solve the BA problem, it generally relies on a good initialization. In contrast, the under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization. Coupled with object space error formulation, recent works have shown its ability to solve (small-scale) initialization-free bundle adjustment problem. We introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series. Importantly, we link the power series expansion to Riemannian manifold optimization. This projective framework is crucial to solve large-scale bundle adjustment problem without initialization. Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy. In particular, our work is the first, to our knowledge, that addresses the scalability of BA without initialization and opens new venues for initialization-free Structure-from-Motion."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Concerns on Bias in Large Language Models when Creating Synthetic Personae", "authors": "Helena A. Haxvig", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs). These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them. The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Committee Elections with Candidate Attribute Constraints", "authors": "Aizhong Zhou, Fengbo Wang, Jiong Guo", "subjects": "Subjects:\nComputational Complexity (cs.CC); Computer Science and Game Theory (cs.GT)", "abstract": "In many real-world applications of committee elections, the candidates are associated with certain attributes and the chosen committee is required to satisfy some constraints posed on the candidate attributes. For instance, when dress collocation, it is generally acknowledged that when wearing a tie, you'd better wear a shirt, and wearing a suit, you'd better wear leather shoes. Here, dresses are categorized by upper garment, lower garment, shoes this http URL, and upper garment is with the attribute tie and shirt, lower garment is with the attribute suit, and shoes is with the attribute leather. And two constraints \"tie infers shirt\" and \"suit infers leather shoes\" are proposed. We study this variant of committee elections from the computational complexity viewpoint. Given a set of candidates, each with some attributes and a profit, and a set of constraints, given as propositional logical expressions of the attributes, the task is to compute a set of k candidates, whose attributes satisfy all constraints and whose total profit achieves a given bound. We achieve a dichotomy concerning classical complexity with no length limit on constraints: the problem is polynomial-time solvable, if the following two conditions are fulfilled: 1) each candidate has only one attribute and 2) each attribute occurs at most once in the constraints. It becomes NP-hard if one of the two conditions is violated. Moreover, we examine its parameterized complexity. The parameterization with the number of constraints, the size of the committee, or the total profit bound as parameter leads to para-NP-hardness or W[1]-hardness, while with the number of attributes or the number of candidates as parameter, the problem turns out to be fixed-parameter tractable."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Fair Voting Outcomes with Impact and Novelty Compromises? Unraveling Biases of Equal Shares in Participatory Budgeting", "authors": "Sajan Maharjan, Srijoni Majumdar, Evangelos Pournaras", "subjects": "Subjects:\nMultiagent Systems (cs.MA)", "abstract": "Participatory budgeting, as a paradigm for democratic innovations, engages citizens in the distribution of a public budget to projects, which they propose and vote for implementation. So far, voting algorithms have been devised and studied in social choice literature to elect projects that are popular, while others prioritize on a proportional representation of voters' preferences, for instance, equal shares. However, the anticipated impact and novelty in the broader society by the winning projects, as selected by different algorithms, remains totally under-explored, lacking both a universal theory of impact for voting and a rigorous framework for impact and novelty assessments. This papers tackles this grand challenge towards new axiomatic foundations for designing effective and fair voting methods. This is via new and striking insights derived from a large-scale analysis of biases over 345 real-world voting outcomes, characterized for the first time by a novel portfolio of impact and novelty metrics. We find strong causal evidence that equal shares comes with impact loss in several infrastructural projects of different cost levels that have been so far over-represented. However, it also comes with a novel, yet over-represented, impact gain in welfare, education and culture. We discuss broader implications of these results and how impact loss can be mitigated at the stage of campaign design and project ideation."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Cryptocurrency Risk, Trust, and Acceptance in Thailand: A Comparative Study with Switzerland", "authors": "Kanyanut Suriyan, Tim Weingaertner", "subjects": "Subjects:\nComputational Engineering, Finance, and Science (cs.CE)", "abstract": "The adoption of the Pao Tang digital wallet in Thailand, promoted under the Khon la Krueng (50-50 Co-Payment) Scheme, illustrates Thailand's receptiveness to digital financial instruments, amassing over 40 million users in just three years during the COVID-19 social distancing era. Nevertheless, acceptance of this platform does not confirm a broad understanding of cryptocurrencies and Web 3.0 technologies in the region. Through a mix of documentary research, online surveys and a targeted interview with the Pao Tang app's founder, this study evaluates the factors behind the Pao Tang platform's success and contrasts it with digital practices in Switzerland. Preliminary outcomes reveal a pronounced knowledge gap in Thailand regarding decentralized technologies. With regulatory frameworks for Web 3.0 and digital currencies still nascent, this research underscores the need for further exploration, serving as a blueprint for shaping strategies, policies, and awareness campaigns in both countries."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Approximation properties relative to continuous scale space for hybrid discretizations of Gaussian derivative operators", "authors": "Tony Lindeberg", "subjects": "Subjects:\nNumerical Analysis (math.NA); Computer Vision and Pattern Recognition (cs.CV)", "abstract": "This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences. The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels. While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels. In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Rapid Co-design of Task-Specialized Whegged Robots for Ad-Hoc Needs", "authors": "Varun Madabushi, Katie M. Popek, Craig Knuth, Galen Mullins, Brian A. Bittner", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "In this work, we investigate the use of co-design methods to iterate upon robot designs in the field, performing time sensitive, ad-hoc tasks. Our method optimizes the morphology and wheg trajectory for a MiniRHex robot, producing 3D printable structures and leg trajectory parameters. Tested in four terrains, we show that robots optimized in simulation exhibit strong sim-to-real transfer and are nearly twice as efficient as the nominal platform when tested in hardware."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks", "authors": "Jarek Duda", "subjects": "Subjects:\nMachine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "Popular artificial neural networks (ANN) optimize parameters for unidirectional value propagation, assuming some guessed parametrization type like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In contrast, for biological neurons e.g. \"it is not uncommon for axonal propagation of action potentials to happen in both directions\" \\cite{axon} - suggesting they are optimized to continuously operate in multidirectional way. Additionally, statistical dependencies a single neuron could model is not just (expected) value dependence, but entire joint distributions including also higher moments. Such agnostic joint distribution neuron would allow for multidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or $\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing. There will be discussed Hierarchical Correlation Reconstruction (HCR) for such neuron model: assuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type parametrization of joint distribution with polynomial basis $f_i$, which allows for flexible, inexpensive processing including nonlinearities, direct model estimation and update, trained through standard backpropagation or novel ways for such structure up to tensor decomposition. Using only pairwise (input-output) dependencies, its expected value prediction becomes KAN-like with trained activation functions as polynomials, can be extended by adding higher order dependencies through included products - in conscious interpretable way, allowing for multidirectional propagation of both values and probability densities."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Energy stable gradient flow schemes for shape and topology optimization in Navier-Stokes flows", "authors": "Jiajie Li, Shengfeng Zhu", "subjects": "Subjects:\nNumerical Analysis (math.NA); Optimization and Control (math.OC); Fluid Dynamics (physics.flu-dyn)", "abstract": "We study topology optimization governed by the incompressible Navier-Stokes flows using a phase field model. Novel stabilized semi-implicit schemes for the gradient flows of Allen-Cahn and Cahn-Hilliard types are proposed for solving the resulting optimal control problem. Unconditional energy stability is shown for the gradient flow schemes in continuous and discrete spaces. Numerical experiments of computational fluid dynamics in 2d and 3d show the effectiveness and robustness of the optimization algorithms proposed."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Fundamental Limits for Jammer-Resilient Communication in Finite-Resolution MIMO", "authors": "Gian Marti, Alexander Stutz-Tirri, Christoph Studer", "subjects": "Subjects:\nInformation Theory (cs.IT)", "abstract": "Spatial filtering based on multiple-input multiple-output (MIMO) processing is a powerful method for jammer mitigation. In principle, a MIMO receiver can null the interference of a single-antenna jammer at the cost of only one degree of freedom - if the number of receive antennas is large, communication performance is barely affected. In this paper, we show that the potential for MIMO jammer mitigation based on the digital outputs of finite-resolution analog-to-digital converters (ADCs) is fundamentally worse: Strong jammers will either cause the ADCs to saturate (when the ADCs' quantization range is small) or drown legitimate communication signals in quantization noise (when the ADCs' quantization range is large). We provide a fundamental bound on the mutual information between the quantized receive signal and the legitimate transmit signal. Our bound shows that, for any fixed ADC resolution, the mutual information tends to zero as the jammer power tends to infinity. Our bound also confirms the intuition that for every 6.02 dB increase in jamming power, the ADC resolution must be increased by 1 bit in order to prevent the mutual information from vanishing."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Leveraging AES Padding: dBs for Nothing and FEC for Free in IoT Systems", "authors": "Jongchan Woo, Vipindev Adat Vasudevan, Benjamin D. Kim, Rafael G. L. D'Oliveira, Alejandro Cohen, Thomas Stahlbuhk, Ken R. Duffy, Muriel M\u00e9dard", "subjects": "Subjects:\nEmerging Technologies (cs.ET); Hardware Architecture (cs.AR); Systems and Control (eess.SY)", "abstract": "The Internet of Things (IoT) represents a significant advancement in digital technology, with its rapidly growing network of interconnected devices. This expansion, however, brings forth critical challenges in data security and reliability, especially under the threat of increasing cyber vulnerabilities. Addressing the security concerns, the Advanced Encryption Standard (AES) is commonly employed for secure encryption in IoT systems. Our study explores an innovative use of AES, by repurposing AES padding bits for error correction and thus introducing a dual-functional method that seamlessly integrates error-correcting capabilities into the standard encryption process. The integration of the state-of-the-art Guessing Random Additive Noise Decoder (GRAND) in the receiver's architecture facilitates the joint decoding and decryption process. This strategic approach not only preserves the existing structure of the transmitter but also significantly enhances communication reliability in noisy environments, achieving a notable over 3 dB gain in Block Error Rate (BLER). Remarkably, this enhanced performance comes with a minimal power overhead at the receiver - less than 15% compared to the traditional decryption-only process, underscoring the efficiency of our hardware design for IoT applications. This paper discusses a comprehensive analysis of our approach, particularly in energy efficiency and system performance, presenting a novel and practical solution for reliable IoT communications."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs", "authors": "Weijia Zhang, Vaishali Pal, Jia-Hong Huang, Evangelos Kanoulas, Maarten de Rijke", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)", "abstract": "Table summarization is a crucial task aimed at condensing information from tabular data into concise and comprehensible textual summaries. However, existing approaches often fall short of adequately meeting users' information and quality requirements and tend to overlook the complexities of real-world queries. In this paper, we propose a novel method to address these limitations by introducing query-focused multi-table summarization. Our approach, which comprises a table serialization module, a summarization controller, and a large language model (LLM), utilizes textual queries and multiple tables to generate query-dependent table summaries tailored to users' information needs. To facilitate research in this area, we present a comprehensive dataset specifically tailored for this task, consisting of 4909 query-summary pairs, each associated with multiple tables. Through extensive experiments using our curated dataset, we demonstrate the effectiveness of our proposed method compared to baseline approaches. Our findings offer insights into the challenges of complex table reasoning for precise summarization, contributing to the advancement of research in query-focused multi-table summarization."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples", "authors": "Peiqin Lin, Andr\u00e9 F. T. Martins, Hinrich Sch\u00fctze", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, presents challenges due to the scarcity of available cross-lingual retrievers and annotated data. In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever with positive/negative English samples, which are constructed based on the predictions of the multilingual large language model for in-context learning. Then, the trained retriever is directly employed to retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on the massively multilingual text classification benchmark of SIB200 with 176 languages demonstrate that XAMPLER substantially improves the in-context learning performance across languages. Our code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          (In)Security of Mobile Apps in Developing Countries: A Systematic Literature Review", "authors": "Alioune Diallo, Jordan Samhi, Tegawend\u00e9 Bissyand\u00e9, Jacques Klein", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "In developing countries, several key sectors, including education, finance, agriculture, and healthcare, mainly deliver their services via mobile app technology on handheld devices. As a result, mobile app security has emerged as a paramount issue in developing countries. In this paper, we investigate the state of research on mobile app security, focusing on developing countries. More specifically, we performed a systematic literature review exploring the research directions taken by existing works, the different security concerns addressed, and the techniques used by researchers to highlight or address app security issues. Our main findings are: (1) the literature includes only a few studies on mobile app security in the context of developing countries ; (2) among the different security concerns that researchers study, vulnerability detection appears to be the leading research topic; (3) FinTech apps are revealed as the main target in the relevant literature. Overall, our work highlights that there is largely room for developing further specialized techniques addressing mobile app security in the context of developing countries."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Full Version: (De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms", "authors": "Ari Rasch", "subjects": "Subjects:\nProgramming Languages (cs.PL)", "abstract": "We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Correlation and Autocorrelation of Data on Complex Networks", "authors": "Rudy Arthur", "subjects": "Subjects:\nSocial and Information Networks (cs.SI); Physics and Society (physics.soc-ph)", "abstract": "Networks where each node has one or more associated numerical values are common in applications. This work studies how summary statistics used for the analysis of spatial data can be applied to non-spatial networks for the purposes of exploratory data analysis. We focus primarily on Moran-type statistics and discuss measures of global autocorrelation, local autocorrelation and global correlation. We introduce null models based on fixing edges and permuting the data or fixing the data and permuting the edges. We demonstrate the use of these statistics on real and synthetic node-valued networks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Exploring Speech Pattern Disorders in Autism using Machine Learning", "authors": "Chuanbo Hu, Jacob Thrasher, Wenqi Li, Mindi Ruan, Xiangxu Yu, Lynn K Paul, Shuo Wang, Xin Li", "subjects": "Subjects:\nSound (cs.SD); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)", "abstract": "Diagnosing autism spectrum disorder (ASD) by identifying abnormal speech patterns from examiner-patient dialogues presents significant challenges due to the subtle and diverse manifestations of speech-related symptoms in affected individuals. This study presents a comprehensive approach to identify distinctive speech patterns through the analysis of examiner-patient dialogues. Utilizing a dataset of recorded dialogues, we extracted 40 speech-related features, categorized into frequency, zero-crossing rate, energy, spectral characteristics, Mel Frequency Cepstral Coefficients (MFCCs), and balance. These features encompass various aspects of speech such as intonation, volume, rhythm, and speech rate, reflecting the complex nature of communicative behaviors in ASD. We employed machine learning for both classification and regression tasks to analyze these speech features. The classification model aimed to differentiate between ASD and non-ASD cases, achieving an accuracy of 87.75%. Regression models were developed to predict speech pattern related variables and a composite score from all variables, facilitating a deeper understanding of the speech dynamics associated with ASD. The effectiveness of machine learning in interpreting intricate speech patterns and the high classification accuracy underscore the potential of computational methods in supporting the diagnostic processes for ASD. This approach not only aids in early detection but also contributes to personalized treatment planning by providing insights into the speech and communication profiles of individuals with ASD."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Web Intelligence Journal in perspective: an analysis of its two decades trajectory", "authors": "Diogenes Ademir Domingos, Victor Emanuel Santos Moura, Antonio Fernando Lavareda Jacob Junior, Fabio Manoel Franca Lobato", "subjects": "Subjects:\nSocial and Information Networks (cs.SI)", "abstract": "The evolution of a thematic area undergoes various changes of perspective and adopts new theoretical approaches that arise from the interactions of the community and a wide range of social needs. The advent of digital technologies, such as social networks, underlines this factor by spreading knowledge and forging links between different communities. Web intelligence is now on the verge of raising questions that broaden the understanding of how artificial intelligence impacts the Web of People, Data, and Things, among other factors. To the best of our knowledge, there is no study that has conducted a longitudinal analysis of the evolution of this community. Thus, we investigate in this paper how Web intelligence has evolved in the last twenty years by carrying out a literature review and bibliometric analysis. Concerning the impact of this research study, increasing attention is devoted to determining which are the most influential papers in the community by referring to citation networks and discovering the most popular and pressing topics through a co-citation analysis and the keywords co-occurrence. The results obtained can guide the direction of new research projects in the area and update the scope and places of interest found in current trends and the relevant journals."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal Violence Detection", "authors": "Shengyang Sun, Xiaojin Gong", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)", "abstract": "Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available. In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges. In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges. Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features. Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features. Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance. Code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DenserRadar: A 4D millimeter-wave radar point cloud detector based on dense LiDAR point clouds", "authors": "Zeyu Han, Junkai Jiang, Xiaokang Ding, Qingwen Meng, Shaobing Xu, Lei He, Jianqiang Wang", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "The 4D millimeter-wave (mmWave) radar, with its robustness in extreme environments, extensive detection range, and capabilities for measuring velocity and elevation, has demonstrated significant potential for enhancing the perception abilities of autonomous driving systems in corner-case scenarios. Nevertheless, the inherent sparsity and noise of 4D mmWave radar point clouds restrict its further development and practical application. In this paper, we introduce a novel 4D mmWave radar point cloud detector, which leverages high-resolution dense LiDAR point clouds. Our approach constructs dense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a specially designed network named DenserRadar. The proposed method surpasses existing probability-based and learning-based radar point cloud detectors in terms of both point cloud density and accuracy on the K-Radar dataset."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Low-Distortion Clustering in Bounded Growth Graphs", "authors": "Yi-Jun Chang, Varsha Dani, Thomas P. Hayes", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)", "abstract": "The well-known clustering algorithm of Miller, Peng, and Xu (SPAA 2013) is useful for many applications, including low-diameter decomposition and low-energy distributed algorithms. One nice property of their clustering, shown in previous work by Chang, Dani, Hayes, and Pettie (PODC 2020), is that distances in the cluster graph are rescaled versions of distances in the original graph, up to an $O(\\log n)$ distortion factor and rounding issues. Minimizing this distortion factor is important for efficiency in computing the clustering, as well as in other applications. We prove that there exist graphs for which an $\\Omega((\\log n)^{1/3})$ distortion factor is necessary for any clustering. We also consider a class of nice graphs which we call uniformly bounded independence graphs. These include, for example, paths, lattice graphs, and \"dense\" unit disk graphs. For these graphs, we prove that clusterings of distortion $O(1)$ always exist, and moreover, we give new efficient distributed algorithms to construct them. This clustering is based on Voronoi cells centered at the vertices of a maximal independent set in a suitable power graph. Applications include low-energy simulation of distributed algorithms in the LOCAL, CONGEST, and RADIO-CONGEST models and efficient approximate solutions to distributed combinatorial optimization problems. We also investigate related lower bounds."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Identifying every building's function in large-scale urban areas with multi-modality remote-sensing data", "authors": "Zhuohong Li, Wei He, Jiepan Li, Hongyan Zhang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "Buildings, as fundamental man-made structures in urban environments, serve as crucial indicators for understanding various city function zones. Rapid urbanization has raised an urgent need for efficiently surveying building footprints and functions. In this study, we proposed a semi-supervised framework to identify every building's function in large-scale urban areas with multi-modality remote-sensing data. In detail, optical images, building height, and nighttime-light data are collected to describe the morphological attributes of buildings. Then, the area of interest (AOI) and building masks from the volunteered geographic information (VGI) data are collected to form sparsely labeled samples. Furthermore, the multi-modality data and weak labels are utilized to train a segmentation model with a semi-supervised strategy. Finally, results are evaluated by 20,000 validation points and statistical survey reports from the government. The evaluations reveal that the produced function maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings in Shanghai, China. This study has the potential to support large-scale urban management and sustainable urban development. All collected data and produced maps are open access at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Enhancing Deep Knowledge Tracing via Diffusion Models for Personalized Adaptive Learning", "authors": "Ming Kuo, Shouvon Sarker, Lijun Qian, Yujian Fu, Xiangfang Li, Xishuang Dong", "subjects": "Subjects:\nComputers and Society (cs.CY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "In contrast to pedagogies like evidence-based teaching, personalized adaptive learning (PAL) distinguishes itself by closely monitoring the progress of individual students and tailoring the learning path to their unique knowledge and requirements. A crucial technique for effective PAL implementation is knowledge tracing, which models students' evolving knowledge to predict their future performance. Based on these predictions, personalized recommendations for resources and learning paths can be made to meet individual needs. Recent advancements in deep learning have successfully enhanced knowledge tracking through Deep Knowledge Tracing (DKT). This paper introduces generative AI models to further enhance DKT. Generative AI models, rooted in deep learning, are trained to generate synthetic data, addressing data scarcity challenges in various applications across fields such as natural language processing (NLP) and computer vision (CV). This study aims to tackle data shortage issues in student learning records to enhance DKT performance for PAL. Specifically, it employs TabDDPM, a diffusion model, to generate synthetic educational records to augment training data for enhancing DKT. The proposed method's effectiveness is validated through extensive experiments on ASSISTments datasets. The experimental results demonstrate that the AI-generated data by TabDDPM significantly improves DKT performance, particularly in scenarios with small data for training and large data for testing."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Lessons from the Use of Natural Language Inference (NLI) in Requirements Engineering Tasks", "authors": "Mohamad Fazelnia, Viktoria Koscinski, Spencer Herzog, Mehdi Mirakhorli", "subjects": "Subjects:\nSoftware Engineering (cs.SE); Computation and Language (cs.CL); Machine Learning (cs.LG)", "abstract": "We investigate the use of Natural Language Inference (NLI) in automating requirements engineering tasks. In particular, we focus on three tasks: requirements classification, identification of requirements specification defects, and detection of conflicts in stakeholders' requirements. While previous research has demonstrated significant benefit in using NLI as a universal method for a broad spectrum of natural language processing tasks, these advantages have not been investigated within the context of software requirements engineering. Therefore, we design experiments to evaluate the use of NLI in requirements analysis. We compare the performance of NLI with a spectrum of approaches, including prompt-based models, conventional transfer learning, Large Language Models (LLMs)-powered chatbot models, and probabilistic models. Through experiments conducted under various learning settings including conventional learning and zero-shot, we demonstrate conclusively that our NLI method surpasses classical NLP methods as well as other LLMs-based and chatbot models in the analysis of requirements specifications. Additionally, we share lessons learned characterizing the learning settings that make NLI a suitable approach for automating requirements engineering tasks."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Integrating LSTM and BERT for Long-Sequence Data Analysis in Intelligent Tutoring Systems", "authors": "Zhaoxing Li, Jujie Yang, Jindi Wang, Lei Shi, Sebastian Stein", "subjects": "Subjects:\nComputers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "abstract": "The field of Knowledge Tracing aims to understand how students learn and master knowledge over time by analyzing their historical behaviour data. To achieve this goal, many researchers have proposed Knowledge Tracing models that use data from Intelligent Tutoring Systems to predict students' subsequent actions. However, with the development of Intelligent Tutoring Systems, large-scale datasets containing long-sequence data began to emerge. Recent deep learning based Knowledge Tracing models face obstacles such as low efficiency, low accuracy, and low interpretability when dealing with large-scale datasets containing long-sequence data. To address these issues and promote the sustainable development of Intelligent Tutoring Systems, we propose a LSTM BERT-based Knowledge Tracing model for long sequence data processing, namely LBKT, which uses a BERT-based architecture with a Rasch model-based embeddings block to deal with different difficulty levels information and an LSTM block to process the sequential characteristic in students' actions. LBKT achieves the best performance on most benchmark datasets on the metrics of ACC and AUC. Additionally, an ablation study is conducted to analyse the impact of each component of LBKT's overall performance. Moreover, we used t-SNE as the visualisation tool to demonstrate the model's embedding strategy. The results indicate that LBKT is faster, more interpretable, and has a lower memory cost than the traditional deep learning based Knowledge Tracing methods."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Dynamic Size Counting in the Population Protocol Model", "authors": "Dominik Kaaser, Maximilian Lohmann", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC)", "abstract": "The population protocol model describes collections of distributed agents that interact in pairs to solve a common task. We consider a dynamic variant of this prominent model, where we assume that an adversary may change the population size at an arbitrary point in time. In this model we tackle the problem of counting the population size: in the dynamic size counting problem the goal is to design an algorithm that computes an approximation of $\\log n$. This estimate can be used to turn static, non-uniform population protocols, i.e., protocols that depend on the population size $n$, into dynamic and loosely-stabilizing protocols. Our contributions in this paper are three-fold. Starting from an arbitrary initial configuration, we first prove that the agents converge quickly to a valid configuration where each agent has a constant-factor approximation of $\\log n$, and once the agents reach such a valid configuration, they stay in it for a polynomial number of time steps. Second, we show how to use our protocol to define a uniform and loosely-stabilizing phase clock for the population protocol model. Finally, we support our theoretical findings by empirical simulations that show that our protocols work well in practice."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Distributed Learning for Wi-Fi AP Load Prediction", "authors": "Dariush Salami, Francesc Wilhelmi, Lorenzo Galati-Giordano, Mika Kasslin", "subjects": "Subjects:\nNetworking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "The increasing cloudification and softwarization of networks foster the interplay among multiple independently managed deployments. An appealing reason for such an interplay lies in distributed Machine Learning (ML), which allows the creation of robust ML models by leveraging collective intelligence and computational power. In this paper, we study the application of the two cornerstones of distributed learning, namely Federated Learning (FL) and Knowledge Distillation (KD), on the Wi-Fi Access Point (AP) load prediction use case. The analysis conducted in this paper is done on a dataset that contains real measurements from a large Wi-Fi campus network, which we use to train the ML model under study based on different strategies. Performance evaluation includes relevant aspects for the suitability of distributed learning operation in real use cases, including the predictive performance, the associated communication overheads, or the energy consumption. In particular, we prove that distributed learning can improve the predictive accuracy centralized ML solutions by up to 93% while reducing the communication overheads and the energy cost by 80%."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Learning-to-learn enables rapid learning with phase-change memory-based in-memory computing", "authors": "Thomas Ortner, Horst Petschenig, Athanasios Vasilopoulos, Roland Renner, \u0160pela Brglez, Thomas Limbacher, Enrique Pi\u00f1ero, Alejandro Linares Barranco, Angeliki Pantazi, Robert Legenstein", "subjects": "Subjects:\nNeural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)", "abstract": "There is a growing demand for low-power, autonomously learning artificial intelligence (AI) systems that can be applied at the edge and rapidly adapt to the specific situation at deployment site. However, current AI models struggle in such scenarios, often requiring extensive fine-tuning, computational resources, and data. In contrast, humans can effortlessly adjust to new tasks by transferring knowledge from related ones. The concept of learning-to-learn (L2L) mimics this process and enables AI models to rapidly adapt with only little computational effort and data. In-memory computing neuromorphic hardware (NMHW) is inspired by the brain's operating principles and mimics its physical co-location of memory and compute. In this work, we pair L2L with in-memory computing NMHW based on phase-change memory devices to build efficient AI models that can rapidly adapt to new tasks. We demonstrate the versatility of our approach in two scenarios: a convolutional neural network performing image classification and a biologically-inspired spiking neural network generating motor commands for a real robotic arm. Both models rapidly learn with few parameter updates. Deployed on the NMHW, they perform on-par with their software equivalents. Moreover, meta-training of these models can be performed in software with high-precision, alleviating the need for accurate hardware models."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Ordinal Behavior Classification of Student Online Course Interactions", "authors": "Thomas Trask", "subjects": "Subjects:\nComputers and Society (cs.CY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "The study in interaction patterns between students in on-campus and MOOC-style online courses has been broadly studied for the last 11 years. Yet there remains a gap in the literature comparing the habits of students completing the same course offered in both on-campus and MOOC-style online formats. This study will look at browser-based usage patterns for students in the Georgia Tech CS1301 edx course for both the online course offered to on-campus students and the MOOCstyle course offered to anyone to determine what, if any, patterns exist between the two cohorts."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Learning Object Semantic Similarity with Self-Supervision", "authors": "Arthur Aubret, Timothy Schauml\u00f6ffel, Gemma Roig, Jochen Triesch", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)", "abstract": "Humans judge the similarity of two objects not just based on their visual appearance but also based on their semantic relatedness. However, it remains unclear how humans learn about semantic relationships between objects and categories. One important source of semantic knowledge is that semantically related objects frequently co-occur in the same context. For instance, forks and plates are perceived as similar, at least in part, because they are often experienced together in a ``kitchen\" or ``eating'' context. Here, we investigate whether a bio-inspired learning principle exploiting such co-occurrence statistics suffices to learn a semantically structured object representation {\\em de novo} from raw visual or combined visual and linguistic input. To this end, we simulate temporal sequences of visual experience by binding together short video clips of real-world scenes showing objects in different contexts. A bio-inspired neural network model aligns close-in-time visual representations while also aligning visual and category label representations to simulate visuo-language alignment. Our results show that our model clusters object representations based on their context, e.g. kitchen or bedroom, in particular in high-level layers of the network, akin to humans. In contrast, lower-level layers tend to better reflect object identity or category. To achieve this, the model exploits two distinct strategies: the visuo-language alignment ensures that different objects of the same category are represented similarly, whereas the temporal alignment leverages that objects from the same context are frequently seen in succession to make their representations more similar. Overall, our work suggests temporal and visuo-language alignment as plausible computational principles for explaining the origins of certain forms of semantic knowledge in humans."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Improving Automated Distractor Generation for Math Multiple-choice Questions with Overgenerate-and-rank", "authors": "Alexander Scarlatos, Wanyong Feng, Digory Smith, Simon Woodhead, Andrew Lan", "subjects": "Subjects:\nComputers and Society (cs.CY); Machine Learning (cs.LG)", "abstract": "Multiple-choice questions (MCQs) are commonly used across all levels of math education since they can be deployed and graded at a large scale. A critical component of MCQs is the distractors, i.e., incorrect answers crafted to reflect student errors or misconceptions. Automatically generating them in math MCQs, e.g., with large language models, has been challenging. In this work, we propose a novel method to enhance the quality of generated distractors through overgenerate-and-rank, training a ranking model to predict how likely distractors are to be selected by real students. Experimental results on a real-world dataset and human evaluation with math teachers show that our ranking model increases alignment with human-authored distractors, although human-authored ones are still preferred over generated ones."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty", "authors": "Luca Mossina, Joseba Dalmau, L\u00e9o and\u00e9ol", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "We propose a post-hoc, computationally lightweight method to quantify predictive uncertainty in semantic image segmentation. Our approach uses conformal prediction to generate statistically valid prediction sets that are guaranteed to include the ground-truth segmentation mask at a predefined confidence level. We introduce a novel visualization technique of conformalized predictions based on heatmaps, and provide metrics to assess their empirical validity. We demonstrate the effectiveness of our approach on well-known benchmark datasets and image segmentation prediction models, and conclude with practical insights."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Hybrid Convolutional Neural Networks with Reliability Guarantee", "authors": "Hans Dermot Doran, Suzana Veljanovska", "subjects": "Subjects:\nArtificial Intelligence (cs.AI)", "abstract": "Making AI safe and dependable requires the generation of dependable models and dependable execution of those models. We propose redundant execution as a well-known technique that can be used to ensure reliable execution of the AI model. This generic technique will extend the application scope of AI-accelerators that do not feature well-documented safety or dependability properties. Typical redundancy techniques incur at least double or triple the computational expense of the original. We adopt a co-design approach, integrating reliable model execution with non-reliable execution, focusing that additional computational expense only where it is strictly necessary. We describe the design, implementation and some preliminary results of a hybrid CNN."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          The Potential and Implications of Generative AI on HCI Education", "authors": "Ahmed Kharrufa, Ian G Johnson", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines. As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI. In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module. We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions. Our insights are based on replies to a survey sent out to the students after completing the module. Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students' understanding of key concepts and ideas and highlighting knowledge gaps. We also discuss potential pitfalls that should be considered and the need to assess students' literacies and assumptions of GAIs as pedagogical tools. Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice. We end with a discussion of our findings in relation to the TPACK framework in HCI."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          An efficient truncation scheme for Eulerian and total Lagrangian SPH methods", "authors": "Zhentong Wang, Chi Zhang, Oskar J. Haidn, Xiangyu Hu", "subjects": "Subjects:\nComputational Engineering, Finance, and Science (cs.CE)", "abstract": "In smoothed particle hydrodynamics (SPH) method, the particle-based approximations are implemented via kernel functions, and the evaluation of performance involves two key criteria: numerical accuracy and computational efficiency. In the SPH community, the Wendland kernel reigns as the prevailing choice due to its commendable accuracy and reasonable computational efficiency. Nevertheless, there exists an urgent need to enhance the computational efficiency of numerical methods while upholding accuracy. In this paper, we employ a truncation approach to limit the compact support of the Wendland kernel to 1.6h. This decision is based on the observation that particles within the range of 1.6h to 2h make negligible contributions, practically approaching zero, to the SPH approximation. To address integration errors stemming from the truncation, we incorporate the Laguerre-Gauss kernel for particle relaxation due to the fact that this kernel has been demonstrated to enable the attainment of particle distributions with reduced residue and integration errors \\cite{wang2023fourth}. Furthermore, we introduce the kernel gradient correction to rectify numerical errors from the SPH approximation of kernel gradient and the truncated compact support. A comprehensive set of numerical examples including fluid dynamics in Eulerian formulation and solid dynamics in total Lagrangian formulation are tested and have demonstrated that truncated and standard Wendland kernels enable achieve the same level accuracy but the former significantly increase the computational efficiency."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Analysis of the SQP Method for Hyperbolic PDE-Constrained Optimization in Acoustic Full Waveform Inversion", "authors": "Luis Ammann, Irwin Yousept", "subjects": "Subjects:\nNumerical Analysis (math.NA); Optimization and Control (math.OC)", "abstract": "In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered. The model arises from the acoustic full waveform inversion in the time domain. The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure. This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques. We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations. Our final theoretical result is the R-superlinear convergence of the SQP method."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Selective Classification Under Distribution Shifts", "authors": "Hengyue Liang, Le Peng, Ju Sun", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "abstract": "In selective classification (SC), a classifier abstains from making predictions that are likely to be wrong to avoid excessive errors. To deploy imperfect classifiers -- imperfect either due to intrinsic statistical noise of data or for robustness issue of the classifier or beyond -- in high-stakes scenarios, SC appears to be an attractive and necessary path to follow. Despite decades of research in SC, most previous SC methods still focus on the ideal statistical setting only, i.e., the data distribution at deployment is the same as that of training, although practical data can come from the wild. To bridge this gap, in this paper, we propose an SC framework that takes into account distribution shifts, termed generalized selective classification, that covers label-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature. We focus on non-training-based confidence-score functions for generalized SC on deep learning (DL) classifiers and propose two novel margin-based score functions. Through extensive analysis and experiments, we show that our proposed score functions are more effective and reliable than the existing ones for generalized SC on a variety of classification tasks and DL classifiers."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language", "authors": "Julia Krebs, Evie Malaia, Ronnie B. Wilbur, Isabella Fessl, Hans-Peter Wiesinger, Hermann Schwameder, Dietmar Roehm", "subjects": "Subjects:\nComputation and Language (cs.CL); Neurons and Cognition (q-bio.NC)", "abstract": "Across a number of sign languages, temporal and spatial characteristics of dominant hand articulation are used to express semantic and grammatical features. In this study of Austrian Sign Language (\u00d6sterreichische Geb\u00e4rdensprache, or \u00d6GS), motion capture data of four Deaf signers is used to quantitatively characterize the kinematic parameters of sign production in verbs and adjectives. We investigate (1) the difference in production between verbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking an endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in intensified vs. non-intensified (plain) forms. Motion capture data analysis using linear-mixed effects models (LME) indicates that both the endpoint marking in verbs, as well as marking of intensification in adjectives, are expressed by movement modulation in \u00d6GS. While the semantic distinction between verb types (telic/atelic) is marked by higher peak velocity and shorter duration for telic signs compared to atelic ones, the grammatical distinction (intensification) in adjectives is expressed by longer duration for intensified compared to non-intensified adjectives. The observed individual differences of signers might be interpreted as personal signing style."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Dual-Motor Actuator for Ceiling Robots with High Force and High Speed Capabilities", "authors": "Ian Lalonde, Jeff Denis, Mathieu Lamy, Alexandre Girard", "subjects": "Subjects:\nRobotics (cs.RO); Systems and Control (eess.SY)", "abstract": "Patient transfer devices allow to move patients passively in hospitals and care centers. Instead of hoisting the patient, it would be beneficial in some cases to assist their movement, enabling them to move by themselves. However, patient assistance requires devices capable of precisely controlling output forces at significantly higher speeds than those used for patient transfers alone, and a single motor solution would be over-sized and show poor efficiency to do both functions. This paper presents a dual-motor actuator and control schemes adapted for a patient mobility equipment that can be used to transfer patients, assist patient in their movement, and help prevent falls. The prototype is shown to be able to lift patients weighing up to 318 kg, to assist a patient with a desired force of up to 100 kg with a precision of 7.8%. Also, a smart control scheme to manage falls is shown to be able to stop a patient who is falling by applying a desired deceleration."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with Probability Map Guided Multi-Format Feature Fusion", "authors": "Bing Zhu, Zixin He, Weiyi Xiong, Guanhua Ding, Jianan Liu, Tao Huang, Wei Chen, Wei Xiang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks. However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied. This has been a long-standing hindrance to the improvement of pose estimation accuracy. To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method. ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body. Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %. The emphasis of our study is focusing on the position information that is not exploited before in radar singal. This provides direction to investigate other potential non-redundant information from mmWave rader."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Picking watermarks from noise (PWFN): an improved robust watermarking model against intensive distortions", "authors": "Sijing Xie, Chengxin Zhao, Nan Sun, Wei Li, Hefei Ling", "subjects": "Subjects:\nMultimedia (cs.MM); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "Digital watermarking is the process of embedding secret information by altering images in a way that is undetectable to the human eye. To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-decoder architecture by adding different noises to the noise layer. The decoder then extracts the watermarked information from the distorted image. However, this method can only resist weak noise attacks. To improve the robustness of the algorithm against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder. The module is aimed at reducing noise and recovering some of the information lost during an attack. Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency. Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities. In addition, ablation experiments show the superiority of our proposed module."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Custom Gradient Estimators are Straight-Through Estimators in Disguise", "authors": "Matt Schoenbauer, Daniele Moro, Lukasz Lew, Andrew Howard", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "Quantization-aware training comes with a fundamental challenge: the derivative of quantization functions such as rounding are zero almost everywhere and nonexistent elsewhere. Various differentiable approximations of quantization functions have been proposed to address this issue. In this paper, we prove that when the learning rate is sufficiently small, a large class of weight gradient estimators is equivalent with the straight through estimator (STE). Specifically, after swapping in the STE and adjusting both the weight initialization and the learning rate in SGD, the model will train in almost exactly the same way as it did with the original gradient estimator. Moreover, we show that for adaptive learning rate algorithms like Adam, the same result can be seen without any modifications to the weight initialization and learning rate. We experimentally show that these results hold for both a small convolutional model trained on the MNIST dataset and for a ResNet50 model trained on ImageNet."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective", "authors": "Huaiyuan Xu, Junliang Chen, Shiyu Meng, Yi Wang, Lap-Pui Chau", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)", "abstract": "3D occupancy perception technology aims to observe and understand dense 3D environments for autonomous vehicles. Owing to its comprehensive perception capability, this technology is emerging as a trend in autonomous driving perception systems, and is attracting significant attention from both industry and academia. Similar to traditional bird's-eye view (BEV) perception, 3D occupancy perception has the nature of multi-source input and the necessity for information fusion. However, the difference is that it captures vertical structures that are ignored by 2D BEV. In this survey, we review the most recent works on 3D occupancy perception, and provide in-depth analyses of methodologies with various input modalities. Specifically, we summarize general network pipelines, highlight information fusion techniques, and discuss effective network training. We evaluate and analyze the occupancy perception performance of the state-of-the-art on the most popular datasets. Furthermore, challenges and future research directions are discussed. We hope this report will inspire the community and encourage more research work on 3D occupancy perception. A comprehensive list of studies in this survey is available in an active repository that continuously collects the latest work: this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Air Gap: Protecting Privacy-Conscious Conversational Agents", "authors": "Eugene Bagdasaryan, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, Daniel Ramage", "subjects": "Subjects:\nCryptography and Security (cs.CR); Computation and Language (cs.CL); Machine Learning (cs.LG)", "abstract": "The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns. While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors. We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand. Grounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task. Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality. For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Encoder-Decoder Framework for Interactive Free Verses with Generation with Controllable High-Quality Rhyming", "authors": "Tommaso Pasini, Alejo L\u00f3pez-\u00c1vila, Husam Quteineh, Gerasimos Lampouras, Jinhua Du, Yubing Wang, Ze Li, Yusen Sun", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Composing poetry or lyrics involves several creative factors, but a challenging aspect of generation is the adherence to a more or less strict metric and rhyming pattern. To address this challenge specifically, previous work on the task has mainly focused on reverse language modeling, which brings the critical selection of each rhyming word to the forefront of each verse. On the other hand, reversing the word order requires that models be trained from scratch with this task-specific goal and cannot take advantage of transfer learning from a Pretrained Language Model (PLM). We propose a novel fine-tuning approach that prepends the rhyming word at the start of each lyric, which allows the critical rhyming decision to be made before the model commits to the content of the lyric (as during reverse language modeling), but maintains compatibility with the word order of regular PLMs as the lyric itself is still generated in left-to-right order. We conducted extensive experiments to compare this fine-tuning against the current state-of-the-art strategies for rhyming, finding that our approach generates more readable text and better rhyming capabilities. Furthermore, we furnish a high-quality dataset in English and 12 other languages, analyse the approach's feasibility in a multilingual context, provide extensive experimental results shedding light on good and bad practices for lyrics generation, and propose metrics to compare methods in the future."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Detection of a piecewise linear crack with one incident wave", "authors": "Xiaoxu Xu, Guanqiu Ma, Guanghui Hu", "subjects": "Subjects:\nNumerical Analysis (math.NA); Analysis of PDEs (math.AP)", "abstract": "This paper is concerned with inverse crack scattering problems for time-harmonic acoustic waves. We prove that a piecewise linear crack with the sound-soft boundary condition in two dimensions can be uniquely determined by the far-field data corresponding to a single incident plane wave or point source. We propose two non-iterative methods for imaging the location and shape of a crack. The first one is a contrast sampling method, while the second one is a variant of the classical factorization method but only with one incoming wave. Newton's iteration method is then employed for getting a more precise reconstruction result. Numerical examples are presented to show the effectiveness of the proposed hybrid method."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Randomized quasi-Monte Carlo and Owen's boundary growth condition: A spectral analysis", "authors": "Yang Liu", "subjects": "Subjects:\nNumerical Analysis (math.NA)", "abstract": "In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen's boundary growth condition [Owen, 2006] via spectral analysis. Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol' sequence, applying the Fourier transform and Walsh--Fourier transform, respectively, for this analysis. Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator's variance closely aligns with the exponent specified in Owen's boundary growth condition for both sequence types. We also provide guidance on choosing the importance sampling density to minimize RQMC estimator variance."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          A score-based particle method for homogeneous Landau equation", "authors": "Yan Huang, Li Wang", "subjects": "Subjects:\nNumerical Analysis (math.NA); Machine Learning (cs.LG)", "abstract": "We propose a novel score-based particle method for solving the Landau equation in plasmas, that seamlessly integrates learning with structure-preserving particle methods [arXiv:1910.03080]. Building upon the Lagrangian viewpoint of the Landau equation, a central challenge stems from the nonlinear dependence of the velocity field on the density. Our primary innovation lies in recognizing that this nonlinearity is in the form of the score function, which can be approximated dynamically via techniques from score-matching. The resulting method inherits the conservation properties of the deterministic particle method while sidestepping the necessity for kernel density estimation in [arXiv:1910.03080]. This streamlines computation and enhances scalability with dimensionality. Furthermore, we provide a theoretical estimate by demonstrating that the KL divergence between our approximation and the true solution can be effectively controlled by the score-matching loss. Additionally, by adopting the flow map viewpoint, we derive an update formula for exact density computation. Extensive examples have been provided to show the efficiency of the method, including a physically relevant case of Coulomb interaction."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning", "authors": "Inderjeet Nair, Lu Wang", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)", "abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs with infinite activity", "authors": "Ariel Neufeld, Philipp Schmocker, Sizhou Wu", "subjects": "Subjects:\nNumerical Analysis (math.NA); Machine Learning (cs.LG); Probability (math.PR); Mathematical Finance (q-fin.MF)", "abstract": "In this paper, we present a randomized extension of the deep splitting algorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)] using random neural networks suitable to approximately solve both high-dimensional nonlinear parabolic PDEs and PIDEs with jumps having (possibly) infinite activity. We provide a full error analysis of our so-called random deep splitting method. In particular, we prove that our random deep splitting method converges to the (unique viscosity) solution of the nonlinear PDE or PIDE under consideration. Moreover, we empirically analyze our random deep splitting method by considering several numerical examples including both nonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of financial derivatives under default risk. In particular, we empirically demonstrate in all examples that our random deep splitting method can approximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within seconds."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Systematic Use of Random Self-Reducibility against Physical Attacks", "authors": "Ferhat Erata, TingHung Chiu, Anthony Etim, Srilalith Nampally, Tejas Raju, Rajashree Ramu, Ruzica Piskac, Timos Antonopoulos, Wenjie Xiong, Jakub Szefer", "subjects": "Subjects:\nCryptography and Security (cs.CR)", "abstract": "This work presents a novel, black-box software-based countermeasure against physical attacks including power side-channel and fault-injection attacks. The approach uses the concept of random self-reducibility and self-correctness to add randomness and redundancy in the execution for protection. Our approach is at the operation level, is not algorithm-specific, and thus, can be applied for protecting a wide range of algorithms. The countermeasure is empirically evaluated against attacks over operations like modular exponentiation, modular multiplication, polynomial multiplication, and number theoretic transforms. An end-to-end implementation of this countermeasure is demonstrated for RSA-CRT signature algorithm and Kyber Key Generation public key cryptosystems. The countermeasure reduced the power side-channel leakage by two orders of magnitude, to an acceptably secure level in TVLA analysis. For fault injection, the countermeasure reduces the number of faults to 95.4% in average."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          SINBAD: Saliency-informed detection of breakage caused by ad blocking", "authors": "Saiid El Hajj Chehade (1), Sandra Siby (2), Carmela Troncoso (1) ((1) EPFL, (2) Imperial College London)", "subjects": "Subjects:\nCryptography and Security (cs.CR); Machine Learning (cs.LG)", "abstract": "Privacy-enhancing blocking tools based on filter-list rules tend to break legitimate functionality. Filter-list maintainers could benefit from automated breakage detection tools that allow them to proactively fix problematic rules before deploying them to millions of users. We introduce SINBAD, an automated breakage detector that improves the accuracy over the state of the art by 20%, and is the first to detect dynamic breakage and breakage caused by style-oriented filter rules. The success of SINBAD is rooted in three innovations: (1) the use of user-reported breakage issues in forums that enable the creation of a high-quality dataset for training in which only breakage that users perceive as an issue is included; (2) the use of 'web saliency' to automatically identify user-relevant regions of a website on which to prioritize automated interactions aimed at triggering breakage; and (3) the analysis of webpages via subtrees which enables fine-grained identification of problematic filter rules."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Agent-Constrained Truthful Two-Facility Location Games", "authors": "Argyrios Deligkas, Mohammad Lotfi, Alexandros A. Voudouris", "subjects": "Subjects:\nComputer Science and Game Theory (cs.GT)", "abstract": "We consider a truthful two-facility location problem in which there is set of agents with private locations on the line of real numbers, and the goal is to place two facilities at different locations chosen from the set of those reported by the agents. Given a feasible solution, each agent suffers an individual cost which is either its total distance to both facilities (sum-variant) or its distance to the farthest facility (max-variant). For both variants, we show tight bounds on the approximation ratio of deterministic and randomized mechanisms in terms of the social cost, the total individual cost of the agents."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Graded Relevance Scoring of Written Essays with Dense Retrieval", "authors": "Salam Albatarni, Sohaila Eltanbouly, Tamer Elsayed", "subjects": "Subjects:\nInformation Retrieval (cs.IR)", "abstract": "Automated Essay Scoring automates the grading process of essays, providing a great advantage for improving the writing proficiency of students. While holistic essay scoring research is prevalent, a noticeable gap exists in scoring essays for specific quality traits. In this work, we focus on the relevance trait, which measures the ability of the student to stay on-topic throughout the entire essay. We propose a novel approach for graded relevance scoring of written essays that employs dense retrieval encoders. Dense representations of essays at different relevance levels then form clusters in the embeddings space, such that their centroids are potentially separate enough to effectively represent their relevance levels. We hence use the simple 1-Nearest-Neighbor classification over those centroids to determine the relevance level of an unseen essay. As an effective unsupervised dense encoder, we leverage Contriever, which is pre-trained with contrastive learning and demonstrated comparable performance to supervised dense retrieval models. We tested our approach on both task-specific (i.e., training and testing on same task) and cross-task (i.e., testing on unseen task) scenarios using the widely used ASAP++ dataset. Our method establishes a new state-of-the-art performance in the task-specific scenario, while its extension for the cross-task scenario exhibited a performance that is on par with the state-of-the-art model for that scenario. We also analyzed the performance of our approach in a more practical few-shot scenario, showing that it can significantly reduce the labeling cost while sacrificing only 10% of its effectiveness."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Guided Combinatorial Algorithms for Submodular Maximization", "authors": "Yixin Chen, Ankur Nath, Chunli Peng, Alan Kuhnle", "subjects": "Subjects:\nData Structures and Algorithms (cs.DS); Discrete Mathematics (cs.DM); Machine Learning (cs.LG)", "abstract": "For constrained, not necessarily monotone submodular maximization, guiding the measured continuous greedy algorithm with a local search algorithm currently obtains the state-of-the-art approximation factor of 0.401 \\citep{buchbinder2023constrained}. These algorithms rely upon the multilinear extension and the Lovasz extension of a submodular set function. However, the state-of-the-art approximation factor of combinatorial algorithms has remained $1/e \\approx 0.367$ \\citep{buchbinder2014submodular}. In this work, we develop combinatorial analogues of the guided measured continuous greedy algorithm and obtain approximation ratio of $0.385$ in $\\oh{ kn }$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. Further, we derandomize these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation", "authors": "Drew Walker, Annie Thorne, Sudeshna Das, Jennifer Love, Hannah LF Cooper, Melvin Livingston III, Abeed Sarker", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs. The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation. These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers. Results: Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of .84 and .79, positive-label recall and precision values ranging from .71 to .86, and accuracies aligning closely with human annotator agreement (.87). Discussion: This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text, and identified trends in stigmatizing language use in an EHR setting. Additional labeled data may help improve lower scare quote model performance. Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Anomaly Detection in Certificate Transparency Logs", "authors": "Richard Ostert\u00e1g, Martin Stanek", "subjects": "Subjects:\nCryptography and Security (cs.CR); Machine Learning (cs.LG)", "abstract": "We propose an anomaly detection technique for X.509 certificates utilizing Isolation Forest. This method can be beneficial when compliance testing with X.509 linters proves unsatisfactory, and we seek to identify anomalies beyond standards compliance. The technique is validated on a sample of certificates from Certificate Transparency logs."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Gamification in Software Engineering Education: a Tertiary Study", "authors": "Simone Tonh\u00e3o, Marcelo Shigenaga, Julio Herculani, Andressa Medeiros, Aline Amaral, Williamson Silva, Thelma Colanzi, Igor Steinmacher", "subjects": "Subjects:\nSoftware Engineering (cs.SE)", "abstract": "As the significance of Software Engineering (SE) professionals continues to grow in the industry, the adoption of gamification techniques for training purposes has gained traction due to its potential to enhance class appeal through game-derived elements. This paper presents a tertiary study investigating the application of gamification in Software Engineering (SE) education. The study was conducted in response to recent systematic literature reviews and mappings on the topic. The findings reveal that the areas of SE most frequently gamified are Software Testing and Software Quality, with competition and cooperation being the most commonly utilized gamification elements. Additionally, the majority of studies focus on structural gamification, where game elements are employed to modify the learning environment without altering the content. The results demonstrate the potential of gamification to improve students' engagement and motivation throughout the SE learning process, while also impacting other aspects such as performance improvement, skill development, and fostering good SE practices. However, caution is advised as unplanned and incorrectly applied gamification measures may lead to significant declines in performance and motivation. (English Version of the paper in Portuguese available here: this http URL"}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          MOTLEE: Collaborative Multi-Object Tracking Using Temporal Consistency for Neighboring Robot Frame Alignment", "authors": "Mason B. Peterson, Parker C. Lusk, Antonio Avila, Jonathan P. How", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "Knowing the locations of nearby moving objects is important for a mobile robot to operate safely in a dynamic environment. Dynamic object tracking performance can be improved if robots share observations of tracked objects with nearby team members in real-time. To share observations, a robot must make up-to-date estimates of the transformation from its coordinate frame to the frame of each neighbor, which can be challenging because of odometry drift. We present Multiple Object Tracking with Localization Error Elimination (MOTLEE), a complete system for a multi-robot team to accurately estimate frame transformations and collaboratively track dynamic objects. To accomplish this, robots use open-set image-segmentation methods to build object maps of their environment and then use our Temporally Consistent Alignment of Frames Filter (TCAFF) to align maps and estimate coordinate frame transformations without any initial knowledge of neighboring robot poses. We show that our method for aligning frames enables a team of four robots to collaboratively track six pedestrians with accuracy similar to that of a system with ground truth localization in a challenging hardware demonstration. The code and hardware dataset are available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Broadcast Channel Synthesis from Shared Randomness", "authors": "Malhar A. Managoli, Vinod M. Prabhakaran", "subjects": "Subjects:\nInformation Theory (cs.IT)", "abstract": "We study the problem of synthesising a two-user broadcast channel using a common message, where each output terminal shares an independent source of randomness with the input terminal. This generalises two problems studied in the literature (Cuff, IEEE Trans. Inform. Theory, 2013; Kurri this http URL., IEEE Trans. Inform. Theory, 2021). We give an inner bound on the tradeoff region between the rates of communication and shared randomness, and a lower bound on the minimum communication rate. Although the bounds presented here are not tight in general, they are tight for some special cases, including the aforementioned problems."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Exponential time propagators for elastodynamics", "authors": "Paavai Pari, Bikash Kanungo, Vikram Gavini", "subjects": "Subjects:\nNumerical Analysis (math.NA); Computational Engineering, Finance, and Science (cs.CE)", "abstract": "We propose a computationally efficient and systematically convergent approach for elastodynamics simulations. We recast the second-order dynamical equation of elastodynamics into an equivalent first-order system of coupled equations, so as to express the solution in the form of a Magnus expansion. With any spatial discretization, it entails computing the exponential of a matrix acting upon a vector. We employ an adaptive Krylov subspace approach to inexpensively and and accurately evaluate the action of the exponential matrix on a vector. In particular, we use an apriori error estimate to predict the optimal Kyrlov subspace size required for each time-step size. We show that the Magnus expansion truncated after its first term provides quadratic and superquadratic convergence in the time-step for nonlinear and linear elastodynamics, respectively. We demonstrate the accuracy and efficiency of the proposed method for one linear (linear cantilever beam) and three nonlinear (nonlinear cantilever beam, soft tissue elastomer, and hyperelastic rubber) benchmark systems. For a desired accuracy in energy, displacement, and velocity, our method allows for $10-100\\times$ larger time-steps than conventional time-marching schemes such as Newmark-$\\beta$ method. Computationally, it translates to a $\\sim$$1000\\times$ and $\\sim$$10-100\\times$ speed-up over conventional time-marching schemes for linear and nonlinear elastodynamics, respectively."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          SPIDER: Improved Succinct Rank and Select Performance", "authors": "Matthew D. Laws, Jocelyn Bliven, Kit Conklin, Elyes Laalai, Samuel McCauley, Zach S. Sturdevant", "subjects": "Subjects:\nData Structures and Algorithms (cs.DS)", "abstract": "Rank and select data structures seek to preprocess a bit vector to quickly answer two kinds of queries: rank(i) gives the number of 1 bits in slots 0 through i, and select(j) gives the first slot s with rank(s) = j. A succinct data structure can answer these queries while using space much smaller than the size of the original bit vector. State of the art succinct rank and select data structures use as little as 4% extra space while answering rank and select queries quickly. Rank queries can be answered using only a handful of array accesses. Select queries can be answered by starting with similar array accesses, followed by a linear scan. Despite these strong results, a tradeoff remains: data structures that use under 4% space are significantly slower at answering rank and select queries than less-space-efficient data structures (using, say, > 20% extra space). In this paper we make significant progress towards closing this gap. We give a new data structure, SPIDER, which uses 3.82% extra space. SPIDER gives the best rank query time for data sets of 8 billion or more bits, even compared to less space-efficient data structures. For select queries, SPIDER outperforms all data structures that use less than 4% space, and significantly closes the gap in select performance between data structures with less than 4% space, and those that use more (over 20%) space. SPIDER makes two main technical contributions. For rank queries, it improves performance by interleaving the metadata with the bit vector to improve cache efficiency. For select queries, it uses predictions to almost eliminate the cost of the linear scan. These predictions are inspired by recent results on data structures with machine-learned predictions, adapted to the succinct data structure setting. Our results hold on both real and synthetic data, showing that these predictions are effective in practice."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models", "authors": "Jinglin Xu, Yijie Guo, Yuxin Peng", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \\textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers", "authors": "Jiuxiang Gu, Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Junze Yin", "subjects": "Subjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)", "abstract": "Large Language Models (LLMs) have profoundly changed the world. Their self-attention mechanism is the key to the success of transformers in LLMs. However, the quadratic computational cost $O(n^2)$ to the length $n$ input sequence is the notorious obstacle for further improvement and scalability in the longer context. In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices. We propose a $\\mathsf{conv}$ basis system, \"similar\" to the rank basis, and show that any lower triangular (attention) matrix can always be decomposed as a sum of $k$ structured convolution matrices in this basis system. We then design an algorithm to quickly decompose the attention matrix into $k$ convolution matrices. Thanks to Fast Fourier Transforms (FFT), the attention {\\it inference} can be computed in $O(knd \\log n)$ time, where $d$ is the hidden dimension. In practice, we have $ d \\ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma. Thus, when $kd = n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. Our approach can avoid explicitly computing the $n \\times n$ attention matrix, which may largely alleviate the quadratic computational complexity. Furthermore, our algorithm works on any input matrices. This work provides a new paradigm for accelerating attention computation in transformers to enable their application to longer contexts."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation", "authors": "Jonas Kohler, Albert Pumarola, Edgar Sch\u00f6nfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, Ali Thabet", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          \"Community Guidelines Make this the Best Party on the Internet\": An In-Depth Study of Online Platforms' Content Moderation Policies", "authors": "Brennan Schaffner, Arjun Nitin Bhagoji, Siyuan Cheng, Jacqueline Mei, Jay L. Shen, Grace Wang, Marshini Chetty, Nick Feamster, Genevieve Lakier, Chenhao Tan", "subjects": "Subjects:\nHuman-Computer Interaction (cs.HC); Social and Information Networks (cs.SI)", "abstract": "Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants", "authors": "Masoud Moghani, Lars Doorenbos, William Chung-Ho Panitch, Sean Huver, Mahdi Azizian, Ken Goldberg, Animesh Garg", "subjects": "Subjects:\nRobotics (cs.RO)", "abstract": "In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions. Project website: this http URL"}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          myAURA: Personalized health library for epilepsy management via knowledge graph sparsification and visualization", "authors": "Rion Brattig Correia, Jordan C. Rozum, Leonard Cross, Jack Felag, Michael Gallant, Ziqi Guo, Bruce W. Herr II, Aehong Min, Deborah Stungis Rocha, Xuan Wang, Katy B\u00f6rner, Wendy Miller, Luis M. Rocha", "subjects": "Subjects:\nInformation Retrieval (cs.IR); Digital Libraries (cs.DL)", "abstract": "Objective: We report the development of the patient-centered myAURA application and suite of methods designed to aid epilepsy patients, caregivers, and researchers in making decisions about care and self-management. Materials and Methods: myAURA rests on the federation of an unprecedented collection of heterogeneous data resources relevant to epilepsy, such as biomedical databases, social media, and electronic health records. A generalizable, open-source methodology was developed to compute a multi-layer knowledge graph linking all this heterogeneous data via the terms of a human-centered biomedical dictionary. Results: The power of the approach is first exemplified in the study of the drug-drug interaction phenomenon. Furthermore, we employ a novel network sparsification methodology using the metric backbone of weighted graphs, which reveals the most important edges for inference, recommendation, and visualization, such as pharmacology factors patients discuss on social media. The network sparsification approach also allows us to extract focused digital cohorts from social media whose discourse is more relevant to epilepsy or other biomedical problems. Finally, we present our patient-centered design and pilot-testing of myAURA, including its user interface, based on focus groups and other stakeholder input. Discussion: The ability to search and explore myAURA's heterogeneous data sources via a sparsified multi-layer knowledge graph, as well as the combination of those layers in a single map, are useful features for integrating relevant information for epilepsy. Conclusion: Our stakeholder-driven, scalable approach to integrate traditional and non-traditional data sources, enables biomedical discovery and data-powered patient self-management in epilepsy, and is generalizable to other chronic conditions."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN Training", "authors": "Renjie Liu, Yichuan Wang, Xiao Yan, Zhenkun Cai, Minjie Wang, Haitian Jiang, Bo Tang, Jinyang Li", "subjects": "Subjects:\nMachine Learning (cs.LG)", "abstract": "Graph neural networks (GNNs) are machine learning models specialized for graph data and widely used in many applications. To train GNNs on large graphs that exceed CPU memory, several systems store data on disk and conduct out-of-core processing. However, these systems suffer from either read amplification when reading node features that are usually smaller than a disk page or degraded model accuracy by treating the graph as disconnected partitions. To close this gap, we build a system called DiskGNN, which achieves high I/O efficiency and thus fast training without hurting model accuracy. The key technique used by DiskGNN is offline sampling, which helps decouple graph sampling from model computation. In particular, by conducting graph sampling beforehand, DiskGNN acquires the node features that will be accessed by model computation, and such information is utilized to pack the target node features contiguously on disk to avoid read amplification. Besides, \\name{} also adopts designs including four-level feature store to fully utilize the memory hierarchy to cache node features and reduce disk access, batched packing to accelerate the feature packing process, and pipelined training to overlap disk access with other operations. We compare DiskGNN with Ginex and MariusGNN, which are state-of-the-art systems for out-of-core GNN training. The results show that DiskGNN can speed up the baselines by over 8x while matching their best model accuracy."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          RACH Traffic Prediction in Massive Machine Type Communications", "authors": "Hossein Mehri, Hao Chen, Hani Mehrpouyan", "subjects": "Subjects:\nSystems and Control (eess.SY); Machine Learning (cs.LG)", "abstract": "Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks. However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments. Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks. This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks. The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns. Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network. Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios. We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics. Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Stability and Performance Analysis of Discrete-Time ReLU Recurrent Neural Networks", "authors": "Sahel Vahedi Noori, Bin Hu, Geir Dullerud, Peter Seiler", "subjects": "Subjects:\nSystems and Control (eess.SY); Machine Learning (cs.LG); Optimization and Control (math.OC)", "abstract": "This paper presents sufficient conditions for the stability and $\\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions. These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU. Our stability and performance condition uses these QCs along with a \"lifted\" representation for the ReLU RNN. We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU. We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning", "authors": "Jingfeng Yao, Xinggang Wang, Yuehao Song, Huangxuan Zhao, Jun Ma, Yajie Chen, Wenyu Liu, Bo Wang", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV)", "abstract": "The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination. Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks. EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation. Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field. Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning. The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice. Our codes and models are available at: this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Cellular Traffic Prediction Using Online Prediction Algorithms", "authors": "Hossein Mehri, Hao Chen, Hani Mehrpouyan", "subjects": "Subjects:\nSystems and Control (eess.SY); Machine Learning (cs.LG)", "abstract": "The advent of 5G technology promises a paradigm shift in the realm of telecommunications, offering unprecedented speeds and connectivity. However, the efficient management of traffic in 5G networks remains a critical challenge. It is due to the dynamic and heterogeneous nature of network traffic, varying user behaviors, extended network size, and diverse applications, all of which demand highly accurate and adaptable prediction models to optimize network resource allocation and management. This paper investigates the efficacy of live prediction algorithms for forecasting cellular network traffic in real-time scenarios. We apply two live prediction algorithms on machine learning models, one of which is recently proposed Fast LiveStream Prediction (FLSP) algorithm. We examine the performance of these algorithms under two distinct data gathering methodologies: synchronous, where all network cells report statistics simultaneously, and asynchronous, where reporting occurs across consecutive time slots. Our study delves into the impact of these gathering scenarios on the predictive performance of traffic models. Our study reveals that the FLSP algorithm can halve the required bandwidth for asynchronous data reporting compared to conventional online prediction algorithms, while simultaneously enhancing prediction accuracy and reducing processing load. Additionally, we conduct a thorough analysis of algorithmic complexity and memory requirements across various machine learning models. Through empirical evaluation, we provide insights into the trade-offs inherent in different prediction strategies, offering valuable guidance for network optimization and resource allocation in dynamic environments."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          An LSTM-Based Chord Generation System Using Chroma Histogram Representations", "authors": "Jack Hardwick", "subjects": "Subjects:\nSound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)", "abstract": "This paper proposes a system for chord generation to monophonic symbolic melodies using an LSTM-based model trained on chroma histogram representations of chords. Chroma representations promise more harmonically rich generation than chord label-based approaches, whilst maintaining a small number of dimensions in the dataset. This system is shown to be suitable for limited real-time use. While it does not meet the state-of-the-art for coherent long-term generation, it does show diatonic generation with cadential chord relationships. The need for further study into chroma histograms as an extracted feature in chord generation tasks is highlighted."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          BenthicNet: A global compilation of seafloor images for deep learning applications", "authors": "Scott C. Lowe, Benjamin Misiuk, Isaac Xu, Shakhboz Abdulazizov, Amit R. Baroi, Alex C. Bastos, Merlin Best, Vicki Ferrini, Ariell Friedman, Deborah Hart, Ove Hoegh-Guldberg, Daniel Ierodiaconou, Julia Mackin-McLaughlin, Kathryn Markey, Pedro S. Menandro, Jacquomo Monk, Shreya Nemani, John O'Brien, Elizabeth Oh, Luba Y. Reshitnyk, Katleen Robert, Chris M. Roelfsema, Jessica A. Sameoto, Alexandre C. G. Schimel, Jordan A. Thomson, Brittany R. Wilson, Melisa C. Wong, Craig J. Brown, Thomas Trappenberg", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "Advances in underwater imaging enable the collection of extensive seafloor image datasets that are necessary for monitoring important benthic ecosystems. The ability to collect seafloor imagery has outpaced our capacity to analyze it, hindering expedient mobilization of this crucial environmental information. Recent machine learning approaches provide opportunities to increase the efficiency with which seafloor image datasets are analyzed, yet large and consistent datasets necessary to support development of such approaches are scarce. Here we present BenthicNet: a global compilation of seafloor imagery designed to support the training and evaluation of large-scale image recognition models. An initial set of over 11.4 million images was collected and curated to represent a diversity of seafloor environments using a representative subset of 1.3 million images. These are accompanied by 2.6 million annotations translated to the CATAMI scheme, which span 190,000 of the images. A large deep learning model was trained on this compilation and preliminary results suggest it has utility for automating large and small-scale image analysis tasks. The compilation and model are made openly available for use by the scientific community at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Advancing Blockchain Scalability: A Linear Optimization Framework for Diversified Node Allocation in Shards", "authors": "Bj\u00f6rn Assmann, Samuel J. Burri", "subjects": "Subjects:\nDistributed, Parallel, and Cluster Computing (cs.DC)", "abstract": "Blockchain technology, while revolutionary in enabling decentralized transactions, faces scalability challenges as the ledger must be replicated across all nodes of the chain, limiting throughput and efficiency. Sharding, which divides the chain into smaller segments, called shards, offers a solution by enabling parallel transaction processing. However, sharding introduces new complexities, notably how to allocate nodes to shards without compromising the network's security. This paper introduces a novel linear optimization framework for node allocation to shards that addresses decentralization constraints while minimizing resource consumption. In contrast to traditional methods that depend on random or trust-based assignments, our approach evaluates node characteristics, including ownership, hardware, and geographical distribution, and requires an explicit specification of decentralization targets with respect to these characteristics. By employing linear optimization, the framework identifies a resource-efficient node set meeting these targets. Adopted by the Internet Computer Protocol (ICP) community, this framework proves its utility in real-world blockchain applications. It provides a quantitative tool for node onboarding and offboarding decisions, balancing decentralization and resource considerations."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          LLMs with Personalities in Multi-issue Negotiation Games", "authors": "Sean Noh, Ho-Chun Herbert Chang", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "abstract": "Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be \"jail broken\" to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models", "authors": "Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj K. Jha, Yuchen Liu", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Signal Processing (eess.SP)", "abstract": "Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge", "authors": "Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, Paul Denny", "subjects": "Subjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "abstract": "Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          You Only Cache Once: Decoder-Decoder Architectures for Language Models", "authors": "Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu Wei", "subjects": "Subjects:\nComputation and Language (cs.CL)", "abstract": "We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes. Code is available at this https URL."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "authors": "Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \"Type I hallucinations\". Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term \"Type II hallucinations\". Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving", "authors": "Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang, Liang Pan, Kai Chen, Wei Tsang Ooi, Ziwei Liu", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)", "abstract": "Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems."}
{"main_page": "https://arxiv.org/abs/", "pdf": "https://arxiv.org/pdf/", "title": "Title:\n          OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies", "authors": "Lingdong Kong, Youquan Liu, Lai Xing Ng, Benoit R. Cottereau, Wei Tsang Ooi", "subjects": "Subjects:\nComputer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "abstract": "Event-based semantic segmentation (ESS) is a fundamental yet challenging task for event camera sensing. The difficulties in interpreting and annotating event data limit its scalability. While domain adaptation from images to event data can help to mitigate this issue, there exist data representational differences that require additional effort to resolve. In this work, for the first time, we synergize information from image, text, and event-data domains and introduce OpenESS to enable scalable ESS in an open-world, annotation-efficient manner. We achieve this goal by transferring the semantically rich CLIP knowledge from image-text pairs to event streams. To pursue better cross-modality adaptation, we propose a frame-to-event contrastive distillation and a text-to-event semantic consistency regularization. Experimental results on popular ESS benchmarks showed our approach outperforms existing methods. Notably, we achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either event or frame labels."}
